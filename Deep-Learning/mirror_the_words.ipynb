{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "题目：Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.下载text8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000000, ' collectively and that goods be distributed by need not labor an early anarchist communist was joseph d jacque the first person to describe himself as libertarian unlike proudhon he argued that it is ')\n",
      "(10000, ' anarchism originated as a term of abuse first used against early working class radicals including t')\n"
     ]
    }
   ],
   "source": [
    "valid_size = 10000\n",
    "train_size = 10000000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:valid_size+train_size]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:200])\n",
    "print(valid_size, valid_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 26, 0)\n",
      "('a', 'z', ' ')\n"
     ]
    }
   ],
   "source": [
    "# 将字符转为id\n",
    "import string\n",
    "\n",
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ylevitcelloc dna aht\n",
      "nr redrob htiw elihc \n"
     ]
    }
   ],
   "source": [
    "def mirror(text):\n",
    "    words = text.split(\" \")\n",
    "    mirror = []\n",
    "    for word in words:\n",
    "        mirror.append(word[::-1])\n",
    "    return \" \".join(mirror)\n",
    "\n",
    "print(mirror(\" collectively and tha\"))\n",
    "print(mirror(\"rn border with chile \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 20\n",
    "\n",
    "def build_dataset(text):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    length = len(text) / seq_len\n",
    "    for i in range(length):\n",
    "        line = text[i:i+seq_len]\n",
    "        mirror_line = mirror(line)\n",
    "        dataset.append([char2id(ch) for ch in line])\n",
    "        labels.append([char2id(ch) for ch in mirror_line])\n",
    "    return dataset, labels\n",
    "\n",
    "train_set, train_labels = build_dataset(train_text)\n",
    "valid_set, valid_labels = build_dataset(valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 3, 15, 12, 12, 5, 3, 20, 9, 22, 5, 12, 25, 0, 1, 14, 4, 0, 20, 8], [0, 25, 12, 5, 22, 9, 20, 3, 5, 12, 12, 15, 3, 0, 4, 14, 1, 0, 8, 20])\n",
      "(' collectively and th', ' ylevitcelloc dna ht')\n"
     ]
    }
   ],
   "source": [
    "def show_string(data):\n",
    "    return \"\".join([id2char(_id) for _id in data])\n",
    "\n",
    "print(train_set[0], train_labels[0])\n",
    "print(show_string(train_set[0]), show_string(train_labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize(word, seq_len, vec_size):\n",
    "    vec = np.zeros((seq_len, vec_size), dtype=int)\n",
    "    for i, ch in enumerate(word):\n",
    "        vec[i, ch] = 1\n",
    "    return vec\n",
    "\n",
    "def vectorize_dataset(dataset, labels):\n",
    "    x = np.zeros((len(dataset), seq_len, vocabulary_size), dtype=int)\n",
    "    y = np.zeros((len(dataset), seq_len, vocabulary_size), dtype=int)\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        x[i] = vectorize(dataset[i], seq_len, vocabulary_size)\n",
    "        y[i] = vectorize(labels[i], seq_len, vocabulary_size)\n",
    "    return x, y\n",
    "\n",
    "train_x, train_y = vectorize_dataset(train_set, train_labels)\n",
    "valid_x, valid_y = vectorize_dataset(valid_set, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_x:', (500000, 20, 27))\n",
      "('train_y:', (500000, 20, 27))\n",
      "('valid_x:', (500, 20, 27))\n",
      "('valid_y:', (500, 20, 27))\n"
     ]
    }
   ],
   "source": [
    "print(\"train_x:\", train_x.shape)\n",
    "print(\"train_y:\", train_y.shape)\n",
    "print(\"valid_x:\", valid_x.shape)\n",
    "print(\"valid_y:\", valid_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers.core import Dense, RepeatVector\n",
    "\n",
    "def build_model(input_size, seq_len, hidden_size):\n",
    "    \"\"\"建立一个 sequence to sequence 模型\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(GRU(input_shape=(None, input_size), units=hidden_size, return_sequences=False))\n",
    "    model.add(Dense(hidden_size, activation=\"relu\"))\n",
    "    model.add(RepeatVector(seq_len))\n",
    "    model.add(GRU(units=hidden_size, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(units=input_size, activation=\"linear\")))\n",
    "    model.compile(loss=\"mse\", optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model(vocabulary_size, seq_len, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.训练与评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500000 samples, validate on 500 samples\n",
      "Epoch 1/128\n",
      " 51968/500000 [==>...........................] - ETA: 2603s - loss: 0.0332"
     ]
    }
   ],
   "source": [
    "model.fit(train_x, train_y,\n",
    "          batch_size=128, \n",
    "          nb_epoch=128,\n",
    "          verbose=1,\n",
    "          validation_data=(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecnemres ot ecnelees\n",
      "nr remrow htiw elihc\n",
      " ylecitcelloc dna ht\n",
      "e na melub ollam mml\n"
     ]
    }
   ],
   "source": [
    "def test_output(test_str):\n",
    "    test_case = np.zeros((1, seq_len, vocabulary_size), dtype=int)\n",
    "    test_case[0] = vectorize([char2id(ch) for ch in test_str], seq_len, vocabulary_size)\n",
    "\n",
    "    pred = model.predict(test_case)[0]\n",
    "    print(''.join([id2char(i) for i in pred.argmax(axis=1)]))\n",
    "\n",
    "test_output(\"sequence to sequence\")\n",
    "test_output(\"rn border with chile\")\n",
    "test_output(\" collectively and th\")\n",
    "test_output(\"i am kalen hello guy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
