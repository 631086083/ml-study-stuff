{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we'll load the text file and convert it into integers for our network to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\r\\n\\r\\n\\r\\nHappy families are all alike; every unhappy family is unhappy in its own\\r\\nway.\\r\\n\\r\\nEve'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31, 66, 57, 74, 78, 61, 76,  3, 16,  1,  0,  1,  0,  1,  0, 38, 57,\n",
       "       74, 74, 81,  3, 64, 57, 69, 65, 70, 65, 61, 75,  3, 57, 76, 61,  3,\n",
       "       57, 70, 70,  3, 57, 70, 65, 67, 61, 26,  3, 61, 80, 61, 76, 81,  3,\n",
       "       77, 72, 66, 57, 74, 74, 81,  3, 64, 57, 69, 65, 70, 81,  3, 65, 75,\n",
       "        3, 77, 72, 66, 57, 74, 74, 81,  3, 65, 72,  3, 65, 78, 75,  3, 71,\n",
       "       79, 72,  1,  0, 79, 57, 81, 15,  1,  0,  1,  0, 33, 80, 61], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the number of batches. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 182000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[31, 66, 57, 74, 78, 61, 76,  3, 16,  1],\n",
       "       [ 3, 74, 76, 61, 75, 75,  3, 66, 61, 76],\n",
       "       [72,  3, 78, 71,  3, 75, 74, 61, 57, 67],\n",
       "       [61, 69, 74, 71, 76, 57, 76, 81,  3, 61],\n",
       "       [ 3,  4, 50, 66, 61, 81,  7, 76, 61,  3],\n",
       "       [72, 72, 71, 78, 65, 59, 61, 62, 15,  3],\n",
       "       [65, 72, 63, 61, 76, 75,  3, 79, 65, 78],\n",
       "       [ 3, 69, 71, 69, 61, 72, 78,  3, 79, 66],\n",
       "       [65, 64, 78,  3, 71, 64,  3, 78, 66, 61],\n",
       "       [70,  3, 62, 57, 81, 13,  3, 57, 72, 62]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I'll write another function to grab batches out of the arrays made by split data. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "        x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "    \n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "        y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "        y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Build the RNN layers\n",
    "    with tf.name_scope(\"RNN_cells\"):\n",
    "        def build_cell(lstm_size, keep_prob):\n",
    "            lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "            drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "            return drop\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    \n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each cell output\n",
    "    with tf.name_scope('sequence_reshape'):\n",
    "        seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "        output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer and calculate the cost\n",
    "    with tf.name_scope('logits'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                               name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        tf.summary.histogram('softmax_w', softmax_w)\n",
    "        tf.summary.histogram('softmax_b', softmax_b)\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        preds = tf.nn.softmax(logits, name='predictions')\n",
    "        tf.summary.histogram('predictions', preds)\n",
    "    \n",
    "    with tf.name_scope('cost'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "        cost = tf.reduce_mean(loss, name='cost')\n",
    "        tf.summary.scalar('cost', cost)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    with tf.name_scope('train'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer', 'merged']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. The two you probably haven't seen before are `lstm_size` and `num_layers`. These set the number of hidden units in the LSTM layers and the number of LSTM layers, respectively. Of course, making these bigger will improve the network's performance but you'll have to watch out for overfitting. If your validation loss is much larger than the training loss, you're probably overfitting. Decrease the size of the network or decrease the dropout keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p checkpoints/anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, file_writer):\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Use the line below to load a checkpoint and resume training\n",
    "        #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "\n",
    "        n_batches = int(train_x.shape[1]/num_steps)\n",
    "        iterations = n_batches * epochs\n",
    "        for e in range(epochs):\n",
    "\n",
    "            # Train network\n",
    "            new_state = sess.run(model.initial_state)\n",
    "            loss = 0\n",
    "            for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "                iteration = e*n_batches + b\n",
    "                start = time.time()\n",
    "                feed = {model.inputs: x,\n",
    "                        model.targets: y,\n",
    "                        model.keep_prob: 0.5,\n",
    "                        model.initial_state: new_state}\n",
    "                summary, batch_loss, new_state, _ = sess.run([model.merged, model.cost, \n",
    "                                                              model.final_state, model.optimizer], \n",
    "                                                              feed_dict=feed)\n",
    "                loss += batch_loss\n",
    "                end = time.time()\n",
    "                print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                      'Iteration {}/{}'.format(iteration, iterations),\n",
    "                      'Training loss: {:.4f}'.format(loss/b),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "\n",
    "                file_writer.add_summary(summary, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch 1/20 ', 'Iteration 1/3620', 'Training loss: 4.4307', '0.4522 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 2/3620', 'Training loss: 4.4194', '0.4393 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 3/3620', 'Training loss: 4.4057', '0.4022 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 4/3620', 'Training loss: 4.3868', '0.3943 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 5/3620', 'Training loss: 4.3488', '0.4345 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 6/3620', 'Training loss: 4.2752', '0.4296 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 7/3620', 'Training loss: 4.1922', '0.3995 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 8/3620', 'Training loss: 4.1176', '0.3963 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 9/3620', 'Training loss: 4.0552', '0.4377 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 10/3620', 'Training loss: 3.9938', '0.4362 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 11/3620', 'Training loss: 3.9422', '0.4066 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 12/3620', 'Training loss: 3.8968', '0.4016 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 13/3620', 'Training loss: 3.8563', '0.4277 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 14/3620', 'Training loss: 3.8201', '0.4550 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 15/3620', 'Training loss: 3.7898', '0.4099 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 16/3620', 'Training loss: 3.7597', '0.3956 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 17/3620', 'Training loss: 3.7337', '0.4381 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 18/3620', 'Training loss: 3.7104', '0.4475 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 19/3620', 'Training loss: 3.6889', '0.4082 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 20/3620', 'Training loss: 3.6683', '0.4053 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 21/3620', 'Training loss: 3.6488', '0.4422 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 22/3620', 'Training loss: 3.6311', '0.4418 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 23/3620', 'Training loss: 3.6150', '0.4061 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 24/3620', 'Training loss: 3.5994', '0.4018 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 25/3620', 'Training loss: 3.5850', '0.4354 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 26/3620', 'Training loss: 3.5715', '0.4457 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 27/3620', 'Training loss: 3.5590', '0.4010 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 28/3620', 'Training loss: 3.5473', '0.4017 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 29/3620', 'Training loss: 3.5360', '0.4893 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 30/3620', 'Training loss: 3.5248', '0.5136 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 31/3620', 'Training loss: 3.5143', '0.4157 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 32/3620', 'Training loss: 3.5052', '0.4010 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 33/3620', 'Training loss: 3.4967', '0.4376 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 34/3620', 'Training loss: 3.4877', '0.4366 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 35/3620', 'Training loss: 3.4787', '0.4011 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 36/3620', 'Training loss: 3.4712', '0.3989 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 37/3620', 'Training loss: 3.4637', '0.4427 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 38/3620', 'Training loss: 3.4560', '0.4454 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 39/3620', 'Training loss: 3.4486', '0.3963 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 40/3620', 'Training loss: 3.4415', '0.4075 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 41/3620', 'Training loss: 3.4342', '0.4499 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 42/3620', 'Training loss: 3.4281', '0.4262 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 43/3620', 'Training loss: 3.4219', '0.4032 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 44/3620', 'Training loss: 3.4157', '0.3970 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 45/3620', 'Training loss: 3.4097', '0.4552 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 46/3620', 'Training loss: 3.4040', '0.4379 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 47/3620', 'Training loss: 3.3986', '0.4096 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 48/3620', 'Training loss: 3.3932', '0.4104 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 49/3620', 'Training loss: 3.3885', '0.4690 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 50/3620', 'Training loss: 3.3841', '0.3973 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 51/3620', 'Training loss: 3.3796', '0.3999 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 52/3620', 'Training loss: 3.3748', '0.4211 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 53/3620', 'Training loss: 3.3703', '0.4609 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 54/3620', 'Training loss: 3.3659', '0.4195 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 55/3620', 'Training loss: 3.3617', '0.4920 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 56/3620', 'Training loss: 3.3576', '0.4789 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 57/3620', 'Training loss: 3.3535', '0.4388 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 58/3620', 'Training loss: 3.3495', '0.3974 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 59/3620', 'Training loss: 3.3456', '0.4020 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 60/3620', 'Training loss: 3.3422', '0.4379 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 61/3620', 'Training loss: 3.3386', '0.4370 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 62/3620', 'Training loss: 3.3348', '0.4079 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 63/3620', 'Training loss: 3.3315', '0.4041 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 64/3620', 'Training loss: 3.3280', '0.4448 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 65/3620', 'Training loss: 3.3250', '0.4394 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 66/3620', 'Training loss: 3.3220', '0.4071 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 67/3620', 'Training loss: 3.3186', '0.3971 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 68/3620', 'Training loss: 3.3154', '0.4368 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 69/3620', 'Training loss: 3.3124', '0.4375 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 70/3620', 'Training loss: 3.3093', '0.4063 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 71/3620', 'Training loss: 3.3063', '0.4016 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 72/3620', 'Training loss: 3.3032', '0.4333 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 73/3620', 'Training loss: 3.3002', '0.4303 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 74/3620', 'Training loss: 3.2975', '0.3986 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 75/3620', 'Training loss: 3.2948', '0.4070 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 76/3620', 'Training loss: 3.2920', '0.4492 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 77/3620', 'Training loss: 3.2892', '0.4225 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 78/3620', 'Training loss: 3.2867', '0.4040 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 79/3620', 'Training loss: 3.2840', '0.4019 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 80/3620', 'Training loss: 3.2817', '0.4676 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 81/3620', 'Training loss: 3.2790', '0.5095 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 82/3620', 'Training loss: 3.2763', '0.4449 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 83/3620', 'Training loss: 3.2736', '0.4068 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 84/3620', 'Training loss: 3.2713', '0.4708 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 85/3620', 'Training loss: 3.2687', '0.4057 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 86/3620', 'Training loss: 3.2662', '0.4056 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 87/3620', 'Training loss: 3.2636', '0.4218 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 88/3620', 'Training loss: 3.2608', '0.4557 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 89/3620', 'Training loss: 3.2583', '0.4054 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 90/3620', 'Training loss: 3.2557', '0.4052 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 91/3620', 'Training loss: 3.2530', '0.4217 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 92/3620', 'Training loss: 3.2507', '0.4536 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 93/3620', 'Training loss: 3.2482', '0.4049 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 94/3620', 'Training loss: 3.2458', '0.4008 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 95/3620', 'Training loss: 3.2433', '0.4290 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 96/3620', 'Training loss: 3.2408', '0.4461 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 97/3620', 'Training loss: 3.2383', '0.3997 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 98/3620', 'Training loss: 3.2354', '0.4037 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 99/3620', 'Training loss: 3.2330', '0.4310 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 100/3620', 'Training loss: 3.2304', '0.4462 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 101/3620', 'Training loss: 3.2279', '0.3974 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 102/3620', 'Training loss: 3.2253', '0.4026 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 103/3620', 'Training loss: 3.2227', '0.4233 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 104/3620', 'Training loss: 3.2201', '0.4528 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 105/3620', 'Training loss: 3.2175', '0.4050 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 106/3620', 'Training loss: 3.2149', '0.4564 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 107/3620', 'Training loss: 3.2122', '0.5331 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 108/3620', 'Training loss: 3.2097', '0.4368 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 109/3620', 'Training loss: 3.2069', '0.4007 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 110/3620', 'Training loss: 3.2040', '0.4049 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 111/3620', 'Training loss: 3.2011', '0.4839 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 112/3620', 'Training loss: 3.1983', '0.4086 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 113/3620', 'Training loss: 3.1955', '0.4000 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 114/3620', 'Training loss: 3.1926', '0.4005 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 115/3620', 'Training loss: 3.1899', '0.4842 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 116/3620', 'Training loss: 3.1869', '0.4245 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 117/3620', 'Training loss: 3.1839', '0.4014 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 118/3620', 'Training loss: 3.1810', '0.4037 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 119/3620', 'Training loss: 3.1781', '0.4801 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 120/3620', 'Training loss: 3.1751', '0.4023 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 121/3620', 'Training loss: 3.1722', '0.4029 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 122/3620', 'Training loss: 3.1693', '0.4035 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 123/3620', 'Training loss: 3.1664', '0.4745 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 124/3620', 'Training loss: 3.1637', '0.4105 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 125/3620', 'Training loss: 3.1608', '0.3935 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 126/3620', 'Training loss: 3.1578', '0.3900 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 127/3620', 'Training loss: 3.1549', '0.4610 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 128/3620', 'Training loss: 3.1518', '0.3966 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 129/3620', 'Training loss: 3.1487', '0.4721 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 130/3620', 'Training loss: 3.1457', '0.4867 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 131/3620', 'Training loss: 3.1427', '0.4709 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 132/3620', 'Training loss: 3.1398', '0.5253 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 133/3620', 'Training loss: 3.1368', '0.4712 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 134/3620', 'Training loss: 3.1339', '0.4762 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 135/3620', 'Training loss: 3.1308', '0.3986 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 136/3620', 'Training loss: 3.1279', '0.3908 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 137/3620', 'Training loss: 3.1246', '0.3943 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 138/3620', 'Training loss: 3.1216', '0.4623 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 139/3620', 'Training loss: 3.1186', '0.3939 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 140/3620', 'Training loss: 3.1154', '0.4008 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 141/3620', 'Training loss: 3.1121', '0.3907 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 142/3620', 'Training loss: 3.1092', '0.4632 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 143/3620', 'Training loss: 3.1060', '0.4001 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 144/3620', 'Training loss: 3.1030', '0.3878 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 145/3620', 'Training loss: 3.0999', '0.3893 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 146/3620', 'Training loss: 3.0968', '0.4546 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 147/3620', 'Training loss: 3.0938', '0.4037 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 148/3620', 'Training loss: 3.0908', '0.3915 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 149/3620', 'Training loss: 3.0877', '0.3920 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 150/3620', 'Training loss: 3.0847', '0.4164 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 151/3620', 'Training loss: 3.0819', '0.4241 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 152/3620', 'Training loss: 3.0789', '0.3928 sec/batch')\n",
      "('Epoch 1/20 ', 'Iteration 153/3620', 'Training loss: 3.0759', '0.4060 sec/batch')\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 100\n",
    "num_steps = 100\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "for lstm_size in [128,256,512]:\n",
    "    for num_layers in [1, 2]:\n",
    "        for learning_rate in [0.002, 0.001]:\n",
    "            log_string = 'logs/4/lr={},rl={},ru={}'.format(learning_rate, num_layers, lstm_size)\n",
    "            writer = tf.summary.FileWriter(log_string)\n",
    "            model = build_rnn(len(vocab), \n",
    "                    batch_size=batch_size,\n",
    "                    num_steps=num_steps,\n",
    "                    learning_rate=learning_rate,\n",
    "                    lstm_size=lstm_size,\n",
    "                    num_layers=num_layers)\n",
    "            \n",
    "            train(model, epochs, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/anna/i3560_l512_1.122.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i200_l512_2.432.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i400_l512_1.980.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i600_l512_1.750.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i800_l512_1.595.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1000_l512_1.484.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1200_l512_1.407.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1400_l512_1.349.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1600_l512_1.292.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1800_l512_1.255.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i2000_l512_1.224.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i2200_l512_1.204.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i2400_l512_1.187.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i2600_l512_1.172.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i2800_l512_1.160.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i3000_l512_1.148.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i3200_l512_1.137.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i3400_l512_1.129.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i3560_l512_1.122.ckpt\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/anna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    prime = \"Far\"\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farlathit that if had so\n",
      "like it that it were. He could not trouble to his wife, and there was\n",
      "anything in them of the side of his weaky in the creature at his forteren\n",
      "to him.\n",
      "\n",
      "\"What is it? I can't bread to those,\" said Stepan Arkadyevitch. \"It's not\n",
      "my children, and there is an almost this arm, true it mays already,\n",
      "and tell you what I have say to you, and was not looking at the peasant,\n",
      "why is, I don't know him out, and she doesn't speak to me immediately, as\n",
      "you would say the countess and the more frest an angelembre, and time and\n",
      "things's silent, but I was not in my stand that is in my head. But if he\n",
      "say, and was so feeling with his soul. A child--in his soul of his\n",
      "soul of his soul. He should not see that any of that sense of. Here he\n",
      "had not been so composed and to speak for as in a whole picture, but\n",
      "all the setting and her excellent and society, who had been delighted\n",
      "and see to anywing had been being troed to thousand words on them,\n",
      "we liked him.\n",
      "\n",
      "That set in her money at the table, he came into the party. The capable\n",
      "of his she could not be as an old composure.\n",
      "\n",
      "\"That's all something there will be down becime by throe is\n",
      "such a silent, as in a countess, I should state it out and divorct.\n",
      "The discussion is not for me. I was that something was simply they are\n",
      "all three manshess of a sensitions of mind it all.\"\n",
      "\n",
      "\"No,\" he thought, shouted and lifting his soul. \"While it might see your\n",
      "honser and she, I could burst. And I had been a midelity. And I had a\n",
      "marnief are through the countess,\" he said, looking at him, a chosing\n",
      "which they had been carried out and still solied, and there was a sen that\n",
      "was to be completely, and that this matter of all the seconds of it, and\n",
      "a concipation were to her husband, who came up and conscaously, that he\n",
      "was not the station. All his fourse she was always at the country,,\n",
      "to speak oft, and though they were to hear the delightful throom and\n",
      "whether they came towards the morning, and his living and a coller and\n",
      "hold--the children. \n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i3560_l512_1.122.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farnt him oste wha sorind thans tout thint asd an sesand an hires on thime sind thit aled, ban thand and out hore as the ter hos ton ho te that, was tis tart al the hand sostint him sore an tit an son thes, win he se ther san ther hher tas tarereng,.\n",
      "\n",
      "Anl at an ades in ond hesiln, ad hhe torers teans, wast tar arering tho this sos alten sorer has hhas an siton ther him he had sin he ard ate te anling the sosin her ans and\n",
      "arins asd and ther ale te tot an tand tanginge wath and ho ald, so sot th asend sat hare sother horesinnd, he hesense wing ante her so tith tir sherinn, anded and to the toul anderin he sorit he torsith she se atere an ting ot hand and thit hhe so the te wile har\n",
      "ens ont in the sersise, and we he seres tar aterer, to ato tat or has he he wan ton here won and sen heren he sosering, to to theer oo adent har herere the wosh oute, was serild ward tous hed astend..\n",
      "\n",
      "I's sint on alt in har tor tit her asd hade shithans ored he talereng an soredendere tim tot hees. Tise sor and \n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i200_l512_2.432.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fard as astice her said he celatice of to seress in the raice, and to be the some and sere allats to that said to that the sark and a cast a the wither ald the pacinesse of her had astition, he said to the sount as she west at hissele. Af the cond it he was a fact onthis astisarianing.\n",
      "\n",
      "\n",
      "\"Or a ton to to be that's a more at aspestale as the sont of anstiring as\n",
      "thours and trey.\n",
      "\n",
      "The same wo dangring the\n",
      "raterst, who sore and somethy had ast out an of his book. \"We had's beane were that, and a morted a thay he had to tere. Then to\n",
      "her homent andertersed his his ancouted to the pirsted, the soution for of the pirsice inthirgest and stenciol, with the hard and and\n",
      "a colrice of to be oneres,\n",
      "the song to this anderssad.\n",
      "The could ounterss the said to serom of\n",
      "soment a carsed of sheres of she\n",
      "torded\n",
      "har and want in their of hould, but\n",
      "her told in that in he tad a the same to her. Serghing an her has and with the seed, and the camt ont his about of the\n",
      "sail, the her then all houg ant or to hus to \n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i600_l512_1.750.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farrat, his felt has at it.\n",
      "\n",
      "\"When the pose ther hor exceed\n",
      "to his sheant was,\" weat a sime of his sounsed. The coment and the facily that which had began terede a marilicaly whice whether the pose of his hand, at she was alligated herself the same on she had to\n",
      "taiking to his forthing and streath how to hand\n",
      "began in a lang at some at it, this he cholded not set all her. \"Wo love that is setthing. Him anstering as seen that.\"\n",
      "\n",
      "\"Yes in the man that say the mare a crances is it?\" said Sergazy Ivancatching. \"You doon think were somether is ifficult of a mone of\n",
      "though the most at the countes that the\n",
      "mean on the come to say the most, to\n",
      "his feesing of\n",
      "a man she, whilo he\n",
      "sained and well, that he would still at to said. He wind at his for the sore in the most\n",
      "of hoss and almoved to see him. They have betine the sumper into at he his stire, and what he was that at the so steate of the\n",
      "sound, and shin should have a geest of shall feet on the conderation to she had been at that imporsing the dre\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i1000_l512_1.484.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
