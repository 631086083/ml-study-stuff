{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31, 64, 57, 72, 76, 61, 74,  1, 16,  0,  0,  0, 36, 57, 72, 72, 81,\n",
       "        1, 62, 57, 69, 65, 68, 65, 61, 75,  1, 57, 74, 61,  1, 57, 68, 68,\n",
       "        1, 57, 68, 65, 67, 61, 26,  1, 61, 78, 61, 74, 81,  1, 77, 70, 64,\n",
       "       57, 72, 72, 81,  1, 62, 57, 69, 65, 68, 81,  1, 65, 75,  1, 77, 70,\n",
       "       64, 57, 72, 72, 81,  1, 65, 70,  1, 65, 76, 75,  1, 71, 79, 70,  0,\n",
       "       79, 57, 81, 13,  0,  0, 33, 78, 61, 74, 81, 76, 64, 65, 70], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches\n",
    "\n",
    "Here is where we'll make our mini-batches for training. Remember that we want our batches to be multiple sequences of some desired number of sequence steps. Considering a simple example, our batches would look like this:\n",
    "\n",
    "<img src=\"assets/sequence_batching@1x.png\" width=500px>\n",
    "\n",
    "\n",
    "<br>\n",
    "We have our text encoded as integers as one long array in `encoded`. Let's create a function that will give us an iterator for our batches. I like using [generator functions](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/) to do this. Then we can pass `encoded` into this function and get our batch generator.\n",
    "\n",
    "The first thing we need to do is discard some of the text so we only have completely full batches. Each batch contains $N \\times M$ characters, where $N$ is the batch size (the number of sequences) and $M$ is the number of steps. Then, to get the number of batches we can make from some array `arr`, you divide the length of `arr` by the batch size. Once you know the number of batches and the batch size, you can get the total number of characters to keep.\n",
    "\n",
    "After that, we need to split `arr` into $N$ sequences. You can do this using `arr.reshape(size)` where `size` is a tuple containing the dimensions sizes of the reshaped array. We know we want $N$ sequences (`n_seqs` below), let's make that the size of the first dimension. For the second dimension, you can use `-1` as a placeholder in the size, it'll fill up the array with the appropriate data for you. After this, you should have an array that is $N \\times (M * K)$ where $K$ is the number of batches.\n",
    "\n",
    "Now that we have this array, we can iterate through it to get our batches. The idea is each batch is a $N \\times M$ window on the array. For each subsequent batch, the window moves over by `n_steps`. We also want to create both the input and target arrays. Remember that the targets are the inputs shifted over one character. You'll usually see the first input character used as the last target character, so something like this:\n",
    "```python\n",
    "y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "```\n",
    "where `x` is the input batch and `y` is the target batch.\n",
    "\n",
    "The way I like to do this window is use `range` to take steps of size `n_steps` from $0$ to `arr.shape[1]`, the total number of steps in each sequence. That way, the integers you get from `range` always point to the start of a batch, and each window is `n_steps` wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    # Get the number of characters per batch and number of batches we can make\n",
    "    characters_per_batch = n_seqs * n_steps\n",
    "    n_batches = len(arr)//characters_per_batch\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * characters_per_batch]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[31 64 57 72 76 61 74  1 16  0]\n",
      " [ 1 57 69  1 70 71 76  1 63 71]\n",
      " [78 65 70 13  0  0  3 53 61 75]\n",
      " [70  1 60 77 74 65 70 63  1 64]\n",
      " [ 1 65 76  1 65 75 11  1 75 65]\n",
      " [ 1 37 76  1 79 57 75  0 71 70]\n",
      " [64 61 70  1 59 71 69 61  1 62]\n",
      " [26  1 58 77 76  1 70 71 79  1]\n",
      " [76  1 65 75 70  7 76 13  1 48]\n",
      " [ 1 75 57 65 60  1 76 71  1 64]]\n",
      "\n",
      "y\n",
      " [[64 57 72 76 61 74  1 16  0  0]\n",
      " [57 69  1 70 71 76  1 63 71 65]\n",
      " [65 70 13  0  0  3 53 61 75 11]\n",
      " [ 1 60 77 74 65 70 63  1 64 65]\n",
      " [65 76  1 65 75 11  1 75 65 74]\n",
      " [37 76  1 79 57 75  0 71 70 68]\n",
      " [61 70  1 59 71 69 61  1 62 71]\n",
      " [ 1 58 77 76  1 70 71 79  1 75]\n",
      " [ 1 65 75 70  7 76 13  1 48 64]\n",
      " [75 57 65 60  1 76 71  1 64 61]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented `get_batches` correctly, the above output should look something like \n",
    "```\n",
    "x\n",
    " [[55 63 69 22  6 76 45  5 16 35]\n",
    " [ 5 69  1  5 12 52  6  5 56 52]\n",
    " [48 29 12 61 35 35  8 64 76 78]\n",
    " [12  5 24 39 45 29 12 56  5 63]\n",
    " [ 5 29  6  5 29 78 28  5 78 29]\n",
    " [ 5 13  6  5 36 69 78 35 52 12]\n",
    " [63 76 12  5 18 52  1 76  5 58]\n",
    " [34  5 73 39  6  5 12 52 36  5]\n",
    " [ 6  5 29 78 12 79  6 61  5 59]\n",
    " [ 5 78 69 29 24  5  6 52  5 63]]\n",
    "\n",
    "y\n",
    " [[63 69 22  6 76 45  5 16 35 35]\n",
    " [69  1  5 12 52  6  5 56 52 29]\n",
    " [29 12 61 35 35  8 64 76 78 28]\n",
    " [ 5 24 39 45 29 12 56  5 63 29]\n",
    " [29  6  5 29 78 28  5 78 29 45]\n",
    " [13  6  5 36 69 78 35 52 12 43]\n",
    " [76 12  5 18 52  1 76  5 58 52]\n",
    " [ 5 73 39  6  5 12 52 36  5 78]\n",
    " [ 5 29 78 12 79  6 61  5 59 63]\n",
    " [78 69 29 24  5  6 52  5 63 76]]\n",
    " ```\n",
    " although the exact numbers will be different. Check to make sure the data is shifted over one step for `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Below is where you'll build the network. We'll break it up into parts so it's easier to reason about each bit. Then we can connect them up into the whole network.\n",
    "\n",
    "<img src=\"assets/charRNN.png\" width=500px>\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "First off we'll create our input placeholders. As usual we need placeholders for the training data and the targets. We'll also create a placeholder for dropout layers called `keep_prob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Define placeholders for inputs, targets, and dropout \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size: Batch size, number of sequences per batch\n",
    "        num_steps: Number of sequence steps in a batch\n",
    "        \n",
    "    '''\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cell\n",
    "\n",
    "Here we will create the LSTM cell we'll use in the hidden layer. We'll use this cell as a building block for the RNN. So we aren't actually defining the RNN here, just the type of cell we'll use in the hidden layer.\n",
    "\n",
    "We first create a basic LSTM cell with\n",
    "\n",
    "```python\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "```\n",
    "\n",
    "where `num_units` is the number of units in the hidden layers in the cell. Then we can add dropout by wrapping it with \n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "```\n",
    "You pass in a cell and it will automatically add dropout to the inputs or outputs. Finally, we can stack up the LSTM cells into layers with [`tf.contrib.rnn.MultiRNNCell`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/MultiRNNCell). With this, you pass in a list of cells and it will send the output of one cell into the next cell. Previously with TensorFlow 1.0, you could do this\n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.MultiRNNCell([cell]*num_layers)\n",
    "```\n",
    "\n",
    "This might look a little weird if you know Python well because this will create a list of the same `cell` object. However, TensorFlow 1.0 will create different weight matrices for all `cell` objects. But, starting with TensorFlow 1.1 you actually need to create new cell objects in the list. To get it to work in TensorFlow 1.1, it should look like\n",
    "\n",
    "```python\n",
    "def build_cell(num_units, keep_prob):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    return drop\n",
    "    \n",
    "tf.contrib.rnn.MultiRNNCell([build_cell(num_units, keep_prob) for _ in range(num_layers)])\n",
    "```\n",
    "\n",
    "Even though this is actually multiple LSTM cells stacked on each other, you can treat the multiple layers as one cell.\n",
    "\n",
    "We also need to create an initial cell state of all zeros. This can be done like so\n",
    "\n",
    "```python\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "```\n",
    "\n",
    "Below, we implement the `build_lstm` function to create these LSTM cells and the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Build the LSTM Cell\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        # Use a basic LSTM cell\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        \n",
    "        # Add dropout to the cell\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Output\n",
    "\n",
    "Here we'll create the output layer. We need to connect the output of the RNN cells to a full connected layer with a softmax output. The softmax output gives us a probability distribution we can use to predict the next character.\n",
    "\n",
    "If our input has batch size $N$, number of steps $M$, and the hidden layer has $L$ hidden units, then the output is a 3D tensor with size $N \\times M \\times L$. The output of each LSTM cell has size $L$, we have $M$ of them, one for each sequence step, and we have $N$ sequences. So the total size is $N \\times M \\times L$.\n",
    "\n",
    "We are using the same fully connected layer, the same weights, for each of the outputs. Then, to make things easier, we should reshape the outputs into a 2D tensor with shape $(M * N) \\times L$. That is, one row for each sequence and step, where the values of each row are the output from the LSTM cells.\n",
    "\n",
    "One we have the outputs reshaped, we can do the matrix multiplication with the weights. We need to wrap the weight and bias variables in a variable scope with `tf.variable_scope(scope_name)` because there are weights being created in the LSTM cells. TensorFlow will throw an error if the weights created here have the same names as the weights created in the LSTM cells, which they will be default. To avoid this, we wrap the variables in a variable scope so we can give them unique names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        x: Input tensor\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # That is, the shape should be batch_size*num_steps rows by lstm_size columns\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loss\n",
    "\n",
    "Next up is the training loss. We get the logits and targets and calculate the softmax cross-entropy loss. First we need to one-hot encode the targets, we're getting them as encoded characters. Then, reshape the one-hot targets so it's a 2D tensor with size $(M*N) \\times C$ where $C$ is the number of classes/characters we have. Remember that we reshaped the LSTM outputs and ran them through a fully connected layer with $C$ units. So our logits will also have size $(M*N) \\times C$.\n",
    "\n",
    "Then we run the logits and targets through `tf.nn.softmax_cross_entropy_with_logits` and find the mean to get the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hot encode targets and reshape to match logits, one row per batch_size per step\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Here we build the optimizer. Normal RNNs have have issues gradients exploding and disappearing. LSTMs fix the disappearance problem, but the gradients can still grow without bound. To fix this, we can clip the gradients above some threshold. That is, if a gradient is larger than that threshold, we set it to the threshold. This will ensure the gradients never grow overly large. Then we use an AdamOptimizer for the learning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network\n",
    "\n",
    "Now we can put all the pieces together and build a class for the network. To actually run data through the LSTM cells, we will use [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn). This function will pass the hidden and cell states across LSTM cells appropriately for us. It returns the outputs for each LSTM cell at each step for each sequence in the mini-batch. It also gives us the final LSTM state. We want to save this state as `final_state` so we can pass it to the first LSTM cell in the the next mini-batch run. For `tf.nn.dynamic_rnn`, we pass in the cell and initial state we get from `build_lstm`, as well as our input sequences. Also, we need to one-hot encode the inputs before going into the RNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN and collect the outputs\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. \n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to copy it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100        # Sequences per batch\n",
    "num_steps = 100         # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for training\n",
    "\n",
    "This is typical training code, passing inputs and targets into the network, then running the optimizer. Here we also get back the final LSTM state for the mini-batch. Then, we pass that state back into the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}.ckpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 1...  Training loss: 4.4206...  17.1224 sec/batch\n",
      "Epoch: 1/20...  Training Step: 2...  Training loss: 4.3305...  14.0747 sec/batch\n",
      "Epoch: 1/20...  Training Step: 3...  Training loss: 3.8574...  14.3327 sec/batch\n",
      "Epoch: 1/20...  Training Step: 4...  Training loss: 5.2415...  14.0171 sec/batch\n",
      "Epoch: 1/20...  Training Step: 5...  Training loss: 4.3354...  14.5475 sec/batch\n",
      "Epoch: 1/20...  Training Step: 6...  Training loss: 3.8991...  13.8566 sec/batch\n",
      "Epoch: 1/20...  Training Step: 7...  Training loss: 3.7242...  13.9260 sec/batch\n",
      "Epoch: 1/20...  Training Step: 8...  Training loss: 3.5782...  13.8914 sec/batch\n",
      "Epoch: 1/20...  Training Step: 9...  Training loss: 3.4858...  14.6064 sec/batch\n",
      "Epoch: 1/20...  Training Step: 10...  Training loss: 3.4521...  15.3699 sec/batch\n",
      "Epoch: 1/20...  Training Step: 11...  Training loss: 3.4171...  16.3956 sec/batch\n",
      "Epoch: 1/20...  Training Step: 12...  Training loss: 3.4012...  14.3758 sec/batch\n",
      "Epoch: 1/20...  Training Step: 13...  Training loss: 3.3717...  14.2741 sec/batch\n",
      "Epoch: 1/20...  Training Step: 14...  Training loss: 3.3753...  17.6353 sec/batch\n",
      "Epoch: 1/20...  Training Step: 15...  Training loss: 3.3353...  14.3465 sec/batch\n",
      "Epoch: 1/20...  Training Step: 16...  Training loss: 3.3114...  16.9124 sec/batch\n",
      "Epoch: 1/20...  Training Step: 17...  Training loss: 3.3053...  15.7761 sec/batch\n",
      "Epoch: 1/20...  Training Step: 18...  Training loss: 3.3259...  15.7281 sec/batch\n",
      "Epoch: 1/20...  Training Step: 19...  Training loss: 3.2994...  16.2114 sec/batch\n",
      "Epoch: 1/20...  Training Step: 20...  Training loss: 3.2430...  14.8255 sec/batch\n",
      "Epoch: 1/20...  Training Step: 21...  Training loss: 3.2665...  14.4069 sec/batch\n",
      "Epoch: 1/20...  Training Step: 22...  Training loss: 3.2570...  14.4242 sec/batch\n",
      "Epoch: 1/20...  Training Step: 23...  Training loss: 3.2540...  15.2952 sec/batch\n",
      "Epoch: 1/20...  Training Step: 24...  Training loss: 3.2463...  14.4698 sec/batch\n",
      "Epoch: 1/20...  Training Step: 25...  Training loss: 3.2327...  14.4873 sec/batch\n",
      "Epoch: 1/20...  Training Step: 26...  Training loss: 3.2507...  14.6199 sec/batch\n",
      "Epoch: 1/20...  Training Step: 27...  Training loss: 3.2405...  14.8910 sec/batch\n",
      "Epoch: 1/20...  Training Step: 28...  Training loss: 3.2089...  14.2361 sec/batch\n",
      "Epoch: 1/20...  Training Step: 29...  Training loss: 3.2115...  14.3727 sec/batch\n",
      "Epoch: 1/20...  Training Step: 30...  Training loss: 3.2188...  14.2146 sec/batch\n",
      "Epoch: 1/20...  Training Step: 31...  Training loss: 3.2318...  12.0953 sec/batch\n",
      "Epoch: 1/20...  Training Step: 32...  Training loss: 3.2055...  12.9868 sec/batch\n",
      "Epoch: 1/20...  Training Step: 33...  Training loss: 3.2020...  12.0984 sec/batch\n",
      "Epoch: 1/20...  Training Step: 34...  Training loss: 3.2154...  12.1551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 35...  Training loss: 3.1916...  13.2806 sec/batch\n",
      "Epoch: 1/20...  Training Step: 36...  Training loss: 3.2054...  14.2913 sec/batch\n",
      "Epoch: 1/20...  Training Step: 37...  Training loss: 3.1703...  14.3077 sec/batch\n",
      "Epoch: 1/20...  Training Step: 38...  Training loss: 3.1756...  14.2255 sec/batch\n",
      "Epoch: 1/20...  Training Step: 39...  Training loss: 3.1693...  14.2533 sec/batch\n",
      "Epoch: 1/20...  Training Step: 40...  Training loss: 3.1756...  15.3975 sec/batch\n",
      "Epoch: 1/20...  Training Step: 41...  Training loss: 3.1620...  14.5771 sec/batch\n",
      "Epoch: 1/20...  Training Step: 42...  Training loss: 3.1726...  14.3585 sec/batch\n",
      "Epoch: 1/20...  Training Step: 43...  Training loss: 3.1592...  14.0957 sec/batch\n",
      "Epoch: 1/20...  Training Step: 44...  Training loss: 3.1580...  12.8665 sec/batch\n",
      "Epoch: 1/20...  Training Step: 45...  Training loss: 3.1580...  12.8615 sec/batch\n",
      "Epoch: 1/20...  Training Step: 46...  Training loss: 3.1707...  12.6061 sec/batch\n",
      "Epoch: 1/20...  Training Step: 47...  Training loss: 3.1746...  13.4418 sec/batch\n",
      "Epoch: 1/20...  Training Step: 48...  Training loss: 3.1778...  12.5060 sec/batch\n",
      "Epoch: 1/20...  Training Step: 49...  Training loss: 3.1707...  12.8802 sec/batch\n",
      "Epoch: 1/20...  Training Step: 50...  Training loss: 3.1700...  12.5980 sec/batch\n",
      "Epoch: 1/20...  Training Step: 51...  Training loss: 3.1570...  12.6520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 52...  Training loss: 3.1518...  12.6141 sec/batch\n",
      "Epoch: 1/20...  Training Step: 53...  Training loss: 3.1613...  13.1119 sec/batch\n",
      "Epoch: 1/20...  Training Step: 54...  Training loss: 3.1423...  12.5445 sec/batch\n",
      "Epoch: 1/20...  Training Step: 55...  Training loss: 3.1530...  12.7410 sec/batch\n",
      "Epoch: 1/20...  Training Step: 56...  Training loss: 3.1358...  13.4929 sec/batch\n",
      "Epoch: 1/20...  Training Step: 57...  Training loss: 3.1440...  17.4194 sec/batch\n",
      "Epoch: 1/20...  Training Step: 58...  Training loss: 3.1457...  14.6135 sec/batch\n",
      "Epoch: 1/20...  Training Step: 59...  Training loss: 3.1302...  14.2753 sec/batch\n",
      "Epoch: 1/20...  Training Step: 60...  Training loss: 3.1406...  13.9770 sec/batch\n",
      "Epoch: 1/20...  Training Step: 61...  Training loss: 3.1474...  14.3993 sec/batch\n",
      "Epoch: 1/20...  Training Step: 62...  Training loss: 3.1595...  14.2730 sec/batch\n",
      "Epoch: 1/20...  Training Step: 63...  Training loss: 3.1635...  14.4099 sec/batch\n",
      "Epoch: 1/20...  Training Step: 64...  Training loss: 3.1193...  14.2598 sec/batch\n",
      "Epoch: 1/20...  Training Step: 65...  Training loss: 3.1256...  14.1437 sec/batch\n",
      "Epoch: 1/20...  Training Step: 66...  Training loss: 3.1648...  16.7623 sec/batch\n",
      "Epoch: 1/20...  Training Step: 67...  Training loss: 3.1491...  15.3538 sec/batch\n",
      "Epoch: 1/20...  Training Step: 68...  Training loss: 3.1043...  15.1795 sec/batch\n",
      "Epoch: 1/20...  Training Step: 69...  Training loss: 3.1262...  13.5409 sec/batch\n",
      "Epoch: 1/20...  Training Step: 70...  Training loss: 3.1403...  15.8482 sec/batch\n",
      "Epoch: 1/20...  Training Step: 71...  Training loss: 3.1313...  14.5913 sec/batch\n",
      "Epoch: 1/20...  Training Step: 72...  Training loss: 3.1498...  14.8633 sec/batch\n",
      "Epoch: 1/20...  Training Step: 73...  Training loss: 3.1319...  11.4323 sec/batch\n",
      "Epoch: 1/20...  Training Step: 74...  Training loss: 3.1371...  8.4085 sec/batch\n",
      "Epoch: 1/20...  Training Step: 75...  Training loss: 3.1413...  7.6186 sec/batch\n",
      "Epoch: 1/20...  Training Step: 76...  Training loss: 3.1380...  8.8478 sec/batch\n",
      "Epoch: 1/20...  Training Step: 77...  Training loss: 3.1319...  7.5450 sec/batch\n",
      "Epoch: 1/20...  Training Step: 78...  Training loss: 3.1270...  8.0128 sec/batch\n",
      "Epoch: 1/20...  Training Step: 79...  Training loss: 3.1234...  7.9030 sec/batch\n",
      "Epoch: 1/20...  Training Step: 80...  Training loss: 3.1078...  7.1451 sec/batch\n",
      "Epoch: 1/20...  Training Step: 81...  Training loss: 3.1126...  7.3765 sec/batch\n",
      "Epoch: 1/20...  Training Step: 82...  Training loss: 3.1284...  7.0512 sec/batch\n",
      "Epoch: 1/20...  Training Step: 83...  Training loss: 3.1228...  7.3334 sec/batch\n",
      "Epoch: 1/20...  Training Step: 84...  Training loss: 3.1154...  7.0915 sec/batch\n",
      "Epoch: 1/20...  Training Step: 85...  Training loss: 3.0969...  7.2799 sec/batch\n",
      "Epoch: 1/20...  Training Step: 86...  Training loss: 3.1032...  7.8834 sec/batch\n",
      "Epoch: 1/20...  Training Step: 87...  Training loss: 3.0961...  7.7223 sec/batch\n",
      "Epoch: 1/20...  Training Step: 88...  Training loss: 3.1012...  8.4105 sec/batch\n",
      "Epoch: 1/20...  Training Step: 89...  Training loss: 3.1155...  8.4238 sec/batch\n",
      "Epoch: 1/20...  Training Step: 90...  Training loss: 3.1106...  7.0612 sec/batch\n",
      "Epoch: 1/20...  Training Step: 91...  Training loss: 3.1042...  7.8936 sec/batch\n",
      "Epoch: 1/20...  Training Step: 92...  Training loss: 3.0967...  7.8627 sec/batch\n",
      "Epoch: 1/20...  Training Step: 93...  Training loss: 3.1004...  8.2554 sec/batch\n",
      "Epoch: 1/20...  Training Step: 94...  Training loss: 3.0961...  7.3453 sec/batch\n",
      "Epoch: 1/20...  Training Step: 95...  Training loss: 3.0885...  7.1803 sec/batch\n",
      "Epoch: 1/20...  Training Step: 96...  Training loss: 3.0784...  7.3629 sec/batch\n",
      "Epoch: 1/20...  Training Step: 97...  Training loss: 3.0936...  7.3407 sec/batch\n",
      "Epoch: 1/20...  Training Step: 98...  Training loss: 3.0762...  7.2469 sec/batch\n",
      "Epoch: 1/20...  Training Step: 99...  Training loss: 3.0809...  7.2477 sec/batch\n",
      "Epoch: 1/20...  Training Step: 100...  Training loss: 3.0728...  7.2599 sec/batch\n",
      "Epoch: 1/20...  Training Step: 101...  Training loss: 3.0726...  7.2859 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 102...  Training loss: 3.0627...  7.3297 sec/batch\n",
      "Epoch: 1/20...  Training Step: 103...  Training loss: 3.0867...  7.4399 sec/batch\n",
      "Epoch: 1/20...  Training Step: 104...  Training loss: 3.0748...  8.5007 sec/batch\n",
      "Epoch: 1/20...  Training Step: 105...  Training loss: 3.0929...  8.7160 sec/batch\n",
      "Epoch: 1/20...  Training Step: 106...  Training loss: 3.0865...  7.1440 sec/batch\n",
      "Epoch: 1/20...  Training Step: 107...  Training loss: 3.0486...  7.4718 sec/batch\n",
      "Epoch: 1/20...  Training Step: 108...  Training loss: 3.0572...  7.0819 sec/batch\n",
      "Epoch: 1/20...  Training Step: 109...  Training loss: 3.0634...  7.4083 sec/batch\n",
      "Epoch: 1/20...  Training Step: 110...  Training loss: 3.0376...  7.1628 sec/batch\n",
      "Epoch: 1/20...  Training Step: 111...  Training loss: 3.0476...  8.0659 sec/batch\n",
      "Epoch: 1/20...  Training Step: 112...  Training loss: 3.0507...  7.2936 sec/batch\n",
      "Epoch: 1/20...  Training Step: 113...  Training loss: 3.0317...  7.9053 sec/batch\n",
      "Epoch: 1/20...  Training Step: 114...  Training loss: 3.0167...  9.4933 sec/batch\n",
      "Epoch: 1/20...  Training Step: 115...  Training loss: 3.0189...  7.1358 sec/batch\n",
      "Epoch: 1/20...  Training Step: 116...  Training loss: 3.0132...  7.8282 sec/batch\n",
      "Epoch: 1/20...  Training Step: 117...  Training loss: 3.0068...  7.6313 sec/batch\n",
      "Epoch: 1/20...  Training Step: 118...  Training loss: 3.0145...  7.1251 sec/batch\n",
      "Epoch: 1/20...  Training Step: 119...  Training loss: 3.0168...  7.3161 sec/batch\n",
      "Epoch: 1/20...  Training Step: 120...  Training loss: 2.9884...  7.0433 sec/batch\n",
      "Epoch: 1/20...  Training Step: 121...  Training loss: 3.0205...  7.5310 sec/batch\n",
      "Epoch: 1/20...  Training Step: 122...  Training loss: 3.0064...  7.4377 sec/batch\n",
      "Epoch: 1/20...  Training Step: 123...  Training loss: 3.0016...  7.1563 sec/batch\n",
      "Epoch: 1/20...  Training Step: 124...  Training loss: 3.0077...  7.2063 sec/batch\n",
      "Epoch: 1/20...  Training Step: 125...  Training loss: 2.9810...  7.1312 sec/batch\n",
      "Epoch: 1/20...  Training Step: 126...  Training loss: 2.9623...  7.2111 sec/batch\n",
      "Epoch: 1/20...  Training Step: 127...  Training loss: 2.9813...  7.1193 sec/batch\n",
      "Epoch: 1/20...  Training Step: 128...  Training loss: 2.9895...  6.9650 sec/batch\n",
      "Epoch: 1/20...  Training Step: 129...  Training loss: 2.9567...  7.2803 sec/batch\n",
      "Epoch: 1/20...  Training Step: 130...  Training loss: 2.9614...  7.0806 sec/batch\n",
      "Epoch: 1/20...  Training Step: 131...  Training loss: 2.9565...  7.2379 sec/batch\n",
      "Epoch: 1/20...  Training Step: 132...  Training loss: 2.9243...  7.0129 sec/batch\n",
      "Epoch: 1/20...  Training Step: 133...  Training loss: 2.9250...  7.1591 sec/batch\n",
      "Epoch: 1/20...  Training Step: 134...  Training loss: 2.9225...  7.0574 sec/batch\n",
      "Epoch: 1/20...  Training Step: 135...  Training loss: 2.8995...  7.6133 sec/batch\n",
      "Epoch: 1/20...  Training Step: 136...  Training loss: 2.8818...  9.8880 sec/batch\n",
      "Epoch: 1/20...  Training Step: 137...  Training loss: 2.8994...  7.9860 sec/batch\n",
      "Epoch: 1/20...  Training Step: 138...  Training loss: 2.8749...  8.1920 sec/batch\n",
      "Epoch: 1/20...  Training Step: 139...  Training loss: 2.8996...  7.7858 sec/batch\n",
      "Epoch: 1/20...  Training Step: 140...  Training loss: 2.8743...  10.7016 sec/batch\n",
      "Epoch: 1/20...  Training Step: 141...  Training loss: 2.8788...  7.8666 sec/batch\n",
      "Epoch: 1/20...  Training Step: 142...  Training loss: 2.8401...  9.4268 sec/batch\n",
      "Epoch: 1/20...  Training Step: 143...  Training loss: 2.8330...  9.4599 sec/batch\n",
      "Epoch: 1/20...  Training Step: 144...  Training loss: 2.8203...  7.2492 sec/batch\n",
      "Epoch: 1/20...  Training Step: 145...  Training loss: 2.8281...  7.2223 sec/batch\n",
      "Epoch: 1/20...  Training Step: 146...  Training loss: 2.8199...  7.0158 sec/batch\n",
      "Epoch: 1/20...  Training Step: 147...  Training loss: 2.8189...  7.2461 sec/batch\n",
      "Epoch: 1/20...  Training Step: 148...  Training loss: 2.8318...  7.2719 sec/batch\n",
      "Epoch: 1/20...  Training Step: 149...  Training loss: 2.7782...  7.0648 sec/batch\n",
      "Epoch: 1/20...  Training Step: 150...  Training loss: 2.7821...  7.1862 sec/batch\n",
      "Epoch: 1/20...  Training Step: 151...  Training loss: 2.8284...  7.0515 sec/batch\n",
      "Epoch: 1/20...  Training Step: 152...  Training loss: 2.8352...  7.1763 sec/batch\n",
      "Epoch: 1/20...  Training Step: 153...  Training loss: 2.8063...  7.0858 sec/batch\n",
      "Epoch: 1/20...  Training Step: 154...  Training loss: 2.7856...  7.7360 sec/batch\n",
      "Epoch: 1/20...  Training Step: 155...  Training loss: 2.7518...  7.4923 sec/batch\n",
      "Epoch: 1/20...  Training Step: 156...  Training loss: 2.7602...  7.3214 sec/batch\n",
      "Epoch: 1/20...  Training Step: 157...  Training loss: 2.7191...  7.0904 sec/batch\n",
      "Epoch: 1/20...  Training Step: 158...  Training loss: 2.7341...  7.2225 sec/batch\n",
      "Epoch: 1/20...  Training Step: 159...  Training loss: 2.6973...  7.0044 sec/batch\n",
      "Epoch: 1/20...  Training Step: 160...  Training loss: 2.7375...  7.2352 sec/batch\n",
      "Epoch: 1/20...  Training Step: 161...  Training loss: 2.7073...  6.9935 sec/batch\n",
      "Epoch: 1/20...  Training Step: 162...  Training loss: 2.6692...  7.2375 sec/batch\n",
      "Epoch: 1/20...  Training Step: 163...  Training loss: 2.6659...  7.0093 sec/batch\n",
      "Epoch: 1/20...  Training Step: 164...  Training loss: 2.6781...  7.1827 sec/batch\n",
      "Epoch: 1/20...  Training Step: 165...  Training loss: 2.6706...  6.9577 sec/batch\n",
      "Epoch: 1/20...  Training Step: 166...  Training loss: 2.6532...  7.2348 sec/batch\n",
      "Epoch: 1/20...  Training Step: 167...  Training loss: 2.6590...  7.0234 sec/batch\n",
      "Epoch: 1/20...  Training Step: 168...  Training loss: 2.6341...  7.2295 sec/batch\n",
      "Epoch: 1/20...  Training Step: 169...  Training loss: 2.6365...  7.0105 sec/batch\n",
      "Epoch: 1/20...  Training Step: 170...  Training loss: 2.6030...  7.1272 sec/batch\n",
      "Epoch: 1/20...  Training Step: 171...  Training loss: 2.6324...  7.2106 sec/batch\n",
      "Epoch: 1/20...  Training Step: 172...  Training loss: 2.6875...  7.1067 sec/batch\n",
      "Epoch: 1/20...  Training Step: 173...  Training loss: 2.6659...  7.4057 sec/batch\n",
      "Epoch: 1/20...  Training Step: 174...  Training loss: 2.6580...  7.0075 sec/batch\n",
      "Epoch: 1/20...  Training Step: 175...  Training loss: 2.6432...  7.8350 sec/batch\n",
      "Epoch: 1/20...  Training Step: 176...  Training loss: 2.7108...  6.9259 sec/batch\n",
      "Epoch: 1/20...  Training Step: 177...  Training loss: 2.5937...  6.7475 sec/batch\n",
      "Epoch: 1/20...  Training Step: 178...  Training loss: 2.5763...  7.5384 sec/batch\n",
      "Epoch: 1/20...  Training Step: 179...  Training loss: 2.5738...  8.1708 sec/batch\n",
      "Epoch: 1/20...  Training Step: 180...  Training loss: 2.5561...  7.8274 sec/batch\n",
      "Epoch: 1/20...  Training Step: 181...  Training loss: 2.5654...  8.2751 sec/batch\n",
      "Epoch: 1/20...  Training Step: 182...  Training loss: 2.5683...  7.5046 sec/batch\n",
      "Epoch: 1/20...  Training Step: 183...  Training loss: 2.5418...  6.7496 sec/batch\n",
      "Epoch: 1/20...  Training Step: 184...  Training loss: 2.5745...  7.8725 sec/batch\n",
      "Epoch: 1/20...  Training Step: 185...  Training loss: 2.5762...  9.0574 sec/batch\n",
      "Epoch: 1/20...  Training Step: 186...  Training loss: 2.5347...  8.7768 sec/batch\n",
      "Epoch: 1/20...  Training Step: 187...  Training loss: 2.5125...  7.4341 sec/batch\n",
      "Epoch: 1/20...  Training Step: 188...  Training loss: 2.5045...  6.9119 sec/batch\n",
      "Epoch: 1/20...  Training Step: 189...  Training loss: 2.5076...  7.0240 sec/batch\n",
      "Epoch: 1/20...  Training Step: 190...  Training loss: 2.5052...  6.9938 sec/batch\n",
      "Epoch: 1/20...  Training Step: 191...  Training loss: 2.5186...  6.7650 sec/batch\n",
      "Epoch: 1/20...  Training Step: 192...  Training loss: 2.4796...  6.6328 sec/batch\n",
      "Epoch: 1/20...  Training Step: 193...  Training loss: 2.5099...  7.4294 sec/batch\n",
      "Epoch: 1/20...  Training Step: 194...  Training loss: 2.4954...  7.4257 sec/batch\n",
      "Epoch: 1/20...  Training Step: 195...  Training loss: 2.4900...  7.8329 sec/batch\n",
      "Epoch: 1/20...  Training Step: 196...  Training loss: 2.4883...  7.2421 sec/batch\n",
      "Epoch: 1/20...  Training Step: 197...  Training loss: 2.4819...  9.5050 sec/batch\n",
      "Epoch: 1/20...  Training Step: 198...  Training loss: 2.4623...  9.5207 sec/batch\n",
      "Epoch: 2/20...  Training Step: 199...  Training loss: 2.5459...  8.9612 sec/batch\n",
      "Epoch: 2/20...  Training Step: 200...  Training loss: 2.4459...  7.4191 sec/batch\n",
      "Epoch: 2/20...  Training Step: 201...  Training loss: 2.4596...  7.2386 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20...  Training Step: 202...  Training loss: 2.4756...  7.4055 sec/batch\n",
      "Epoch: 2/20...  Training Step: 203...  Training loss: 2.4692...  7.5801 sec/batch\n",
      "Epoch: 2/20...  Training Step: 204...  Training loss: 2.4679...  7.8619 sec/batch\n",
      "Epoch: 2/20...  Training Step: 205...  Training loss: 2.4645...  8.4519 sec/batch\n",
      "Epoch: 2/20...  Training Step: 206...  Training loss: 2.4616...  7.5153 sec/batch\n",
      "Epoch: 2/20...  Training Step: 207...  Training loss: 2.4836...  8.3518 sec/batch\n",
      "Epoch: 2/20...  Training Step: 208...  Training loss: 2.4477...  7.8676 sec/batch\n",
      "Epoch: 2/20...  Training Step: 209...  Training loss: 2.4393...  8.4925 sec/batch\n",
      "Epoch: 2/20...  Training Step: 210...  Training loss: 2.4542...  8.3312 sec/batch\n",
      "Epoch: 2/20...  Training Step: 211...  Training loss: 2.4383...  8.3959 sec/batch\n",
      "Epoch: 2/20...  Training Step: 212...  Training loss: 2.4610...  6.6928 sec/batch\n",
      "Epoch: 2/20...  Training Step: 213...  Training loss: 2.4462...  7.9418 sec/batch\n",
      "Epoch: 2/20...  Training Step: 214...  Training loss: 2.4402...  7.0201 sec/batch\n",
      "Epoch: 2/20...  Training Step: 215...  Training loss: 2.4420...  8.2497 sec/batch\n",
      "Epoch: 2/20...  Training Step: 216...  Training loss: 2.4758...  6.7157 sec/batch\n",
      "Epoch: 2/20...  Training Step: 217...  Training loss: 2.4394...  6.7674 sec/batch\n",
      "Epoch: 2/20...  Training Step: 218...  Training loss: 2.4107...  10.0037 sec/batch\n",
      "Epoch: 2/20...  Training Step: 219...  Training loss: 2.4143...  10.0495 sec/batch\n",
      "Epoch: 2/20...  Training Step: 220...  Training loss: 2.4514...  8.4709 sec/batch\n",
      "Epoch: 2/20...  Training Step: 221...  Training loss: 2.4166...  9.2807 sec/batch\n",
      "Epoch: 2/20...  Training Step: 222...  Training loss: 2.4105...  10.1111 sec/batch\n",
      "Epoch: 2/20...  Training Step: 223...  Training loss: 2.4070...  9.3584 sec/batch\n",
      "Epoch: 2/20...  Training Step: 224...  Training loss: 2.4137...  6.9920 sec/batch\n",
      "Epoch: 2/20...  Training Step: 225...  Training loss: 2.3993...  7.6083 sec/batch\n",
      "Epoch: 2/20...  Training Step: 226...  Training loss: 2.4032...  8.0456 sec/batch\n",
      "Epoch: 2/20...  Training Step: 227...  Training loss: 2.4230...  8.9991 sec/batch\n",
      "Epoch: 2/20...  Training Step: 228...  Training loss: 2.4093...  8.5338 sec/batch\n",
      "Epoch: 2/20...  Training Step: 229...  Training loss: 2.4174...  9.6513 sec/batch\n",
      "Epoch: 2/20...  Training Step: 230...  Training loss: 2.3845...  7.7247 sec/batch\n",
      "Epoch: 2/20...  Training Step: 231...  Training loss: 2.3782...  7.2740 sec/batch\n",
      "Epoch: 2/20...  Training Step: 232...  Training loss: 2.4041...  9.0473 sec/batch\n",
      "Epoch: 2/20...  Training Step: 233...  Training loss: 2.3765...  10.2681 sec/batch\n",
      "Epoch: 2/20...  Training Step: 234...  Training loss: 2.3951...  7.6081 sec/batch\n",
      "Epoch: 2/20...  Training Step: 235...  Training loss: 2.3791...  8.0359 sec/batch\n",
      "Epoch: 2/20...  Training Step: 236...  Training loss: 2.3534...  7.4238 sec/batch\n",
      "Epoch: 2/20...  Training Step: 237...  Training loss: 2.3615...  7.4188 sec/batch\n",
      "Epoch: 2/20...  Training Step: 238...  Training loss: 2.3674...  7.3206 sec/batch\n",
      "Epoch: 2/20...  Training Step: 239...  Training loss: 2.3519...  9.0183 sec/batch\n",
      "Epoch: 2/20...  Training Step: 240...  Training loss: 2.3620...  11.0299 sec/batch\n",
      "Epoch: 2/20...  Training Step: 241...  Training loss: 2.3580...  7.0715 sec/batch\n",
      "Epoch: 2/20...  Training Step: 242...  Training loss: 2.3454...  7.4904 sec/batch\n",
      "Epoch: 2/20...  Training Step: 243...  Training loss: 2.3694...  7.7906 sec/batch\n",
      "Epoch: 2/20...  Training Step: 244...  Training loss: 2.3103...  8.2982 sec/batch\n",
      "Epoch: 2/20...  Training Step: 245...  Training loss: 2.3764...  8.0257 sec/batch\n",
      "Epoch: 2/20...  Training Step: 246...  Training loss: 2.3458...  7.9244 sec/batch\n",
      "Epoch: 2/20...  Training Step: 247...  Training loss: 2.3467...  7.2556 sec/batch\n",
      "Epoch: 2/20...  Training Step: 248...  Training loss: 2.3672...  7.5933 sec/batch\n",
      "Epoch: 2/20...  Training Step: 249...  Training loss: 2.3321...  6.7418 sec/batch\n",
      "Epoch: 2/20...  Training Step: 250...  Training loss: 2.3609...  6.6330 sec/batch\n",
      "Epoch: 2/20...  Training Step: 251...  Training loss: 2.3381...  6.5831 sec/batch\n",
      "Epoch: 2/20...  Training Step: 252...  Training loss: 2.3262...  7.8729 sec/batch\n",
      "Epoch: 2/20...  Training Step: 253...  Training loss: 2.3344...  9.4605 sec/batch\n",
      "Epoch: 2/20...  Training Step: 254...  Training loss: 2.3412...  8.4379 sec/batch\n",
      "Epoch: 2/20...  Training Step: 255...  Training loss: 2.3379...  6.9015 sec/batch\n",
      "Epoch: 2/20...  Training Step: 256...  Training loss: 2.3199...  7.2133 sec/batch\n",
      "Epoch: 2/20...  Training Step: 257...  Training loss: 2.3217...  8.3533 sec/batch\n",
      "Epoch: 2/20...  Training Step: 258...  Training loss: 2.3423...  7.8753 sec/batch\n",
      "Epoch: 2/20...  Training Step: 259...  Training loss: 2.3163...  7.3055 sec/batch\n",
      "Epoch: 2/20...  Training Step: 260...  Training loss: 2.3372...  7.1144 sec/batch\n",
      "Epoch: 2/20...  Training Step: 261...  Training loss: 2.3370...  8.5267 sec/batch\n",
      "Epoch: 2/20...  Training Step: 262...  Training loss: 2.3096...  9.4744 sec/batch\n",
      "Epoch: 2/20...  Training Step: 263...  Training loss: 2.2979...  8.8103 sec/batch\n",
      "Epoch: 2/20...  Training Step: 264...  Training loss: 2.3337...  7.8704 sec/batch\n",
      "Epoch: 2/20...  Training Step: 265...  Training loss: 2.3047...  6.7547 sec/batch\n",
      "Epoch: 2/20...  Training Step: 266...  Training loss: 2.2787...  6.5146 sec/batch\n",
      "Epoch: 2/20...  Training Step: 267...  Training loss: 2.2836...  7.0397 sec/batch\n",
      "Epoch: 2/20...  Training Step: 268...  Training loss: 2.3042...  6.9371 sec/batch\n",
      "Epoch: 2/20...  Training Step: 269...  Training loss: 2.3186...  6.9681 sec/batch\n",
      "Epoch: 2/20...  Training Step: 270...  Training loss: 2.3095...  7.0976 sec/batch\n",
      "Epoch: 2/20...  Training Step: 271...  Training loss: 2.3075...  6.8857 sec/batch\n",
      "Epoch: 2/20...  Training Step: 272...  Training loss: 2.2876...  6.9915 sec/batch\n",
      "Epoch: 2/20...  Training Step: 273...  Training loss: 2.2864...  6.2761 sec/batch\n",
      "Epoch: 2/20...  Training Step: 274...  Training loss: 2.3279...  6.3860 sec/batch\n",
      "Epoch: 2/20...  Training Step: 275...  Training loss: 2.2921...  6.3631 sec/batch\n",
      "Epoch: 2/20...  Training Step: 276...  Training loss: 2.2998...  6.3148 sec/batch\n",
      "Epoch: 2/20...  Training Step: 277...  Training loss: 2.2750...  6.3904 sec/batch\n",
      "Epoch: 2/20...  Training Step: 278...  Training loss: 2.2693...  6.2673 sec/batch\n",
      "Epoch: 2/20...  Training Step: 279...  Training loss: 2.2548...  6.2370 sec/batch\n",
      "Epoch: 2/20...  Training Step: 280...  Training loss: 2.2902...  6.4269 sec/batch\n",
      "Epoch: 2/20...  Training Step: 281...  Training loss: 2.2524...  6.5091 sec/batch\n",
      "Epoch: 2/20...  Training Step: 282...  Training loss: 2.2493...  6.3373 sec/batch\n",
      "Epoch: 2/20...  Training Step: 283...  Training loss: 2.2363...  6.4835 sec/batch\n",
      "Epoch: 2/20...  Training Step: 284...  Training loss: 2.2597...  7.0952 sec/batch\n",
      "Epoch: 2/20...  Training Step: 285...  Training loss: 2.2663...  6.7775 sec/batch\n",
      "Epoch: 2/20...  Training Step: 286...  Training loss: 2.2600...  9.7085 sec/batch\n",
      "Epoch: 2/20...  Training Step: 287...  Training loss: 2.2365...  8.7122 sec/batch\n",
      "Epoch: 2/20...  Training Step: 288...  Training loss: 2.2694...  7.9918 sec/batch\n",
      "Epoch: 2/20...  Training Step: 289...  Training loss: 2.2309...  7.3443 sec/batch\n",
      "Epoch: 2/20...  Training Step: 290...  Training loss: 2.2606...  6.8772 sec/batch\n",
      "Epoch: 2/20...  Training Step: 291...  Training loss: 2.2439...  6.9926 sec/batch\n",
      "Epoch: 2/20...  Training Step: 292...  Training loss: 2.2260...  7.3542 sec/batch\n",
      "Epoch: 2/20...  Training Step: 293...  Training loss: 2.2323...  7.3354 sec/batch\n",
      "Epoch: 2/20...  Training Step: 294...  Training loss: 2.2365...  7.8640 sec/batch\n",
      "Epoch: 2/20...  Training Step: 295...  Training loss: 2.2438...  8.0719 sec/batch\n",
      "Epoch: 2/20...  Training Step: 296...  Training loss: 2.2354...  7.1995 sec/batch\n",
      "Epoch: 2/20...  Training Step: 297...  Training loss: 2.2187...  7.3631 sec/batch\n",
      "Epoch: 2/20...  Training Step: 298...  Training loss: 2.2101...  7.4744 sec/batch\n",
      "Epoch: 2/20...  Training Step: 299...  Training loss: 2.2455...  7.4484 sec/batch\n",
      "Epoch: 2/20...  Training Step: 300...  Training loss: 2.2379...  7.4895 sec/batch\n",
      "Epoch: 2/20...  Training Step: 301...  Training loss: 2.2168...  7.2043 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20...  Training Step: 302...  Training loss: 2.2227...  7.3762 sec/batch\n",
      "Epoch: 2/20...  Training Step: 303...  Training loss: 2.2127...  7.1084 sec/batch\n",
      "Epoch: 2/20...  Training Step: 304...  Training loss: 2.2324...  7.0273 sec/batch\n",
      "Epoch: 2/20...  Training Step: 305...  Training loss: 2.2236...  6.8115 sec/batch\n",
      "Epoch: 2/20...  Training Step: 306...  Training loss: 2.2471...  7.6326 sec/batch\n",
      "Epoch: 2/20...  Training Step: 307...  Training loss: 2.2396...  7.0125 sec/batch\n",
      "Epoch: 2/20...  Training Step: 308...  Training loss: 2.2151...  6.3983 sec/batch\n",
      "Epoch: 2/20...  Training Step: 309...  Training loss: 2.2244...  6.5743 sec/batch\n",
      "Epoch: 2/20...  Training Step: 310...  Training loss: 2.2337...  6.9924 sec/batch\n",
      "Epoch: 2/20...  Training Step: 311...  Training loss: 2.2063...  7.4768 sec/batch\n",
      "Epoch: 2/20...  Training Step: 312...  Training loss: 2.2047...  6.8101 sec/batch\n",
      "Epoch: 2/20...  Training Step: 313...  Training loss: 2.2100...  8.0284 sec/batch\n",
      "Epoch: 2/20...  Training Step: 314...  Training loss: 2.1787...  7.3290 sec/batch\n",
      "Epoch: 2/20...  Training Step: 315...  Training loss: 2.2146...  7.7010 sec/batch\n",
      "Epoch: 2/20...  Training Step: 316...  Training loss: 2.2025...  7.7767 sec/batch\n",
      "Epoch: 2/20...  Training Step: 317...  Training loss: 2.2296...  9.5145 sec/batch\n",
      "Epoch: 2/20...  Training Step: 318...  Training loss: 2.2105...  8.0582 sec/batch\n",
      "Epoch: 2/20...  Training Step: 319...  Training loss: 2.2268...  7.2646 sec/batch\n",
      "Epoch: 2/20...  Training Step: 320...  Training loss: 2.1844...  7.1622 sec/batch\n",
      "Epoch: 2/20...  Training Step: 321...  Training loss: 2.1847...  7.4099 sec/batch\n",
      "Epoch: 2/20...  Training Step: 322...  Training loss: 2.2272...  7.3567 sec/batch\n",
      "Epoch: 2/20...  Training Step: 323...  Training loss: 2.1993...  7.4417 sec/batch\n",
      "Epoch: 2/20...  Training Step: 324...  Training loss: 2.1634...  6.9988 sec/batch\n",
      "Epoch: 2/20...  Training Step: 325...  Training loss: 2.2085...  7.0699 sec/batch\n",
      "Epoch: 2/20...  Training Step: 326...  Training loss: 2.1969...  7.0052 sec/batch\n",
      "Epoch: 2/20...  Training Step: 327...  Training loss: 2.1956...  7.9374 sec/batch\n",
      "Epoch: 2/20...  Training Step: 328...  Training loss: 2.1922...  8.1059 sec/batch\n",
      "Epoch: 2/20...  Training Step: 329...  Training loss: 2.1781...  7.7420 sec/batch\n",
      "Epoch: 2/20...  Training Step: 330...  Training loss: 2.1593...  7.1192 sec/batch\n",
      "Epoch: 2/20...  Training Step: 331...  Training loss: 2.1905...  6.7951 sec/batch\n",
      "Epoch: 2/20...  Training Step: 332...  Training loss: 2.1993...  6.8814 sec/batch\n",
      "Epoch: 2/20...  Training Step: 333...  Training loss: 2.1773...  7.3555 sec/batch\n",
      "Epoch: 2/20...  Training Step: 334...  Training loss: 2.1878...  7.2831 sec/batch\n",
      "Epoch: 2/20...  Training Step: 335...  Training loss: 2.1887...  6.5847 sec/batch\n",
      "Epoch: 2/20...  Training Step: 336...  Training loss: 2.1837...  7.1975 sec/batch\n",
      "Epoch: 2/20...  Training Step: 337...  Training loss: 2.2108...  9.3357 sec/batch\n",
      "Epoch: 2/20...  Training Step: 338...  Training loss: 2.1626...  7.3415 sec/batch\n",
      "Epoch: 2/20...  Training Step: 339...  Training loss: 2.1940...  6.8864 sec/batch\n",
      "Epoch: 2/20...  Training Step: 340...  Training loss: 2.1605...  7.4720 sec/batch\n",
      "Epoch: 2/20...  Training Step: 341...  Training loss: 2.1678...  7.1225 sec/batch\n",
      "Epoch: 2/20...  Training Step: 342...  Training loss: 2.1659...  6.4478 sec/batch\n",
      "Epoch: 2/20...  Training Step: 343...  Training loss: 2.1577...  6.8005 sec/batch\n",
      "Epoch: 2/20...  Training Step: 344...  Training loss: 2.1927...  6.8367 sec/batch\n",
      "Epoch: 2/20...  Training Step: 345...  Training loss: 2.1783...  6.9201 sec/batch\n",
      "Epoch: 2/20...  Training Step: 346...  Training loss: 2.1905...  6.6244 sec/batch\n",
      "Epoch: 2/20...  Training Step: 347...  Training loss: 2.1636...  6.8510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 348...  Training loss: 2.1489...  8.9372 sec/batch\n",
      "Epoch: 2/20...  Training Step: 349...  Training loss: 2.1703...  7.3009 sec/batch\n",
      "Epoch: 2/20...  Training Step: 350...  Training loss: 2.1976...  7.5229 sec/batch\n",
      "Epoch: 2/20...  Training Step: 351...  Training loss: 2.1673...  6.4209 sec/batch\n",
      "Epoch: 2/20...  Training Step: 352...  Training loss: 2.1648...  6.2989 sec/batch\n",
      "Epoch: 2/20...  Training Step: 353...  Training loss: 2.1414...  527.1762 sec/batch\n",
      "Epoch: 2/20...  Training Step: 354...  Training loss: 2.1438...  6.4094 sec/batch\n",
      "Epoch: 2/20...  Training Step: 355...  Training loss: 2.1390...  8.8709 sec/batch\n",
      "Epoch: 2/20...  Training Step: 356...  Training loss: 2.1436...  9.6247 sec/batch\n",
      "Epoch: 2/20...  Training Step: 357...  Training loss: 2.1154...  797.6929 sec/batch\n",
      "Epoch: 2/20...  Training Step: 358...  Training loss: 2.1837...  6.2084 sec/batch\n",
      "Epoch: 2/20...  Training Step: 359...  Training loss: 2.1601...  6.8584 sec/batch\n",
      "Epoch: 2/20...  Training Step: 360...  Training loss: 2.1415...  8.6980 sec/batch\n",
      "Epoch: 2/20...  Training Step: 361...  Training loss: 2.1398...  8.5011 sec/batch\n",
      "Epoch: 2/20...  Training Step: 362...  Training loss: 2.1418...  8.4818 sec/batch\n",
      "Epoch: 2/20...  Training Step: 363...  Training loss: 2.1433...  8.4293 sec/batch\n",
      "Epoch: 2/20...  Training Step: 364...  Training loss: 2.1301...  8.6408 sec/batch\n",
      "Epoch: 2/20...  Training Step: 365...  Training loss: 2.1470...  8.5106 sec/batch\n",
      "Epoch: 2/20...  Training Step: 366...  Training loss: 2.1634...  8.6108 sec/batch\n",
      "Epoch: 2/20...  Training Step: 367...  Training loss: 2.1306...  141.7667 sec/batch\n",
      "Epoch: 2/20...  Training Step: 368...  Training loss: 2.1235...  6.5401 sec/batch\n",
      "Epoch: 2/20...  Training Step: 369...  Training loss: 2.1228...  7.1192 sec/batch\n",
      "Epoch: 2/20...  Training Step: 370...  Training loss: 2.1294...  8.7111 sec/batch\n",
      "Epoch: 2/20...  Training Step: 371...  Training loss: 2.1461...  9.1226 sec/batch\n",
      "Epoch: 2/20...  Training Step: 372...  Training loss: 2.1522...  380.7526 sec/batch\n",
      "Epoch: 2/20...  Training Step: 373...  Training loss: 2.1381...  6.2643 sec/batch\n",
      "Epoch: 2/20...  Training Step: 374...  Training loss: 2.1312...  8.2470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 375...  Training loss: 2.1100...  8.9912 sec/batch\n",
      "Epoch: 2/20...  Training Step: 376...  Training loss: 2.1235...  29.2237 sec/batch\n",
      "Epoch: 2/20...  Training Step: 377...  Training loss: 2.0972...  7.2236 sec/batch\n",
      "Epoch: 2/20...  Training Step: 378...  Training loss: 2.0796...  8.9029 sec/batch\n",
      "Epoch: 2/20...  Training Step: 379...  Training loss: 2.0967...  10.5260 sec/batch\n",
      "Epoch: 2/20...  Training Step: 380...  Training loss: 2.1150...  58.1062 sec/batch\n",
      "Epoch: 2/20...  Training Step: 381...  Training loss: 2.1161...  6.8389 sec/batch\n",
      "Epoch: 2/20...  Training Step: 382...  Training loss: 2.1290...  8.3963 sec/batch\n",
      "Epoch: 2/20...  Training Step: 383...  Training loss: 2.1097...  9.2604 sec/batch\n",
      "Epoch: 2/20...  Training Step: 384...  Training loss: 2.0966...  38.8081 sec/batch\n",
      "Epoch: 2/20...  Training Step: 385...  Training loss: 2.1020...  8.5356 sec/batch\n",
      "Epoch: 2/20...  Training Step: 386...  Training loss: 2.0883...  7.0682 sec/batch\n",
      "Epoch: 2/20...  Training Step: 387...  Training loss: 2.0879...  6.2187 sec/batch\n",
      "Epoch: 2/20...  Training Step: 388...  Training loss: 2.1043...  6.2728 sec/batch\n",
      "Epoch: 2/20...  Training Step: 389...  Training loss: 2.1141...  6.1685 sec/batch\n",
      "Epoch: 2/20...  Training Step: 390...  Training loss: 2.0770...  6.2292 sec/batch\n",
      "Epoch: 2/20...  Training Step: 391...  Training loss: 2.1022...  6.2470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 392...  Training loss: 2.0979...  6.1726 sec/batch\n",
      "Epoch: 2/20...  Training Step: 393...  Training loss: 2.0703...  6.3646 sec/batch\n",
      "Epoch: 2/20...  Training Step: 394...  Training loss: 2.0929...  6.2346 sec/batch\n",
      "Epoch: 2/20...  Training Step: 395...  Training loss: 2.0961...  6.2656 sec/batch\n",
      "Epoch: 2/20...  Training Step: 396...  Training loss: 2.0664...  6.1794 sec/batch\n",
      "Epoch: 3/20...  Training Step: 397...  Training loss: 2.1625...  6.1345 sec/batch\n",
      "Epoch: 3/20...  Training Step: 398...  Training loss: 2.0763...  6.2066 sec/batch\n",
      "Epoch: 3/20...  Training Step: 399...  Training loss: 2.0729...  6.0949 sec/batch\n",
      "Epoch: 3/20...  Training Step: 400...  Training loss: 2.0903...  6.2446 sec/batch\n",
      "Epoch: 3/20...  Training Step: 401...  Training loss: 2.0833...  6.7174 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20...  Training Step: 402...  Training loss: 2.0555...  6.2456 sec/batch\n",
      "Epoch: 3/20...  Training Step: 403...  Training loss: 2.0860...  6.1584 sec/batch\n",
      "Epoch: 3/20...  Training Step: 404...  Training loss: 2.0917...  6.2156 sec/batch\n",
      "Epoch: 3/20...  Training Step: 405...  Training loss: 2.1255...  7.6265 sec/batch\n",
      "Epoch: 3/20...  Training Step: 406...  Training loss: 2.0724...  9.9695 sec/batch\n",
      "Epoch: 3/20...  Training Step: 407...  Training loss: 2.0729...  9.7035 sec/batch\n",
      "Epoch: 3/20...  Training Step: 408...  Training loss: 2.0642...  9.9070 sec/batch\n",
      "Epoch: 3/20...  Training Step: 409...  Training loss: 2.0708...  10.5975 sec/batch\n",
      "Epoch: 3/20...  Training Step: 410...  Training loss: 2.1082...  9.4385 sec/batch\n",
      "Epoch: 3/20...  Training Step: 411...  Training loss: 2.0691...  9.6336 sec/batch\n",
      "Epoch: 3/20...  Training Step: 412...  Training loss: 2.0518...  8.7498 sec/batch\n",
      "Epoch: 3/20...  Training Step: 413...  Training loss: 2.0609...  12.2454 sec/batch\n",
      "Epoch: 3/20...  Training Step: 414...  Training loss: 2.1113...  7.2806 sec/batch\n",
      "Epoch: 3/20...  Training Step: 415...  Training loss: 2.0745...  6.7590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 416...  Training loss: 2.0631...  6.8270 sec/batch\n",
      "Epoch: 3/20...  Training Step: 417...  Training loss: 2.0505...  8.1116 sec/batch\n",
      "Epoch: 3/20...  Training Step: 418...  Training loss: 2.1110...  8.8934 sec/batch\n",
      "Epoch: 3/20...  Training Step: 419...  Training loss: 2.0658...  7.0019 sec/batch\n",
      "Epoch: 3/20...  Training Step: 420...  Training loss: 2.0567...  7.7690 sec/batch\n",
      "Epoch: 3/20...  Training Step: 421...  Training loss: 2.0730...  8.4423 sec/batch\n",
      "Epoch: 3/20...  Training Step: 422...  Training loss: 2.0358...  8.6634 sec/batch\n",
      "Epoch: 3/20...  Training Step: 423...  Training loss: 2.0444...  10.3464 sec/batch\n",
      "Epoch: 3/20...  Training Step: 424...  Training loss: 2.0600...  9.9047 sec/batch\n",
      "Epoch: 3/20...  Training Step: 425...  Training loss: 2.0904...  9.3716 sec/batch\n",
      "Epoch: 3/20...  Training Step: 426...  Training loss: 2.0672...  9.6170 sec/batch\n",
      "Epoch: 3/20...  Training Step: 427...  Training loss: 2.0574...  9.0202 sec/batch\n",
      "Epoch: 3/20...  Training Step: 428...  Training loss: 2.0273...  9.3235 sec/batch\n",
      "Epoch: 3/20...  Training Step: 429...  Training loss: 2.0524...  9.2972 sec/batch\n",
      "Epoch: 3/20...  Training Step: 430...  Training loss: 2.0797...  8.9606 sec/batch\n",
      "Epoch: 3/20...  Training Step: 431...  Training loss: 2.0397...  8.6636 sec/batch\n",
      "Epoch: 3/20...  Training Step: 432...  Training loss: 2.0434...  8.4365 sec/batch\n",
      "Epoch: 3/20...  Training Step: 433...  Training loss: 2.0436...  6.8808 sec/batch\n",
      "Epoch: 3/20...  Training Step: 434...  Training loss: 2.0083...  8.6792 sec/batch\n",
      "Epoch: 3/20...  Training Step: 435...  Training loss: 1.9990...  7.4866 sec/batch\n",
      "Epoch: 3/20...  Training Step: 436...  Training loss: 2.0129...  8.7167 sec/batch\n",
      "Epoch: 3/20...  Training Step: 437...  Training loss: 2.0226...  8.5253 sec/batch\n",
      "Epoch: 3/20...  Training Step: 438...  Training loss: 2.0362...  6.5067 sec/batch\n",
      "Epoch: 3/20...  Training Step: 439...  Training loss: 2.0176...  6.3095 sec/batch\n",
      "Epoch: 3/20...  Training Step: 440...  Training loss: 2.0135...  6.8858 sec/batch\n",
      "Epoch: 3/20...  Training Step: 441...  Training loss: 2.0405...  6.4035 sec/batch\n",
      "Epoch: 3/20...  Training Step: 442...  Training loss: 1.9826...  6.3450 sec/batch\n",
      "Epoch: 3/20...  Training Step: 443...  Training loss: 2.0343...  6.2966 sec/batch\n",
      "Epoch: 3/20...  Training Step: 444...  Training loss: 2.0077...  6.3897 sec/batch\n",
      "Epoch: 3/20...  Training Step: 445...  Training loss: 2.0207...  6.2256 sec/batch\n",
      "Epoch: 3/20...  Training Step: 446...  Training loss: 2.0635...  6.2214 sec/batch\n",
      "Epoch: 3/20...  Training Step: 447...  Training loss: 1.9981...  6.2520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 448...  Training loss: 2.0741...  6.2718 sec/batch\n",
      "Epoch: 3/20...  Training Step: 449...  Training loss: 2.0145...  6.3225 sec/batch\n",
      "Epoch: 3/20...  Training Step: 450...  Training loss: 2.0192...  7.4978 sec/batch\n",
      "Epoch: 3/20...  Training Step: 451...  Training loss: 2.0059...  6.1467 sec/batch\n",
      "Epoch: 3/20...  Training Step: 452...  Training loss: 2.0291...  6.2406 sec/batch\n",
      "Epoch: 3/20...  Training Step: 453...  Training loss: 2.0217...  6.1747 sec/batch\n",
      "Epoch: 3/20...  Training Step: 454...  Training loss: 2.0067...  8.2883 sec/batch\n",
      "Epoch: 3/20...  Training Step: 455...  Training loss: 2.0051...  7.6268 sec/batch\n",
      "Epoch: 3/20...  Training Step: 456...  Training loss: 2.0512...  7.0100 sec/batch\n",
      "Epoch: 3/20...  Training Step: 457...  Training loss: 2.0130...  8.2034 sec/batch\n",
      "Epoch: 3/20...  Training Step: 458...  Training loss: 2.0469...  9.6432 sec/batch\n",
      "Epoch: 3/20...  Training Step: 459...  Training loss: 2.0397...  12.0747 sec/batch\n",
      "Epoch: 3/20...  Training Step: 460...  Training loss: 2.0212...  9.4276 sec/batch\n",
      "Epoch: 3/20...  Training Step: 461...  Training loss: 2.0115...  10.9285 sec/batch\n",
      "Epoch: 3/20...  Training Step: 462...  Training loss: 2.0366...  12.3691 sec/batch\n",
      "Epoch: 3/20...  Training Step: 463...  Training loss: 2.0166...  11.9983 sec/batch\n",
      "Epoch: 3/20...  Training Step: 464...  Training loss: 1.9819...  9.1194 sec/batch\n",
      "Epoch: 3/20...  Training Step: 465...  Training loss: 1.9915...  8.4639 sec/batch\n",
      "Epoch: 3/20...  Training Step: 466...  Training loss: 2.0076...  8.3008 sec/batch\n",
      "Epoch: 3/20...  Training Step: 467...  Training loss: 2.0431...  8.3137 sec/batch\n",
      "Epoch: 3/20...  Training Step: 468...  Training loss: 2.0145...  8.2442 sec/batch\n",
      "Epoch: 3/20...  Training Step: 469...  Training loss: 2.0178...  8.3399 sec/batch\n",
      "Epoch: 3/20...  Training Step: 470...  Training loss: 1.9876...  8.1287 sec/batch\n",
      "Epoch: 3/20...  Training Step: 471...  Training loss: 1.9922...  7.8996 sec/batch\n",
      "Epoch: 3/20...  Training Step: 472...  Training loss: 2.0324...  8.4664 sec/batch\n",
      "Epoch: 3/20...  Training Step: 473...  Training loss: 1.9917...  7.0271 sec/batch\n",
      "Epoch: 3/20...  Training Step: 474...  Training loss: 1.9988...  6.3305 sec/batch\n",
      "Epoch: 3/20...  Training Step: 475...  Training loss: 1.9621...  6.1493 sec/batch\n",
      "Epoch: 3/20...  Training Step: 476...  Training loss: 1.9757...  6.2518 sec/batch\n",
      "Epoch: 3/20...  Training Step: 477...  Training loss: 1.9581...  6.1375 sec/batch\n",
      "Epoch: 3/20...  Training Step: 478...  Training loss: 2.0109...  6.1476 sec/batch\n",
      "Epoch: 3/20...  Training Step: 479...  Training loss: 1.9552...  6.1696 sec/batch\n",
      "Epoch: 3/20...  Training Step: 480...  Training loss: 1.9708...  6.1714 sec/batch\n",
      "Epoch: 3/20...  Training Step: 481...  Training loss: 1.9404...  6.1817 sec/batch\n",
      "Epoch: 3/20...  Training Step: 482...  Training loss: 1.9604...  6.1247 sec/batch\n",
      "Epoch: 3/20...  Training Step: 483...  Training loss: 1.9724...  6.0658 sec/batch\n",
      "Epoch: 3/20...  Training Step: 484...  Training loss: 1.9661...  6.7346 sec/batch\n",
      "Epoch: 3/20...  Training Step: 485...  Training loss: 1.9528...  6.1534 sec/batch\n",
      "Epoch: 3/20...  Training Step: 486...  Training loss: 1.9894...  6.1572 sec/batch\n",
      "Epoch: 3/20...  Training Step: 487...  Training loss: 1.9550...  6.1321 sec/batch\n",
      "Epoch: 3/20...  Training Step: 488...  Training loss: 1.9780...  6.2020 sec/batch\n",
      "Epoch: 3/20...  Training Step: 489...  Training loss: 1.9483...  6.1113 sec/batch\n",
      "Epoch: 3/20...  Training Step: 490...  Training loss: 1.9599...  6.1496 sec/batch\n",
      "Epoch: 3/20...  Training Step: 491...  Training loss: 1.9524...  6.2132 sec/batch\n",
      "Epoch: 3/20...  Training Step: 492...  Training loss: 1.9708...  6.2411 sec/batch\n",
      "Epoch: 3/20...  Training Step: 493...  Training loss: 1.9768...  6.4948 sec/batch\n",
      "Epoch: 3/20...  Training Step: 494...  Training loss: 1.9526...  6.3142 sec/batch\n",
      "Epoch: 3/20...  Training Step: 495...  Training loss: 1.9512...  6.2765 sec/batch\n",
      "Epoch: 3/20...  Training Step: 496...  Training loss: 1.9297...  6.3109 sec/batch\n",
      "Epoch: 3/20...  Training Step: 497...  Training loss: 1.9806...  6.7063 sec/batch\n",
      "Epoch: 3/20...  Training Step: 498...  Training loss: 1.9686...  7.4149 sec/batch\n",
      "Epoch: 3/20...  Training Step: 499...  Training loss: 1.9569...  6.8813 sec/batch\n",
      "Epoch: 3/20...  Training Step: 500...  Training loss: 1.9576...  7.5718 sec/batch\n",
      "Epoch: 3/20...  Training Step: 501...  Training loss: 1.9446...  8.9849 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20...  Training Step: 502...  Training loss: 1.9715...  7.1559 sec/batch\n",
      "Epoch: 3/20...  Training Step: 503...  Training loss: 1.9561...  7.3035 sec/batch\n",
      "Epoch: 3/20...  Training Step: 504...  Training loss: 1.9756...  7.2287 sec/batch\n",
      "Epoch: 3/20...  Training Step: 505...  Training loss: 1.9837...  6.9993 sec/batch\n",
      "Epoch: 3/20...  Training Step: 506...  Training loss: 1.9652...  6.2196 sec/batch\n",
      "Epoch: 3/20...  Training Step: 507...  Training loss: 1.9617...  6.2051 sec/batch\n",
      "Epoch: 3/20...  Training Step: 508...  Training loss: 1.9466...  6.2711 sec/batch\n",
      "Epoch: 3/20...  Training Step: 509...  Training loss: 1.9529...  6.1416 sec/batch\n",
      "Epoch: 3/20...  Training Step: 510...  Training loss: 1.9459...  6.2759 sec/batch\n",
      "Epoch: 3/20...  Training Step: 511...  Training loss: 1.9345...  8.3751 sec/batch\n",
      "Epoch: 3/20...  Training Step: 512...  Training loss: 1.9104...  7.9345 sec/batch\n",
      "Epoch: 3/20...  Training Step: 513...  Training loss: 1.9557...  10.1128 sec/batch\n",
      "Epoch: 3/20...  Training Step: 514...  Training loss: 1.9381...  8.2794 sec/batch\n",
      "Epoch: 3/20...  Training Step: 515...  Training loss: 1.9483...  8.4006 sec/batch\n",
      "Epoch: 3/20...  Training Step: 516...  Training loss: 1.9457...  7.2256 sec/batch\n",
      "Epoch: 3/20...  Training Step: 517...  Training loss: 1.9571...  6.8350 sec/batch\n",
      "Epoch: 3/20...  Training Step: 518...  Training loss: 1.9138...  6.7965 sec/batch\n",
      "Epoch: 3/20...  Training Step: 519...  Training loss: 1.9213...  6.8945 sec/batch\n",
      "Epoch: 3/20...  Training Step: 520...  Training loss: 1.9707...  8.5945 sec/batch\n",
      "Epoch: 3/20...  Training Step: 521...  Training loss: 1.9295...  7.2884 sec/batch\n",
      "Epoch: 3/20...  Training Step: 522...  Training loss: 1.8978...  8.2158 sec/batch\n",
      "Epoch: 3/20...  Training Step: 523...  Training loss: 1.9638...  8.1445 sec/batch\n",
      "Epoch: 3/20...  Training Step: 524...  Training loss: 1.9541...  7.4849 sec/batch\n",
      "Epoch: 3/20...  Training Step: 525...  Training loss: 1.9280...  7.0966 sec/batch\n",
      "Epoch: 3/20...  Training Step: 526...  Training loss: 1.9476...  7.5704 sec/batch\n",
      "Epoch: 3/20...  Training Step: 527...  Training loss: 1.9159...  6.6662 sec/batch\n",
      "Epoch: 3/20...  Training Step: 528...  Training loss: 1.9072...  7.2413 sec/batch\n",
      "Epoch: 3/20...  Training Step: 529...  Training loss: 1.9475...  7.3442 sec/batch\n",
      "Epoch: 3/20...  Training Step: 530...  Training loss: 1.9477...  6.6754 sec/batch\n",
      "Epoch: 3/20...  Training Step: 531...  Training loss: 1.9347...  7.4510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 532...  Training loss: 1.9389...  7.1043 sec/batch\n",
      "Epoch: 3/20...  Training Step: 533...  Training loss: 1.9426...  7.1473 sec/batch\n",
      "Epoch: 3/20...  Training Step: 534...  Training loss: 1.9364...  6.7840 sec/batch\n",
      "Epoch: 3/20...  Training Step: 535...  Training loss: 1.9664...  6.9383 sec/batch\n",
      "Epoch: 3/20...  Training Step: 536...  Training loss: 1.9253...  6.9473 sec/batch\n",
      "Epoch: 3/20...  Training Step: 537...  Training loss: 1.9545...  7.3757 sec/batch\n",
      "Epoch: 3/20...  Training Step: 538...  Training loss: 1.9224...  7.2839 sec/batch\n",
      "Epoch: 3/20...  Training Step: 539...  Training loss: 1.9267...  7.4306 sec/batch\n",
      "Epoch: 3/20...  Training Step: 540...  Training loss: 1.9469...  8.0516 sec/batch\n",
      "Epoch: 3/20...  Training Step: 541...  Training loss: 1.9141...  8.0311 sec/batch\n",
      "Epoch: 3/20...  Training Step: 542...  Training loss: 1.9348...  7.7372 sec/batch\n",
      "Epoch: 3/20...  Training Step: 543...  Training loss: 1.9422...  8.0120 sec/batch\n",
      "Epoch: 3/20...  Training Step: 544...  Training loss: 1.9586...  7.4721 sec/batch\n",
      "Epoch: 3/20...  Training Step: 545...  Training loss: 1.9327...  8.3857 sec/batch\n",
      "Epoch: 3/20...  Training Step: 546...  Training loss: 1.9188...  7.0302 sec/batch\n",
      "Epoch: 3/20...  Training Step: 547...  Training loss: 1.9119...  6.3093 sec/batch\n",
      "Epoch: 3/20...  Training Step: 548...  Training loss: 1.9586...  6.1304 sec/batch\n",
      "Epoch: 3/20...  Training Step: 549...  Training loss: 1.9344...  6.2285 sec/batch\n",
      "Epoch: 3/20...  Training Step: 550...  Training loss: 1.9399...  7.1562 sec/batch\n",
      "Epoch: 3/20...  Training Step: 551...  Training loss: 1.9221...  7.0958 sec/batch\n",
      "Epoch: 3/20...  Training Step: 552...  Training loss: 1.9146...  7.6411 sec/batch\n",
      "Epoch: 3/20...  Training Step: 553...  Training loss: 1.9269...  7.4949 sec/batch\n",
      "Epoch: 3/20...  Training Step: 554...  Training loss: 1.9219...  9.0261 sec/batch\n",
      "Epoch: 3/20...  Training Step: 555...  Training loss: 1.8890...  8.3140 sec/batch\n",
      "Epoch: 3/20...  Training Step: 556...  Training loss: 1.9584...  7.4600 sec/batch\n",
      "Epoch: 3/20...  Training Step: 557...  Training loss: 1.9414...  8.7257 sec/batch\n",
      "Epoch: 3/20...  Training Step: 558...  Training loss: 1.9184...  8.3435 sec/batch\n",
      "Epoch: 3/20...  Training Step: 559...  Training loss: 1.9281...  7.0459 sec/batch\n",
      "Epoch: 3/20...  Training Step: 560...  Training loss: 1.9226...  7.2581 sec/batch\n",
      "Epoch: 3/20...  Training Step: 561...  Training loss: 1.9126...  7.4350 sec/batch\n",
      "Epoch: 3/20...  Training Step: 562...  Training loss: 1.9050...  8.5597 sec/batch\n",
      "Epoch: 3/20...  Training Step: 563...  Training loss: 1.9112...  8.5999 sec/batch\n",
      "Epoch: 3/20...  Training Step: 564...  Training loss: 1.9604...  8.3391 sec/batch\n",
      "Epoch: 3/20...  Training Step: 565...  Training loss: 1.9104...  8.0330 sec/batch\n",
      "Epoch: 3/20...  Training Step: 566...  Training loss: 1.9099...  8.0927 sec/batch\n",
      "Epoch: 3/20...  Training Step: 567...  Training loss: 1.8908...  9.7874 sec/batch\n",
      "Epoch: 3/20...  Training Step: 568...  Training loss: 1.8913...  8.7029 sec/batch\n",
      "Epoch: 3/20...  Training Step: 569...  Training loss: 1.9259...  9.1829 sec/batch\n",
      "Epoch: 3/20...  Training Step: 570...  Training loss: 1.9195...  8.5692 sec/batch\n",
      "Epoch: 3/20...  Training Step: 571...  Training loss: 1.9081...  11.0302 sec/batch\n",
      "Epoch: 3/20...  Training Step: 572...  Training loss: 1.8900...  10.1650 sec/batch\n",
      "Epoch: 3/20...  Training Step: 573...  Training loss: 1.8831...  11.4554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 574...  Training loss: 1.9126...  8.6985 sec/batch\n",
      "Epoch: 3/20...  Training Step: 575...  Training loss: 1.8763...  7.5895 sec/batch\n",
      "Epoch: 3/20...  Training Step: 576...  Training loss: 1.8692...  8.0140 sec/batch\n",
      "Epoch: 3/20...  Training Step: 577...  Training loss: 1.8749...  8.9985 sec/batch\n",
      "Epoch: 3/20...  Training Step: 578...  Training loss: 1.8938...  9.1237 sec/batch\n",
      "Epoch: 3/20...  Training Step: 579...  Training loss: 1.8887...  8.1555 sec/batch\n",
      "Epoch: 3/20...  Training Step: 580...  Training loss: 1.9129...  9.4315 sec/batch\n",
      "Epoch: 3/20...  Training Step: 581...  Training loss: 1.8988...  10.3252 sec/batch\n",
      "Epoch: 3/20...  Training Step: 582...  Training loss: 1.8745...  8.2768 sec/batch\n",
      "Epoch: 3/20...  Training Step: 583...  Training loss: 1.8917...  9.5167 sec/batch\n",
      "Epoch: 3/20...  Training Step: 584...  Training loss: 1.8741...  10.9236 sec/batch\n",
      "Epoch: 3/20...  Training Step: 585...  Training loss: 1.8828...  10.4585 sec/batch\n",
      "Epoch: 3/20...  Training Step: 586...  Training loss: 1.8915...  8.3505 sec/batch\n",
      "Epoch: 3/20...  Training Step: 587...  Training loss: 1.8981...  8.5458 sec/batch\n",
      "Epoch: 3/20...  Training Step: 588...  Training loss: 1.8605...  9.1857 sec/batch\n",
      "Epoch: 3/20...  Training Step: 589...  Training loss: 1.8954...  8.5889 sec/batch\n",
      "Epoch: 3/20...  Training Step: 590...  Training loss: 1.8713...  8.6331 sec/batch\n",
      "Epoch: 3/20...  Training Step: 591...  Training loss: 1.8540...  8.8896 sec/batch\n",
      "Epoch: 3/20...  Training Step: 592...  Training loss: 1.8794...  8.6989 sec/batch\n",
      "Epoch: 3/20...  Training Step: 593...  Training loss: 1.8865...  9.7418 sec/batch\n",
      "Epoch: 3/20...  Training Step: 594...  Training loss: 1.8600...  8.4108 sec/batch\n",
      "Epoch: 4/20...  Training Step: 595...  Training loss: 1.9631...  8.4941 sec/batch\n",
      "Epoch: 4/20...  Training Step: 596...  Training loss: 1.8703...  9.8206 sec/batch\n",
      "Epoch: 4/20...  Training Step: 597...  Training loss: 1.8711...  8.2764 sec/batch\n",
      "Epoch: 4/20...  Training Step: 598...  Training loss: 1.8737...  9.2065 sec/batch\n",
      "Epoch: 4/20...  Training Step: 599...  Training loss: 1.8707...  9.6584 sec/batch\n",
      "Epoch: 4/20...  Training Step: 600...  Training loss: 1.8326...  7.7939 sec/batch\n",
      "Epoch: 4/20...  Training Step: 601...  Training loss: 1.8663...  7.7961 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20...  Training Step: 602...  Training loss: 1.8537...  8.2642 sec/batch\n",
      "Epoch: 4/20...  Training Step: 603...  Training loss: 1.9123...  7.5612 sec/batch\n",
      "Epoch: 4/20...  Training Step: 604...  Training loss: 1.8554...  7.2101 sec/batch\n",
      "Epoch: 4/20...  Training Step: 605...  Training loss: 1.8454...  8.0698 sec/batch\n",
      "Epoch: 4/20...  Training Step: 606...  Training loss: 1.8520...  7.3922 sec/batch\n",
      "Epoch: 4/20...  Training Step: 607...  Training loss: 1.8677...  10.2435 sec/batch\n",
      "Epoch: 4/20...  Training Step: 608...  Training loss: 1.9088...  6.6751 sec/batch\n",
      "Epoch: 4/20...  Training Step: 609...  Training loss: 1.8626...  6.4217 sec/batch\n",
      "Epoch: 4/20...  Training Step: 610...  Training loss: 1.8448...  8.3387 sec/batch\n",
      "Epoch: 4/20...  Training Step: 611...  Training loss: 1.8649...  8.5014 sec/batch\n",
      "Epoch: 4/20...  Training Step: 612...  Training loss: 1.9032...  8.8029 sec/batch\n",
      "Epoch: 4/20...  Training Step: 613...  Training loss: 1.8727...  7.9657 sec/batch\n",
      "Epoch: 4/20...  Training Step: 614...  Training loss: 1.8741...  8.4134 sec/batch\n",
      "Epoch: 4/20...  Training Step: 615...  Training loss: 1.8609...  8.1027 sec/batch\n",
      "Epoch: 4/20...  Training Step: 616...  Training loss: 1.9105...  8.6556 sec/batch\n",
      "Epoch: 4/20...  Training Step: 617...  Training loss: 1.8631...  8.8589 sec/batch\n",
      "Epoch: 4/20...  Training Step: 618...  Training loss: 1.8652...  8.5378 sec/batch\n",
      "Epoch: 4/20...  Training Step: 619...  Training loss: 1.8606...  9.8860 sec/batch\n",
      "Epoch: 4/20...  Training Step: 620...  Training loss: 1.8331...  8.4463 sec/batch\n",
      "Epoch: 4/20...  Training Step: 621...  Training loss: 1.8366...  7.1609 sec/batch\n",
      "Epoch: 4/20...  Training Step: 622...  Training loss: 1.8679...  6.7320 sec/batch\n",
      "Epoch: 4/20...  Training Step: 623...  Training loss: 1.8935...  7.1317 sec/batch\n",
      "Epoch: 4/20...  Training Step: 624...  Training loss: 1.8805...  8.1600 sec/batch\n",
      "Epoch: 4/20...  Training Step: 625...  Training loss: 1.8572...  7.7399 sec/batch\n",
      "Epoch: 4/20...  Training Step: 626...  Training loss: 1.8280...  7.8617 sec/batch\n",
      "Epoch: 4/20...  Training Step: 627...  Training loss: 1.8616...  6.5259 sec/batch\n",
      "Epoch: 4/20...  Training Step: 628...  Training loss: 1.8818...  6.5731 sec/batch\n",
      "Epoch: 4/20...  Training Step: 629...  Training loss: 1.8449...  7.3753 sec/batch\n",
      "Epoch: 4/20...  Training Step: 630...  Training loss: 1.8458...  7.6193 sec/batch\n",
      "Epoch: 4/20...  Training Step: 631...  Training loss: 1.8329...  7.1235 sec/batch\n",
      "Epoch: 4/20...  Training Step: 632...  Training loss: 1.8120...  8.0887 sec/batch\n",
      "Epoch: 4/20...  Training Step: 633...  Training loss: 1.8061...  8.8829 sec/batch\n",
      "Epoch: 4/20...  Training Step: 634...  Training loss: 1.8217...  7.1404 sec/batch\n",
      "Epoch: 4/20...  Training Step: 635...  Training loss: 1.8205...  8.0254 sec/batch\n",
      "Epoch: 4/20...  Training Step: 636...  Training loss: 1.8572...  7.7389 sec/batch\n",
      "Epoch: 4/20...  Training Step: 637...  Training loss: 1.8243...  7.2927 sec/batch\n",
      "Epoch: 4/20...  Training Step: 638...  Training loss: 1.8119...  7.1302 sec/batch\n",
      "Epoch: 4/20...  Training Step: 639...  Training loss: 1.8588...  7.9624 sec/batch\n",
      "Epoch: 4/20...  Training Step: 640...  Training loss: 1.7931...  8.4503 sec/batch\n",
      "Epoch: 4/20...  Training Step: 641...  Training loss: 1.8421...  7.5690 sec/batch\n",
      "Epoch: 4/20...  Training Step: 642...  Training loss: 1.8162...  6.6244 sec/batch\n",
      "Epoch: 4/20...  Training Step: 643...  Training loss: 1.8204...  6.6825 sec/batch\n",
      "Epoch: 4/20...  Training Step: 644...  Training loss: 1.8738...  6.6545 sec/batch\n",
      "Epoch: 4/20...  Training Step: 645...  Training loss: 1.8034...  7.8920 sec/batch\n",
      "Epoch: 4/20...  Training Step: 646...  Training loss: 1.8978...  7.8870 sec/batch\n",
      "Epoch: 4/20...  Training Step: 647...  Training loss: 1.8306...  7.4221 sec/batch\n",
      "Epoch: 4/20...  Training Step: 648...  Training loss: 1.8339...  6.4111 sec/batch\n",
      "Epoch: 4/20...  Training Step: 649...  Training loss: 1.8199...  6.4786 sec/batch\n",
      "Epoch: 4/20...  Training Step: 650...  Training loss: 1.8495...  6.5069 sec/batch\n",
      "Epoch: 4/20...  Training Step: 651...  Training loss: 1.8546...  6.5624 sec/batch\n",
      "Epoch: 4/20...  Training Step: 652...  Training loss: 1.8149...  6.4021 sec/batch\n",
      "Epoch: 4/20...  Training Step: 653...  Training loss: 1.8082...  6.5599 sec/batch\n",
      "Epoch: 4/20...  Training Step: 654...  Training loss: 1.8636...  7.0737 sec/batch\n",
      "Epoch: 4/20...  Training Step: 655...  Training loss: 1.8281...  6.3739 sec/batch\n",
      "Epoch: 4/20...  Training Step: 656...  Training loss: 1.8738...  6.5177 sec/batch\n",
      "Epoch: 4/20...  Training Step: 657...  Training loss: 1.8574...  7.8126 sec/batch\n",
      "Epoch: 4/20...  Training Step: 658...  Training loss: 1.8457...  6.6766 sec/batch\n",
      "Epoch: 4/20...  Training Step: 659...  Training loss: 1.8212...  6.6652 sec/batch\n",
      "Epoch: 4/20...  Training Step: 660...  Training loss: 1.8574...  6.6143 sec/batch\n",
      "Epoch: 4/20...  Training Step: 661...  Training loss: 1.8467...  6.5906 sec/batch\n",
      "Epoch: 4/20...  Training Step: 662...  Training loss: 1.8077...  9.3119 sec/batch\n",
      "Epoch: 4/20...  Training Step: 663...  Training loss: 1.8180...  7.2442 sec/batch\n",
      "Epoch: 4/20...  Training Step: 664...  Training loss: 1.8208...  7.1840 sec/batch\n",
      "Epoch: 4/20...  Training Step: 665...  Training loss: 1.8594...  6.8971 sec/batch\n",
      "Epoch: 4/20...  Training Step: 666...  Training loss: 1.8392...  8.0775 sec/batch\n",
      "Epoch: 4/20...  Training Step: 667...  Training loss: 1.8506...  8.6499 sec/batch\n",
      "Epoch: 4/20...  Training Step: 668...  Training loss: 1.8031...  7.6238 sec/batch\n",
      "Epoch: 4/20...  Training Step: 669...  Training loss: 1.8132...  7.7203 sec/batch\n",
      "Epoch: 4/20...  Training Step: 670...  Training loss: 1.8532...  7.1845 sec/batch\n",
      "Epoch: 4/20...  Training Step: 671...  Training loss: 1.8277...  7.2857 sec/batch\n",
      "Epoch: 4/20...  Training Step: 672...  Training loss: 1.8217...  7.2933 sec/batch\n",
      "Epoch: 4/20...  Training Step: 673...  Training loss: 1.7852...  6.6503 sec/batch\n",
      "Epoch: 4/20...  Training Step: 674...  Training loss: 1.8179...  6.9326 sec/batch\n",
      "Epoch: 4/20...  Training Step: 675...  Training loss: 1.7716...  6.9695 sec/batch\n",
      "Epoch: 4/20...  Training Step: 676...  Training loss: 1.8272...  7.4196 sec/batch\n",
      "Epoch: 4/20...  Training Step: 677...  Training loss: 1.7770...  7.9702 sec/batch\n",
      "Epoch: 4/20...  Training Step: 678...  Training loss: 1.8085...  8.6006 sec/batch\n",
      "Epoch: 4/20...  Training Step: 679...  Training loss: 1.7699...  9.7845 sec/batch\n",
      "Epoch: 4/20...  Training Step: 680...  Training loss: 1.7866...  8.1489 sec/batch\n",
      "Epoch: 4/20...  Training Step: 681...  Training loss: 1.7879...  8.2673 sec/batch\n",
      "Epoch: 4/20...  Training Step: 682...  Training loss: 1.7990...  9.5417 sec/batch\n",
      "Epoch: 4/20...  Training Step: 683...  Training loss: 1.7584...  7.5136 sec/batch\n",
      "Epoch: 4/20...  Training Step: 684...  Training loss: 1.8170...  7.8676 sec/batch\n",
      "Epoch: 4/20...  Training Step: 685...  Training loss: 1.7808...  6.8818 sec/batch\n",
      "Epoch: 4/20...  Training Step: 686...  Training loss: 1.7907...  7.9568 sec/batch\n",
      "Epoch: 4/20...  Training Step: 687...  Training loss: 1.7768...  7.2409 sec/batch\n",
      "Epoch: 4/20...  Training Step: 688...  Training loss: 1.7832...  8.0416 sec/batch\n",
      "Epoch: 4/20...  Training Step: 689...  Training loss: 1.7805...  7.6083 sec/batch\n",
      "Epoch: 4/20...  Training Step: 690...  Training loss: 1.8036...  7.3590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 691...  Training loss: 1.7937...  7.7225 sec/batch\n",
      "Epoch: 4/20...  Training Step: 692...  Training loss: 1.7769...  8.7766 sec/batch\n",
      "Epoch: 4/20...  Training Step: 693...  Training loss: 1.7728...  8.0424 sec/batch\n",
      "Epoch: 4/20...  Training Step: 694...  Training loss: 1.7577...  8.3413 sec/batch\n",
      "Epoch: 4/20...  Training Step: 695...  Training loss: 1.8087...  10.0692 sec/batch\n",
      "Epoch: 4/20...  Training Step: 696...  Training loss: 1.7929...  9.9588 sec/batch\n",
      "Epoch: 4/20...  Training Step: 697...  Training loss: 1.7888...  10.0270 sec/batch\n",
      "Epoch: 4/20...  Training Step: 698...  Training loss: 1.7876...  9.0912 sec/batch\n",
      "Epoch: 4/20...  Training Step: 699...  Training loss: 1.7856...  7.2653 sec/batch\n",
      "Epoch: 4/20...  Training Step: 700...  Training loss: 1.7854...  7.8947 sec/batch\n",
      "Epoch: 4/20...  Training Step: 701...  Training loss: 1.7889...  8.5282 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20...  Training Step: 702...  Training loss: 1.7947...  7.3992 sec/batch\n",
      "Epoch: 4/20...  Training Step: 703...  Training loss: 1.8055...  7.1609 sec/batch\n",
      "Epoch: 4/20...  Training Step: 704...  Training loss: 1.8060...  7.0256 sec/batch\n",
      "Epoch: 4/20...  Training Step: 705...  Training loss: 1.7921...  6.4769 sec/batch\n",
      "Epoch: 4/20...  Training Step: 706...  Training loss: 1.7895...  6.6042 sec/batch\n",
      "Epoch: 4/20...  Training Step: 707...  Training loss: 1.7981...  7.1063 sec/batch\n",
      "Epoch: 4/20...  Training Step: 708...  Training loss: 1.7819...  6.9118 sec/batch\n",
      "Epoch: 4/20...  Training Step: 709...  Training loss: 1.7642...  7.0288 sec/batch\n",
      "Epoch: 4/20...  Training Step: 710...  Training loss: 1.7502...  6.7964 sec/batch\n",
      "Epoch: 4/20...  Training Step: 711...  Training loss: 1.7857...  7.7737 sec/batch\n",
      "Epoch: 4/20...  Training Step: 712...  Training loss: 1.7737...  7.1946 sec/batch\n",
      "Epoch: 4/20...  Training Step: 713...  Training loss: 1.7805...  7.8615 sec/batch\n",
      "Epoch: 4/20...  Training Step: 714...  Training loss: 1.7732...  6.6524 sec/batch\n",
      "Epoch: 4/20...  Training Step: 715...  Training loss: 1.7868...  6.4819 sec/batch\n",
      "Epoch: 4/20...  Training Step: 716...  Training loss: 1.7541...  6.4725 sec/batch\n",
      "Epoch: 4/20...  Training Step: 717...  Training loss: 1.7514...  6.4599 sec/batch\n",
      "Epoch: 4/20...  Training Step: 718...  Training loss: 1.7917...  6.4526 sec/batch\n",
      "Epoch: 4/20...  Training Step: 719...  Training loss: 1.7674...  7.3357 sec/batch\n",
      "Epoch: 4/20...  Training Step: 720...  Training loss: 1.7314...  7.5513 sec/batch\n",
      "Epoch: 4/20...  Training Step: 721...  Training loss: 1.8012...  8.6608 sec/batch\n",
      "Epoch: 4/20...  Training Step: 722...  Training loss: 1.7934...  7.2518 sec/batch\n",
      "Epoch: 4/20...  Training Step: 723...  Training loss: 1.7738...  8.2031 sec/batch\n",
      "Epoch: 4/20...  Training Step: 724...  Training loss: 1.7694...  8.3561 sec/batch\n",
      "Epoch: 4/20...  Training Step: 725...  Training loss: 1.7488...  6.8524 sec/batch\n",
      "Epoch: 4/20...  Training Step: 726...  Training loss: 1.7519...  8.2058 sec/batch\n",
      "Epoch: 4/20...  Training Step: 727...  Training loss: 1.7847...  8.4944 sec/batch\n",
      "Epoch: 4/20...  Training Step: 728...  Training loss: 1.7833...  9.1556 sec/batch\n",
      "Epoch: 4/20...  Training Step: 729...  Training loss: 1.7890...  7.1973 sec/batch\n",
      "Epoch: 4/20...  Training Step: 730...  Training loss: 1.7822...  7.4264 sec/batch\n",
      "Epoch: 4/20...  Training Step: 731...  Training loss: 1.7903...  7.8369 sec/batch\n",
      "Epoch: 4/20...  Training Step: 732...  Training loss: 1.7868...  8.0109 sec/batch\n",
      "Epoch: 4/20...  Training Step: 733...  Training loss: 1.8116...  8.1831 sec/batch\n",
      "Epoch: 4/20...  Training Step: 734...  Training loss: 1.7634...  8.5605 sec/batch\n",
      "Epoch: 4/20...  Training Step: 735...  Training loss: 1.8142...  7.9571 sec/batch\n",
      "Epoch: 4/20...  Training Step: 736...  Training loss: 1.7647...  8.2214 sec/batch\n",
      "Epoch: 4/20...  Training Step: 737...  Training loss: 1.7780...  7.7374 sec/batch\n",
      "Epoch: 4/20...  Training Step: 738...  Training loss: 1.7843...  8.3951 sec/batch\n",
      "Epoch: 4/20...  Training Step: 739...  Training loss: 1.7544...  7.8578 sec/batch\n",
      "Epoch: 4/20...  Training Step: 740...  Training loss: 1.7897...  9.1787 sec/batch\n",
      "Epoch: 4/20...  Training Step: 741...  Training loss: 1.7820...  7.9286 sec/batch\n",
      "Epoch: 4/20...  Training Step: 742...  Training loss: 1.7952...  8.2467 sec/batch\n",
      "Epoch: 4/20...  Training Step: 743...  Training loss: 1.7895...  8.1731 sec/batch\n",
      "Epoch: 4/20...  Training Step: 744...  Training loss: 1.7598...  8.0692 sec/batch\n",
      "Epoch: 4/20...  Training Step: 745...  Training loss: 1.7509...  9.0971 sec/batch\n",
      "Epoch: 4/20...  Training Step: 746...  Training loss: 1.7915...  7.5048 sec/batch\n",
      "Epoch: 4/20...  Training Step: 747...  Training loss: 1.7779...  8.6017 sec/batch\n",
      "Epoch: 4/20...  Training Step: 748...  Training loss: 1.7733...  8.3753 sec/batch\n",
      "Epoch: 4/20...  Training Step: 749...  Training loss: 1.7543...  7.2740 sec/batch\n",
      "Epoch: 4/20...  Training Step: 750...  Training loss: 1.7523...  8.3686 sec/batch\n",
      "Epoch: 4/20...  Training Step: 751...  Training loss: 1.7774...  7.9512 sec/batch\n",
      "Epoch: 4/20...  Training Step: 752...  Training loss: 1.7695...  7.3844 sec/batch\n",
      "Epoch: 4/20...  Training Step: 753...  Training loss: 1.7350...  8.6705 sec/batch\n",
      "Epoch: 4/20...  Training Step: 754...  Training loss: 1.7887...  9.7446 sec/batch\n",
      "Epoch: 4/20...  Training Step: 755...  Training loss: 1.7909...  7.6015 sec/batch\n",
      "Epoch: 4/20...  Training Step: 756...  Training loss: 1.7611...  7.4800 sec/batch\n",
      "Epoch: 4/20...  Training Step: 757...  Training loss: 1.7831...  7.9781 sec/batch\n",
      "Epoch: 4/20...  Training Step: 758...  Training loss: 1.7746...  8.1262 sec/batch\n",
      "Epoch: 4/20...  Training Step: 759...  Training loss: 1.7623...  8.4752 sec/batch\n",
      "Epoch: 4/20...  Training Step: 760...  Training loss: 1.7595...  8.6605 sec/batch\n",
      "Epoch: 4/20...  Training Step: 761...  Training loss: 1.7710...  7.1150 sec/batch\n",
      "Epoch: 4/20...  Training Step: 762...  Training loss: 1.8180...  6.8712 sec/batch\n",
      "Epoch: 4/20...  Training Step: 763...  Training loss: 1.7463...  7.8231 sec/batch\n",
      "Epoch: 4/20...  Training Step: 764...  Training loss: 1.7563...  8.1381 sec/batch\n",
      "Epoch: 4/20...  Training Step: 765...  Training loss: 1.7459...  7.4410 sec/batch\n",
      "Epoch: 4/20...  Training Step: 766...  Training loss: 1.7403...  7.9546 sec/batch\n",
      "Epoch: 4/20...  Training Step: 767...  Training loss: 1.7720...  7.4465 sec/batch\n",
      "Epoch: 4/20...  Training Step: 768...  Training loss: 1.7639...  7.6352 sec/batch\n",
      "Epoch: 4/20...  Training Step: 769...  Training loss: 1.7649...  8.2359 sec/batch\n",
      "Epoch: 4/20...  Training Step: 770...  Training loss: 1.7381...  7.6487 sec/batch\n",
      "Epoch: 4/20...  Training Step: 771...  Training loss: 1.7382...  7.9265 sec/batch\n",
      "Epoch: 4/20...  Training Step: 772...  Training loss: 1.7706...  7.2332 sec/batch\n",
      "Epoch: 4/20...  Training Step: 773...  Training loss: 1.7252...  7.6395 sec/batch\n",
      "Epoch: 4/20...  Training Step: 774...  Training loss: 1.7191...  6.8805 sec/batch\n",
      "Epoch: 4/20...  Training Step: 775...  Training loss: 1.7238...  7.0069 sec/batch\n",
      "Epoch: 4/20...  Training Step: 776...  Training loss: 1.7435...  7.6517 sec/batch\n",
      "Epoch: 4/20...  Training Step: 777...  Training loss: 1.7452...  7.3012 sec/batch\n",
      "Epoch: 4/20...  Training Step: 778...  Training loss: 1.7671...  8.1067 sec/batch\n",
      "Epoch: 4/20...  Training Step: 779...  Training loss: 1.7417...  8.5131 sec/batch\n",
      "Epoch: 4/20...  Training Step: 780...  Training loss: 1.7322...  7.5448 sec/batch\n",
      "Epoch: 4/20...  Training Step: 781...  Training loss: 1.7483...  7.9379 sec/batch\n",
      "Epoch: 4/20...  Training Step: 782...  Training loss: 1.7255...  8.0236 sec/batch\n",
      "Epoch: 4/20...  Training Step: 783...  Training loss: 1.7352...  8.1549 sec/batch\n",
      "Epoch: 4/20...  Training Step: 784...  Training loss: 1.7502...  7.8764 sec/batch\n",
      "Epoch: 4/20...  Training Step: 785...  Training loss: 1.7475...  8.0278 sec/batch\n",
      "Epoch: 4/20...  Training Step: 786...  Training loss: 1.7230...  8.5440 sec/batch\n",
      "Epoch: 4/20...  Training Step: 787...  Training loss: 1.7470...  8.2952 sec/batch\n",
      "Epoch: 4/20...  Training Step: 788...  Training loss: 1.7174...  7.4397 sec/batch\n",
      "Epoch: 4/20...  Training Step: 789...  Training loss: 1.7118...  9.2654 sec/batch\n",
      "Epoch: 4/20...  Training Step: 790...  Training loss: 1.7417...  8.7305 sec/batch\n",
      "Epoch: 4/20...  Training Step: 791...  Training loss: 1.7378...  8.7448 sec/batch\n",
      "Epoch: 4/20...  Training Step: 792...  Training loss: 1.7258...  7.8980 sec/batch\n",
      "Epoch: 5/20...  Training Step: 793...  Training loss: 1.8209...  9.3428 sec/batch\n",
      "Epoch: 5/20...  Training Step: 794...  Training loss: 1.7384...  8.3834 sec/batch\n",
      "Epoch: 5/20...  Training Step: 795...  Training loss: 1.7281...  8.3170 sec/batch\n",
      "Epoch: 5/20...  Training Step: 796...  Training loss: 1.7325...  7.5413 sec/batch\n",
      "Epoch: 5/20...  Training Step: 797...  Training loss: 1.7244...  7.5986 sec/batch\n",
      "Epoch: 5/20...  Training Step: 798...  Training loss: 1.6846...  8.2657 sec/batch\n",
      "Epoch: 5/20...  Training Step: 799...  Training loss: 1.7247...  7.7078 sec/batch\n",
      "Epoch: 5/20...  Training Step: 800...  Training loss: 1.7176...  9.0287 sec/batch\n",
      "Epoch: 5/20...  Training Step: 801...  Training loss: 1.7590...  8.7169 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20...  Training Step: 802...  Training loss: 1.7286...  7.7151 sec/batch\n",
      "Epoch: 5/20...  Training Step: 803...  Training loss: 1.7121...  7.5230 sec/batch\n",
      "Epoch: 5/20...  Training Step: 804...  Training loss: 1.7140...  7.4356 sec/batch\n",
      "Epoch: 5/20...  Training Step: 805...  Training loss: 1.7289...  7.8378 sec/batch\n",
      "Epoch: 5/20...  Training Step: 806...  Training loss: 1.7631...  7.3367 sec/batch\n",
      "Epoch: 5/20...  Training Step: 807...  Training loss: 1.7217...  7.3137 sec/batch\n",
      "Epoch: 5/20...  Training Step: 808...  Training loss: 1.7126...  7.4342 sec/batch\n",
      "Epoch: 5/20...  Training Step: 809...  Training loss: 1.7287...  7.4966 sec/batch\n",
      "Epoch: 5/20...  Training Step: 810...  Training loss: 1.7616...  7.3488 sec/batch\n",
      "Epoch: 5/20...  Training Step: 811...  Training loss: 1.7371...  7.1472 sec/batch\n",
      "Epoch: 5/20...  Training Step: 812...  Training loss: 1.7459...  7.3495 sec/batch\n",
      "Epoch: 5/20...  Training Step: 813...  Training loss: 1.7130...  7.3528 sec/batch\n",
      "Epoch: 5/20...  Training Step: 814...  Training loss: 1.7600...  9.7863 sec/batch\n",
      "Epoch: 5/20...  Training Step: 815...  Training loss: 1.7248...  8.2350 sec/batch\n",
      "Epoch: 5/20...  Training Step: 816...  Training loss: 1.7260...  8.0397 sec/batch\n",
      "Epoch: 5/20...  Training Step: 817...  Training loss: 1.7213...  8.8922 sec/batch\n",
      "Epoch: 5/20...  Training Step: 818...  Training loss: 1.6879...  7.5330 sec/batch\n",
      "Epoch: 5/20...  Training Step: 819...  Training loss: 1.6968...  7.2303 sec/batch\n",
      "Epoch: 5/20...  Training Step: 820...  Training loss: 1.7382...  7.2216 sec/batch\n",
      "Epoch: 5/20...  Training Step: 821...  Training loss: 1.7580...  7.1782 sec/batch\n",
      "Epoch: 5/20...  Training Step: 822...  Training loss: 1.7319...  8.4045 sec/batch\n",
      "Epoch: 5/20...  Training Step: 823...  Training loss: 1.7206...  9.4935 sec/batch\n",
      "Epoch: 5/20...  Training Step: 824...  Training loss: 1.6991...  7.9713 sec/batch\n",
      "Epoch: 5/20...  Training Step: 825...  Training loss: 1.7509...  10.2003 sec/batch\n",
      "Epoch: 5/20...  Training Step: 826...  Training loss: 1.7450...  9.9408 sec/batch\n",
      "Epoch: 5/20...  Training Step: 827...  Training loss: 1.7124...  11.0501 sec/batch\n",
      "Epoch: 5/20...  Training Step: 828...  Training loss: 1.7067...  10.1475 sec/batch\n",
      "Epoch: 5/20...  Training Step: 829...  Training loss: 1.7021...  11.9512 sec/batch\n",
      "Epoch: 5/20...  Training Step: 830...  Training loss: 1.6811...  8.6979 sec/batch\n",
      "Epoch: 5/20...  Training Step: 831...  Training loss: 1.6628...  9.5337 sec/batch\n",
      "Epoch: 5/20...  Training Step: 832...  Training loss: 1.6928...  11.0979 sec/batch\n",
      "Epoch: 5/20...  Training Step: 833...  Training loss: 1.6908...  8.9443 sec/batch\n",
      "Epoch: 5/20...  Training Step: 834...  Training loss: 1.7299...  8.5330 sec/batch\n",
      "Epoch: 5/20...  Training Step: 835...  Training loss: 1.6907...  7.8069 sec/batch\n",
      "Epoch: 5/20...  Training Step: 836...  Training loss: 1.6755...  8.0435 sec/batch\n",
      "Epoch: 5/20...  Training Step: 837...  Training loss: 1.7196...  9.3774 sec/batch\n",
      "Epoch: 5/20...  Training Step: 838...  Training loss: 1.6673...  8.7624 sec/batch\n",
      "Epoch: 5/20...  Training Step: 839...  Training loss: 1.7109...  7.5929 sec/batch\n",
      "Epoch: 5/20...  Training Step: 840...  Training loss: 1.6924...  9.1060 sec/batch\n",
      "Epoch: 5/20...  Training Step: 841...  Training loss: 1.7034...  9.7135 sec/batch\n",
      "Epoch: 5/20...  Training Step: 842...  Training loss: 1.7460...  11.4046 sec/batch\n",
      "Epoch: 5/20...  Training Step: 843...  Training loss: 1.6816...  9.9730 sec/batch\n",
      "Epoch: 5/20...  Training Step: 844...  Training loss: 1.7729...  8.3249 sec/batch\n",
      "Epoch: 5/20...  Training Step: 845...  Training loss: 1.7052...  8.9049 sec/batch\n",
      "Epoch: 5/20...  Training Step: 846...  Training loss: 1.7063...  7.4915 sec/batch\n",
      "Epoch: 5/20...  Training Step: 847...  Training loss: 1.6938...  9.0543 sec/batch\n",
      "Epoch: 5/20...  Training Step: 848...  Training loss: 1.7151...  11.1124 sec/batch\n",
      "Epoch: 5/20...  Training Step: 849...  Training loss: 1.7274...  11.3265 sec/batch\n",
      "Epoch: 5/20...  Training Step: 850...  Training loss: 1.6868...  9.4035 sec/batch\n",
      "Epoch: 5/20...  Training Step: 851...  Training loss: 1.6881...  11.3632 sec/batch\n",
      "Epoch: 5/20...  Training Step: 852...  Training loss: 1.7448...  11.0087 sec/batch\n",
      "Epoch: 5/20...  Training Step: 853...  Training loss: 1.7039...  9.6700 sec/batch\n",
      "Epoch: 5/20...  Training Step: 854...  Training loss: 1.7458...  10.1310 sec/batch\n",
      "Epoch: 5/20...  Training Step: 855...  Training loss: 1.7311...  8.0053 sec/batch\n",
      "Epoch: 5/20...  Training Step: 856...  Training loss: 1.7272...  8.4676 sec/batch\n",
      "Epoch: 5/20...  Training Step: 857...  Training loss: 1.6979...  9.4932 sec/batch\n",
      "Epoch: 5/20...  Training Step: 858...  Training loss: 1.7204...  9.1039 sec/batch\n",
      "Epoch: 5/20...  Training Step: 859...  Training loss: 1.7257...  8.4796 sec/batch\n",
      "Epoch: 5/20...  Training Step: 860...  Training loss: 1.6914...  9.2706 sec/batch\n",
      "Epoch: 5/20...  Training Step: 861...  Training loss: 1.6918...  11.3644 sec/batch\n",
      "Epoch: 5/20...  Training Step: 862...  Training loss: 1.6931...  8.7723 sec/batch\n",
      "Epoch: 5/20...  Training Step: 863...  Training loss: 1.7369...  8.3229 sec/batch\n",
      "Epoch: 5/20...  Training Step: 864...  Training loss: 1.7201...  8.5943 sec/batch\n",
      "Epoch: 5/20...  Training Step: 865...  Training loss: 1.7237...  9.9659 sec/batch\n",
      "Epoch: 5/20...  Training Step: 866...  Training loss: 1.6884...  9.1349 sec/batch\n",
      "Epoch: 5/20...  Training Step: 867...  Training loss: 1.6953...  11.5020 sec/batch\n",
      "Epoch: 5/20...  Training Step: 868...  Training loss: 1.7233...  8.6181 sec/batch\n",
      "Epoch: 5/20...  Training Step: 869...  Training loss: 1.6968...  9.6484 sec/batch\n",
      "Epoch: 5/20...  Training Step: 870...  Training loss: 1.6970...  9.5668 sec/batch\n",
      "Epoch: 5/20...  Training Step: 871...  Training loss: 1.6514...  9.5468 sec/batch\n",
      "Epoch: 5/20...  Training Step: 872...  Training loss: 1.6899...  8.4596 sec/batch\n",
      "Epoch: 5/20...  Training Step: 873...  Training loss: 1.6540...  8.5026 sec/batch\n",
      "Epoch: 5/20...  Training Step: 874...  Training loss: 1.7058...  8.6291 sec/batch\n",
      "Epoch: 5/20...  Training Step: 875...  Training loss: 1.6493...  8.5622 sec/batch\n",
      "Epoch: 5/20...  Training Step: 876...  Training loss: 1.6850...  9.6001 sec/batch\n",
      "Epoch: 5/20...  Training Step: 877...  Training loss: 1.6640...  9.4798 sec/batch\n",
      "Epoch: 5/20...  Training Step: 878...  Training loss: 1.6770...  9.5444 sec/batch\n",
      "Epoch: 5/20...  Training Step: 879...  Training loss: 1.6759...  7.8511 sec/batch\n",
      "Epoch: 5/20...  Training Step: 880...  Training loss: 1.6767...  8.9410 sec/batch\n",
      "Epoch: 5/20...  Training Step: 881...  Training loss: 1.6457...  9.9988 sec/batch\n",
      "Epoch: 5/20...  Training Step: 882...  Training loss: 1.6987...  9.4700 sec/batch\n",
      "Epoch: 5/20...  Training Step: 883...  Training loss: 1.6593...  8.6330 sec/batch\n",
      "Epoch: 5/20...  Training Step: 884...  Training loss: 1.6737...  9.1649 sec/batch\n",
      "Epoch: 5/20...  Training Step: 885...  Training loss: 1.6709...  9.1964 sec/batch\n",
      "Epoch: 5/20...  Training Step: 886...  Training loss: 1.6663...  10.0130 sec/batch\n",
      "Epoch: 5/20...  Training Step: 887...  Training loss: 1.6604...  10.2011 sec/batch\n",
      "Epoch: 5/20...  Training Step: 888...  Training loss: 1.6880...  7.8866 sec/batch\n",
      "Epoch: 5/20...  Training Step: 889...  Training loss: 1.6826...  7.6467 sec/batch\n",
      "Epoch: 5/20...  Training Step: 890...  Training loss: 1.6510...  7.2084 sec/batch\n",
      "Epoch: 5/20...  Training Step: 891...  Training loss: 1.6639...  7.7714 sec/batch\n",
      "Epoch: 5/20...  Training Step: 892...  Training loss: 1.6445...  7.4739 sec/batch\n",
      "Epoch: 5/20...  Training Step: 893...  Training loss: 1.6847...  8.1590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 894...  Training loss: 1.6698...  7.7117 sec/batch\n",
      "Epoch: 5/20...  Training Step: 895...  Training loss: 1.6637...  7.2580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 896...  Training loss: 1.6777...  8.3307 sec/batch\n",
      "Epoch: 5/20...  Training Step: 897...  Training loss: 1.6675...  7.6174 sec/batch\n",
      "Epoch: 5/20...  Training Step: 898...  Training loss: 1.6736...  9.0217 sec/batch\n",
      "Epoch: 5/20...  Training Step: 899...  Training loss: 1.6771...  7.7881 sec/batch\n",
      "Epoch: 5/20...  Training Step: 900...  Training loss: 1.6804...  7.6319 sec/batch\n",
      "Epoch: 5/20...  Training Step: 901...  Training loss: 1.6845...  8.6119 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20...  Training Step: 902...  Training loss: 1.6919...  7.9309 sec/batch\n",
      "Epoch: 5/20...  Training Step: 903...  Training loss: 1.6671...  8.4729 sec/batch\n",
      "Epoch: 5/20...  Training Step: 904...  Training loss: 1.6771...  8.9095 sec/batch\n",
      "Epoch: 5/20...  Training Step: 905...  Training loss: 1.6759...  8.0318 sec/batch\n",
      "Epoch: 5/20...  Training Step: 906...  Training loss: 1.6614...  7.9710 sec/batch\n",
      "Epoch: 5/20...  Training Step: 907...  Training loss: 1.6577...  7.6744 sec/batch\n",
      "Epoch: 5/20...  Training Step: 908...  Training loss: 1.6398...  8.0315 sec/batch\n",
      "Epoch: 5/20...  Training Step: 909...  Training loss: 1.6811...  7.7742 sec/batch\n",
      "Epoch: 5/20...  Training Step: 910...  Training loss: 1.6655...  8.5120 sec/batch\n",
      "Epoch: 5/20...  Training Step: 911...  Training loss: 1.6607...  8.2790 sec/batch\n",
      "Epoch: 5/20...  Training Step: 912...  Training loss: 1.6630...  9.0044 sec/batch\n",
      "Epoch: 5/20...  Training Step: 913...  Training loss: 1.6724...  9.4347 sec/batch\n",
      "Epoch: 5/20...  Training Step: 914...  Training loss: 1.6353...  8.2205 sec/batch\n",
      "Epoch: 5/20...  Training Step: 915...  Training loss: 1.6350...  7.1581 sec/batch\n",
      "Epoch: 5/20...  Training Step: 916...  Training loss: 1.6829...  9.1893 sec/batch\n",
      "Epoch: 5/20...  Training Step: 917...  Training loss: 1.6593...  7.9054 sec/batch\n",
      "Epoch: 5/20...  Training Step: 918...  Training loss: 1.6235...  8.8253 sec/batch\n",
      "Epoch: 5/20...  Training Step: 919...  Training loss: 1.6877...  9.2734 sec/batch\n",
      "Epoch: 5/20...  Training Step: 920...  Training loss: 1.6818...  8.6047 sec/batch\n",
      "Epoch: 5/20...  Training Step: 921...  Training loss: 1.6506...  9.1096 sec/batch\n",
      "Epoch: 5/20...  Training Step: 922...  Training loss: 1.6396...  8.8194 sec/batch\n",
      "Epoch: 5/20...  Training Step: 923...  Training loss: 1.6272...  7.4511 sec/batch\n",
      "Epoch: 5/20...  Training Step: 924...  Training loss: 1.6301...  12.0494 sec/batch\n",
      "Epoch: 5/20...  Training Step: 925...  Training loss: 1.6738...  9.9857 sec/batch\n",
      "Epoch: 5/20...  Training Step: 926...  Training loss: 1.6728...  8.8359 sec/batch\n",
      "Epoch: 5/20...  Training Step: 927...  Training loss: 1.6674...  9.6398 sec/batch\n",
      "Epoch: 5/20...  Training Step: 928...  Training loss: 1.6750...  8.9998 sec/batch\n",
      "Epoch: 5/20...  Training Step: 929...  Training loss: 1.6970...  7.4574 sec/batch\n",
      "Epoch: 5/20...  Training Step: 930...  Training loss: 1.6750...  6.9934 sec/batch\n",
      "Epoch: 5/20...  Training Step: 931...  Training loss: 1.6730...  7.0889 sec/batch\n",
      "Epoch: 5/20...  Training Step: 932...  Training loss: 1.6531...  7.4782 sec/batch\n",
      "Epoch: 5/20...  Training Step: 933...  Training loss: 1.7052...  7.4135 sec/batch\n",
      "Epoch: 5/20...  Training Step: 934...  Training loss: 1.6588...  6.8573 sec/batch\n",
      "Epoch: 5/20...  Training Step: 935...  Training loss: 1.6677...  7.7686 sec/batch\n",
      "Epoch: 5/20...  Training Step: 936...  Training loss: 1.6760...  7.5001 sec/batch\n",
      "Epoch: 5/20...  Training Step: 937...  Training loss: 1.6430...  7.0405 sec/batch\n",
      "Epoch: 5/20...  Training Step: 938...  Training loss: 1.6760...  7.4445 sec/batch\n",
      "Epoch: 5/20...  Training Step: 939...  Training loss: 1.6724...  7.5771 sec/batch\n",
      "Epoch: 5/20...  Training Step: 940...  Training loss: 1.6834...  7.6600 sec/batch\n",
      "Epoch: 5/20...  Training Step: 941...  Training loss: 1.6773...  7.4745 sec/batch\n",
      "Epoch: 5/20...  Training Step: 942...  Training loss: 1.6501...  9.2097 sec/batch\n",
      "Epoch: 5/20...  Training Step: 943...  Training loss: 1.6279...  8.3822 sec/batch\n",
      "Epoch: 5/20...  Training Step: 944...  Training loss: 1.6681...  10.0598 sec/batch\n",
      "Epoch: 5/20...  Training Step: 945...  Training loss: 1.6704...  10.6548 sec/batch\n",
      "Epoch: 5/20...  Training Step: 946...  Training loss: 1.6593...  10.1056 sec/batch\n",
      "Epoch: 5/20...  Training Step: 947...  Training loss: 1.6597...  8.7142 sec/batch\n",
      "Epoch: 5/20...  Training Step: 948...  Training loss: 1.6538...  7.2599 sec/batch\n",
      "Epoch: 5/20...  Training Step: 949...  Training loss: 1.6676...  6.9339 sec/batch\n",
      "Epoch: 5/20...  Training Step: 950...  Training loss: 1.6575...  7.6311 sec/batch\n",
      "Epoch: 5/20...  Training Step: 951...  Training loss: 1.6224...  8.0204 sec/batch\n",
      "Epoch: 5/20...  Training Step: 952...  Training loss: 1.6786...  9.1402 sec/batch\n",
      "Epoch: 5/20...  Training Step: 953...  Training loss: 1.6923...  10.7088 sec/batch\n",
      "Epoch: 5/20...  Training Step: 954...  Training loss: 1.6515...  7.8646 sec/batch\n",
      "Epoch: 5/20...  Training Step: 955...  Training loss: 1.6677...  8.0084 sec/batch\n",
      "Epoch: 5/20...  Training Step: 956...  Training loss: 1.6609...  8.6298 sec/batch\n",
      "Epoch: 5/20...  Training Step: 957...  Training loss: 1.6548...  9.0733 sec/batch\n",
      "Epoch: 5/20...  Training Step: 958...  Training loss: 1.6525...  9.8894 sec/batch\n",
      "Epoch: 5/20...  Training Step: 959...  Training loss: 1.6663...  8.5064 sec/batch\n",
      "Epoch: 5/20...  Training Step: 960...  Training loss: 1.7177...  8.2630 sec/batch\n",
      "Epoch: 5/20...  Training Step: 961...  Training loss: 1.6502...  7.8830 sec/batch\n",
      "Epoch: 5/20...  Training Step: 962...  Training loss: 1.6517...  7.5861 sec/batch\n",
      "Epoch: 5/20...  Training Step: 963...  Training loss: 1.6422...  8.3086 sec/batch\n",
      "Epoch: 5/20...  Training Step: 964...  Training loss: 1.6384...  9.1679 sec/batch\n",
      "Epoch: 5/20...  Training Step: 965...  Training loss: 1.6730...  7.5757 sec/batch\n",
      "Epoch: 5/20...  Training Step: 966...  Training loss: 1.6564...  8.2750 sec/batch\n",
      "Epoch: 5/20...  Training Step: 967...  Training loss: 1.6624...  7.5539 sec/batch\n",
      "Epoch: 5/20...  Training Step: 968...  Training loss: 1.6394...  7.5206 sec/batch\n",
      "Epoch: 5/20...  Training Step: 969...  Training loss: 1.6347...  8.3369 sec/batch\n",
      "Epoch: 5/20...  Training Step: 970...  Training loss: 1.6680...  8.5827 sec/batch\n",
      "Epoch: 5/20...  Training Step: 971...  Training loss: 1.6225...  7.3383 sec/batch\n",
      "Epoch: 5/20...  Training Step: 972...  Training loss: 1.6179...  7.4971 sec/batch\n",
      "Epoch: 5/20...  Training Step: 973...  Training loss: 1.6096...  7.8899 sec/batch\n",
      "Epoch: 5/20...  Training Step: 974...  Training loss: 1.6266...  7.1958 sec/batch\n",
      "Epoch: 5/20...  Training Step: 975...  Training loss: 1.6441...  7.7310 sec/batch\n",
      "Epoch: 5/20...  Training Step: 976...  Training loss: 1.6529...  8.4221 sec/batch\n",
      "Epoch: 5/20...  Training Step: 977...  Training loss: 1.6356...  7.9627 sec/batch\n",
      "Epoch: 5/20...  Training Step: 978...  Training loss: 1.6269...  8.0987 sec/batch\n",
      "Epoch: 5/20...  Training Step: 979...  Training loss: 1.6653...  8.3493 sec/batch\n",
      "Epoch: 5/20...  Training Step: 980...  Training loss: 1.6265...  8.5120 sec/batch\n",
      "Epoch: 5/20...  Training Step: 981...  Training loss: 1.6490...  7.8682 sec/batch\n",
      "Epoch: 5/20...  Training Step: 982...  Training loss: 1.6460...  7.3336 sec/batch\n",
      "Epoch: 5/20...  Training Step: 983...  Training loss: 1.6419...  9.3871 sec/batch\n",
      "Epoch: 5/20...  Training Step: 984...  Training loss: 1.6199...  10.0843 sec/batch\n",
      "Epoch: 5/20...  Training Step: 985...  Training loss: 1.6448...  8.4965 sec/batch\n",
      "Epoch: 5/20...  Training Step: 986...  Training loss: 1.6195...  8.0018 sec/batch\n",
      "Epoch: 5/20...  Training Step: 987...  Training loss: 1.6117...  7.4720 sec/batch\n",
      "Epoch: 5/20...  Training Step: 988...  Training loss: 1.6444...  7.8097 sec/batch\n",
      "Epoch: 5/20...  Training Step: 989...  Training loss: 1.6343...  7.2208 sec/batch\n",
      "Epoch: 5/20...  Training Step: 990...  Training loss: 1.6263...  7.1895 sec/batch\n",
      "Epoch: 6/20...  Training Step: 991...  Training loss: 1.7324...  7.4082 sec/batch\n",
      "Epoch: 6/20...  Training Step: 992...  Training loss: 1.6352...  6.9926 sec/batch\n",
      "Epoch: 6/20...  Training Step: 993...  Training loss: 1.6343...  7.5223 sec/batch\n",
      "Epoch: 6/20...  Training Step: 994...  Training loss: 1.6360...  7.3051 sec/batch\n",
      "Epoch: 6/20...  Training Step: 995...  Training loss: 1.6150...  7.2537 sec/batch\n",
      "Epoch: 6/20...  Training Step: 996...  Training loss: 1.5996...  7.5328 sec/batch\n",
      "Epoch: 6/20...  Training Step: 997...  Training loss: 1.6316...  7.3214 sec/batch\n",
      "Epoch: 6/20...  Training Step: 998...  Training loss: 1.6144...  8.5462 sec/batch\n",
      "Epoch: 6/20...  Training Step: 999...  Training loss: 1.6555...  8.7063 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1000...  Training loss: 1.6222...  7.6630 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1001...  Training loss: 1.5943...  7.8876 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20...  Training Step: 1002...  Training loss: 1.6129...  7.3574 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1003...  Training loss: 1.6130...  8.8794 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1004...  Training loss: 1.6630...  7.2311 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1005...  Training loss: 1.6165...  6.8540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1006...  Training loss: 1.6226...  7.1659 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1007...  Training loss: 1.6473...  7.9753 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1008...  Training loss: 1.6674...  8.2362 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1009...  Training loss: 1.6444...  7.4032 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1010...  Training loss: 1.6518...  7.1825 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1011...  Training loss: 1.6241...  8.1056 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1012...  Training loss: 1.6603...  8.6529 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1013...  Training loss: 1.6306...  7.6819 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1014...  Training loss: 1.6411...  8.2511 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1015...  Training loss: 1.6352...  9.9177 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1016...  Training loss: 1.6016...  7.3202 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1017...  Training loss: 1.5814...  6.9198 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1018...  Training loss: 1.6379...  6.7011 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1019...  Training loss: 1.6466...  8.1778 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1020...  Training loss: 1.6339...  8.6158 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1021...  Training loss: 1.6249...  7.9045 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1022...  Training loss: 1.5993...  7.0716 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1023...  Training loss: 1.6390...  8.1962 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1024...  Training loss: 1.6500...  7.8833 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1025...  Training loss: 1.6123...  7.2830 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1026...  Training loss: 1.6183...  7.8382 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1027...  Training loss: 1.6055...  8.2849 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1028...  Training loss: 1.5840...  8.1504 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1029...  Training loss: 1.5727...  7.6909 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1030...  Training loss: 1.5937...  7.4566 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1031...  Training loss: 1.5959...  7.4520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1032...  Training loss: 1.6431...  7.8824 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1033...  Training loss: 1.6016...  7.0070 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1034...  Training loss: 1.5916...  7.3799 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1035...  Training loss: 1.6287...  8.7082 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1036...  Training loss: 1.5801...  7.4705 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1037...  Training loss: 1.6028...  8.5346 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1038...  Training loss: 1.6015...  7.6368 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1039...  Training loss: 1.5980...  8.6758 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1040...  Training loss: 1.6408...  8.2494 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1041...  Training loss: 1.5948...  7.3755 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1042...  Training loss: 1.6722...  7.5291 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1043...  Training loss: 1.6186...  9.8586 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1044...  Training loss: 1.6195...  8.7399 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1045...  Training loss: 1.6075...  7.3860 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1046...  Training loss: 1.6184...  8.6043 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1047...  Training loss: 1.6380...  8.4765 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1048...  Training loss: 1.5999...  8.8559 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1049...  Training loss: 1.5985...  9.6075 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1050...  Training loss: 1.6476...  9.3041 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1051...  Training loss: 1.6099...  9.2057 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1052...  Training loss: 1.6600...  8.9986 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1053...  Training loss: 1.6337...  7.5279 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1054...  Training loss: 1.6224...  7.3218 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1055...  Training loss: 1.6101...  7.0699 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1056...  Training loss: 1.6366...  10.1613 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1057...  Training loss: 1.6280...  7.9352 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1058...  Training loss: 1.5933...  8.2724 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1059...  Training loss: 1.6055...  9.2578 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1060...  Training loss: 1.6009...  11.0361 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1061...  Training loss: 1.6567...  11.1397 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1062...  Training loss: 1.6235...  8.1141 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1063...  Training loss: 1.6511...  6.6713 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1064...  Training loss: 1.5981...  6.2252 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1065...  Training loss: 1.6075...  7.9324 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1066...  Training loss: 1.6422...  9.0093 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1067...  Training loss: 1.6043...  7.5559 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1068...  Training loss: 1.6061...  7.2073 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1069...  Training loss: 1.5715...  8.6650 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1070...  Training loss: 1.5970...  7.9948 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1071...  Training loss: 1.5578...  7.3106 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1072...  Training loss: 1.6082...  6.4016 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1073...  Training loss: 1.5683...  7.9546 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1074...  Training loss: 1.5957...  8.9275 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1075...  Training loss: 1.5719...  7.1523 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1076...  Training loss: 1.5942...  7.0685 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1077...  Training loss: 1.5870...  6.6282 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1078...  Training loss: 1.5981...  8.9107 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1079...  Training loss: 1.5760...  8.2152 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1080...  Training loss: 1.6073...  8.5375 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1081...  Training loss: 1.5780...  9.9739 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1082...  Training loss: 1.5961...  6.9824 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1083...  Training loss: 1.5900...  6.2024 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1084...  Training loss: 1.5839...  6.2481 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1085...  Training loss: 1.5845...  6.1447 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1086...  Training loss: 1.5995...  6.1298 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1087...  Training loss: 1.6069...  6.2577 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1088...  Training loss: 1.5594...  6.2262 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1089...  Training loss: 1.5876...  6.2453 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1090...  Training loss: 1.5682...  6.2105 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1091...  Training loss: 1.6073...  6.1604 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1092...  Training loss: 1.5902...  6.1503 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1093...  Training loss: 1.5824...  6.1047 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1094...  Training loss: 1.5815...  6.5876 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1095...  Training loss: 1.5880...  6.3603 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1096...  Training loss: 1.5869...  6.2972 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1097...  Training loss: 1.5880...  6.1368 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1098...  Training loss: 1.6079...  6.1279 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1099...  Training loss: 1.5936...  6.2120 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1100...  Training loss: 1.6093...  6.4052 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20...  Training Step: 1101...  Training loss: 1.5811...  6.6491 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1102...  Training loss: 1.5931...  6.1121 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1103...  Training loss: 1.5845...  6.2209 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1104...  Training loss: 1.5774...  6.1360 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1105...  Training loss: 1.5716...  6.0849 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1106...  Training loss: 1.5465...  6.2872 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1107...  Training loss: 1.5929...  6.1289 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1108...  Training loss: 1.5880...  6.1471 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1109...  Training loss: 1.5821...  6.1676 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1110...  Training loss: 1.5751...  6.1801 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1111...  Training loss: 1.5889...  6.1116 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1112...  Training loss: 1.5372...  6.1304 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1113...  Training loss: 1.5412...  6.2055 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1114...  Training loss: 1.5983...  6.6042 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1115...  Training loss: 1.5753...  6.2307 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1116...  Training loss: 1.5342...  6.3663 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1117...  Training loss: 1.5944...  6.1867 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1118...  Training loss: 1.5948...  6.1724 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1119...  Training loss: 1.5733...  6.4991 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1120...  Training loss: 1.5477...  6.1894 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1121...  Training loss: 1.5388...  6.0945 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1122...  Training loss: 1.5491...  6.1711 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1123...  Training loss: 1.6006...  6.0874 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1124...  Training loss: 1.5952...  6.1743 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1125...  Training loss: 1.6008...  6.2196 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1126...  Training loss: 1.5809...  6.2358 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1127...  Training loss: 1.5985...  6.1711 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1128...  Training loss: 1.5952...  6.1665 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1129...  Training loss: 1.5908...  6.1347 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1130...  Training loss: 1.5721...  6.1020 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1131...  Training loss: 1.6287...  6.1267 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1132...  Training loss: 1.5695...  6.1444 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1133...  Training loss: 1.5785...  6.1200 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1134...  Training loss: 1.6015...  6.1560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1135...  Training loss: 1.5685...  6.1807 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1136...  Training loss: 1.5940...  6.1201 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1137...  Training loss: 1.5907...  6.1429 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1138...  Training loss: 1.6031...  6.1378 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1139...  Training loss: 1.5957...  6.1387 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1140...  Training loss: 1.5762...  6.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1141...  Training loss: 1.5420...  6.0920 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1142...  Training loss: 1.5831...  6.1862 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1143...  Training loss: 1.5870...  6.1572 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1144...  Training loss: 1.5867...  6.1255 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1145...  Training loss: 1.5730...  6.2741 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1146...  Training loss: 1.5745...  6.1366 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1147...  Training loss: 1.5966...  6.1746 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1148...  Training loss: 1.5723...  6.1421 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1149...  Training loss: 1.5459...  6.1185 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1150...  Training loss: 1.5969...  6.1517 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1151...  Training loss: 1.6143...  6.2474 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1152...  Training loss: 1.5810...  6.1067 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1153...  Training loss: 1.5871...  6.2380 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1154...  Training loss: 1.5757...  6.8591 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1155...  Training loss: 1.5684...  7.0623 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1156...  Training loss: 1.5680...  6.1820 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1157...  Training loss: 1.5954...  6.2755 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1158...  Training loss: 1.6334...  6.1760 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1159...  Training loss: 1.5706...  6.0947 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1160...  Training loss: 1.5621...  6.1629 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1161...  Training loss: 1.5540...  6.1008 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1162...  Training loss: 1.5515...  6.1620 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1163...  Training loss: 1.5922...  6.0954 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1164...  Training loss: 1.5822...  6.1300 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1165...  Training loss: 1.5823...  6.3348 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1166...  Training loss: 1.5536...  6.1782 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1167...  Training loss: 1.5418...  6.1382 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1168...  Training loss: 1.5844...  6.0918 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1169...  Training loss: 1.5359...  6.1092 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1170...  Training loss: 1.5303...  6.7015 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1171...  Training loss: 1.5326...  6.1673 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1172...  Training loss: 1.5537...  6.1109 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1173...  Training loss: 1.5644...  6.1318 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1174...  Training loss: 1.5596...  6.0964 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1175...  Training loss: 1.5535...  6.1349 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1176...  Training loss: 1.5428...  6.1203 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1177...  Training loss: 1.5722...  6.1883 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1178...  Training loss: 1.5548...  6.1575 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1179...  Training loss: 1.5539...  6.1951 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1180...  Training loss: 1.5599...  6.1386 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1181...  Training loss: 1.5515...  6.1009 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1182...  Training loss: 1.5407...  6.1080 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1183...  Training loss: 1.5574...  6.1548 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1184...  Training loss: 1.5318...  6.2925 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1185...  Training loss: 1.5256...  6.1186 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1186...  Training loss: 1.5577...  6.2498 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1187...  Training loss: 1.5444...  6.1274 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1188...  Training loss: 1.5391...  6.0995 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1189...  Training loss: 1.6606...  6.2047 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1190...  Training loss: 1.5513...  6.0976 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1191...  Training loss: 1.5567...  6.1581 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1192...  Training loss: 1.5639...  6.1435 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1193...  Training loss: 1.5427...  6.1319 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1194...  Training loss: 1.5190...  6.1662 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1195...  Training loss: 1.5589...  6.1183 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1196...  Training loss: 1.5438...  6.1585 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1197...  Training loss: 1.5627...  6.1202 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1198...  Training loss: 1.5371...  6.1184 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1199...  Training loss: 1.5351...  6.1347 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20...  Training Step: 1200...  Training loss: 1.5275...  6.1763 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1201...  Training loss: 1.5386...  6.1465 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1202...  Training loss: 1.5746...  6.1990 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1203...  Training loss: 1.5378...  6.1235 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1204...  Training loss: 1.5246...  6.2994 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1205...  Training loss: 1.5570...  6.2113 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1206...  Training loss: 1.5732...  6.1286 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1207...  Training loss: 1.5529...  6.5972 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1208...  Training loss: 1.5675...  6.2194 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1209...  Training loss: 1.5334...  6.2278 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1210...  Training loss: 1.5657...  6.1068 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1211...  Training loss: 1.5363...  6.1637 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1212...  Training loss: 1.5485...  6.0529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1213...  Training loss: 1.5408...  6.1753 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1214...  Training loss: 1.4943...  6.0676 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1215...  Training loss: 1.5057...  6.1507 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1216...  Training loss: 1.5590...  6.1810 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1217...  Training loss: 1.5675...  6.1174 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1218...  Training loss: 1.5672...  6.1279 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1219...  Training loss: 1.5300...  6.0972 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1220...  Training loss: 1.5256...  6.1727 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1221...  Training loss: 1.5551...  6.1891 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1222...  Training loss: 1.5514...  6.1039 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1223...  Training loss: 1.5323...  6.1084 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1224...  Training loss: 1.5403...  6.3373 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1225...  Training loss: 1.5143...  6.0995 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1226...  Training loss: 1.4933...  6.1621 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1227...  Training loss: 1.4849...  6.1329 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1228...  Training loss: 1.5123...  6.2105 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1229...  Training loss: 1.5195...  6.1858 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1230...  Training loss: 1.5749...  6.1324 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1231...  Training loss: 1.5284...  6.9484 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1232...  Training loss: 1.5094...  6.1210 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1233...  Training loss: 1.5529...  6.1280 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1234...  Training loss: 1.5075...  6.2174 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1235...  Training loss: 1.5266...  6.1499 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1236...  Training loss: 1.5145...  6.1162 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1237...  Training loss: 1.5265...  6.1310 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1238...  Training loss: 1.5622...  6.2151 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1239...  Training loss: 1.5033...  6.1796 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1240...  Training loss: 1.5829...  6.0566 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1241...  Training loss: 1.5386...  6.2026 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1242...  Training loss: 1.5402...  6.1034 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1243...  Training loss: 1.5267...  6.3530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1244...  Training loss: 1.5380...  6.1351 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1245...  Training loss: 1.5532...  6.1465 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1246...  Training loss: 1.5194...  6.1831 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1247...  Training loss: 1.5097...  6.1885 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1248...  Training loss: 1.5611...  6.1396 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1249...  Training loss: 1.5243...  6.1558 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1250...  Training loss: 1.5754...  6.0915 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1251...  Training loss: 1.5525...  6.1453 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1252...  Training loss: 1.5527...  6.1212 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1253...  Training loss: 1.5315...  6.1779 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1254...  Training loss: 1.5474...  6.1909 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1255...  Training loss: 1.5463...  6.1388 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1256...  Training loss: 1.5124...  6.1495 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1257...  Training loss: 1.5334...  6.1216 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1258...  Training loss: 1.5208...  6.1258 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1259...  Training loss: 1.5783...  6.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1260...  Training loss: 1.5592...  6.5062 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1261...  Training loss: 1.5646...  6.1888 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1262...  Training loss: 1.5190...  6.1422 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1263...  Training loss: 1.5326...  6.2776 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1264...  Training loss: 1.5514...  6.2012 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1265...  Training loss: 1.5277...  6.1604 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1266...  Training loss: 1.5183...  6.1861 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1267...  Training loss: 1.4899...  6.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1268...  Training loss: 1.5335...  6.1747 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1269...  Training loss: 1.4878...  6.1869 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1270...  Training loss: 1.5352...  6.1883 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1271...  Training loss: 1.4705...  6.1683 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1272...  Training loss: 1.5138...  6.1528 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1273...  Training loss: 1.4968...  6.1402 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1274...  Training loss: 1.5243...  6.1612 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1275...  Training loss: 1.4886...  6.1522 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1276...  Training loss: 1.5049...  6.0888 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1277...  Training loss: 1.4845...  6.1279 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1278...  Training loss: 1.5289...  6.1789 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1279...  Training loss: 1.4959...  6.1294 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1280...  Training loss: 1.5073...  6.1693 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1281...  Training loss: 1.4930...  6.1450 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1282...  Training loss: 1.5010...  6.2463 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1283...  Training loss: 1.4924...  6.4361 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1284...  Training loss: 1.5225...  6.2324 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1285...  Training loss: 1.5215...  6.2721 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1286...  Training loss: 1.4753...  6.1889 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1287...  Training loss: 1.4889...  6.2482 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1288...  Training loss: 1.4869...  6.1244 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1289...  Training loss: 1.5193...  6.1895 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1290...  Training loss: 1.5009...  6.1742 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1291...  Training loss: 1.5072...  6.1944 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1292...  Training loss: 1.5102...  6.1950 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1293...  Training loss: 1.5044...  6.1991 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1294...  Training loss: 1.5210...  6.1506 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1295...  Training loss: 1.5148...  6.1736 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1296...  Training loss: 1.5161...  6.1153 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1297...  Training loss: 1.5073...  6.2228 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1298...  Training loss: 1.5299...  6.1480 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20...  Training Step: 1299...  Training loss: 1.4998...  6.0952 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1300...  Training loss: 1.5062...  6.1640 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1301...  Training loss: 1.5139...  6.1007 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1302...  Training loss: 1.4952...  6.2249 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1303...  Training loss: 1.4820...  6.2984 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1304...  Training loss: 1.4678...  6.1579 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1305...  Training loss: 1.5068...  6.1777 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1306...  Training loss: 1.5181...  6.1541 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1307...  Training loss: 1.5039...  6.2104 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1308...  Training loss: 1.4964...  6.2112 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1309...  Training loss: 1.5052...  6.1417 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1310...  Training loss: 1.4675...  6.1967 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1311...  Training loss: 1.4698...  6.1844 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1312...  Training loss: 1.5270...  6.1612 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1313...  Training loss: 1.5067...  6.1522 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1314...  Training loss: 1.4655...  6.5590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1315...  Training loss: 1.5159...  6.1127 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1316...  Training loss: 1.5215...  6.2022 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1317...  Training loss: 1.5012...  6.1362 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1318...  Training loss: 1.4669...  6.1868 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1319...  Training loss: 1.4677...  6.2071 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1320...  Training loss: 1.4851...  6.1521 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1321...  Training loss: 1.5210...  6.1803 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1322...  Training loss: 1.5182...  6.2378 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1323...  Training loss: 1.5121...  6.2659 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1324...  Training loss: 1.5096...  6.1529 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1325...  Training loss: 1.5316...  6.1521 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1326...  Training loss: 1.5163...  6.1957 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1327...  Training loss: 1.5187...  6.1576 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1328...  Training loss: 1.5068...  6.1855 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1329...  Training loss: 1.5696...  6.0991 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1330...  Training loss: 1.5061...  6.1928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1331...  Training loss: 1.5125...  6.1603 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1332...  Training loss: 1.5231...  6.1843 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1333...  Training loss: 1.4914...  6.1303 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1334...  Training loss: 1.5156...  6.1032 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1335...  Training loss: 1.5130...  6.1594 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1336...  Training loss: 1.5404...  6.1934 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1337...  Training loss: 1.5214...  6.1096 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1338...  Training loss: 1.4979...  6.1213 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1339...  Training loss: 1.4681...  6.1876 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1340...  Training loss: 1.4907...  6.1116 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1341...  Training loss: 1.5154...  6.1610 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1342...  Training loss: 1.5062...  6.2329 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1343...  Training loss: 1.5093...  6.1876 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1344...  Training loss: 1.5072...  6.1685 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1345...  Training loss: 1.5091...  6.1581 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1346...  Training loss: 1.4842...  6.1110 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1347...  Training loss: 1.4600...  6.1042 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1348...  Training loss: 1.5218...  6.1598 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1349...  Training loss: 1.5372...  6.1354 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1350...  Training loss: 1.5003...  6.1327 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1351...  Training loss: 1.5135...  6.1171 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1352...  Training loss: 1.5050...  6.1647 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1353...  Training loss: 1.5006...  6.2402 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1354...  Training loss: 1.4946...  6.7062 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1355...  Training loss: 1.5166...  6.7306 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1356...  Training loss: 1.5775...  6.2662 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1357...  Training loss: 1.4914...  6.2346 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1358...  Training loss: 1.4967...  6.7763 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1359...  Training loss: 1.5008...  6.6749 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1360...  Training loss: 1.4756...  6.3393 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1361...  Training loss: 1.5191...  6.4439 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1362...  Training loss: 1.5001...  6.3273 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1363...  Training loss: 1.5197...  6.1818 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1364...  Training loss: 1.4766...  6.2550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1365...  Training loss: 1.4760...  6.1360 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1366...  Training loss: 1.5282...  6.1384 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1367...  Training loss: 1.4748...  6.5980 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1368...  Training loss: 1.4623...  6.1159 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1369...  Training loss: 1.4706...  6.1318 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1370...  Training loss: 1.4857...  6.1412 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1371...  Training loss: 1.4960...  6.1608 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1372...  Training loss: 1.4961...  6.1157 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1373...  Training loss: 1.4833...  6.1083 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1374...  Training loss: 1.4763...  6.1792 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1375...  Training loss: 1.5085...  6.1865 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1376...  Training loss: 1.4913...  6.0761 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1377...  Training loss: 1.4815...  6.1810 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1378...  Training loss: 1.4940...  6.1531 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1379...  Training loss: 1.4815...  6.2096 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1380...  Training loss: 1.4712...  6.1894 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1381...  Training loss: 1.4943...  6.2543 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1382...  Training loss: 1.4563...  6.1594 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1383...  Training loss: 1.4488...  6.1250 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1384...  Training loss: 1.4939...  6.1901 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1385...  Training loss: 1.4865...  6.1250 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1386...  Training loss: 1.4688...  6.0995 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1387...  Training loss: 1.6215...  6.1740 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1388...  Training loss: 1.5101...  6.2464 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1389...  Training loss: 1.4872...  6.2371 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1390...  Training loss: 1.5085...  6.2306 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1391...  Training loss: 1.4749...  6.0868 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1392...  Training loss: 1.4657...  6.1333 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1393...  Training loss: 1.4942...  6.1190 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1394...  Training loss: 1.4784...  6.1302 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1395...  Training loss: 1.5070...  6.1516 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1396...  Training loss: 1.4839...  6.1132 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1397...  Training loss: 1.4712...  6.1527 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20...  Training Step: 1398...  Training loss: 1.4750...  6.2316 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1399...  Training loss: 1.4837...  6.1262 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1400...  Training loss: 1.5107...  6.2930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1401...  Training loss: 1.4783...  6.1814 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1402...  Training loss: 1.4662...  6.1805 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1403...  Training loss: 1.4954...  6.2381 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1404...  Training loss: 1.5194...  6.2079 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1405...  Training loss: 1.4917...  6.1686 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1406...  Training loss: 1.5148...  6.2585 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1407...  Training loss: 1.4831...  6.1284 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1408...  Training loss: 1.5000...  6.1318 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1409...  Training loss: 1.4673...  6.2097 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1410...  Training loss: 1.4960...  6.1691 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1411...  Training loss: 1.4921...  6.1211 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1412...  Training loss: 1.4422...  6.1882 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1413...  Training loss: 1.4458...  6.1324 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1414...  Training loss: 1.4933...  6.1617 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1415...  Training loss: 1.4958...  6.1321 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1416...  Training loss: 1.4963...  6.1252 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1417...  Training loss: 1.4753...  6.1471 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1418...  Training loss: 1.4555...  6.1560 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1419...  Training loss: 1.4991...  6.2169 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1420...  Training loss: 1.4884...  6.6883 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1421...  Training loss: 1.4753...  6.1856 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1422...  Training loss: 1.4787...  6.2275 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1423...  Training loss: 1.4555...  6.1223 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1424...  Training loss: 1.4368...  6.1159 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1425...  Training loss: 1.4354...  6.1432 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1426...  Training loss: 1.4661...  6.2238 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1427...  Training loss: 1.4506...  6.1991 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1428...  Training loss: 1.5058...  6.1548 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1429...  Training loss: 1.4560...  6.1317 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1430...  Training loss: 1.4435...  6.1100 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1431...  Training loss: 1.4931...  6.1958 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1432...  Training loss: 1.4358...  6.1819 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1433...  Training loss: 1.4620...  6.1016 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1434...  Training loss: 1.4642...  6.1867 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1435...  Training loss: 1.4635...  6.1658 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1436...  Training loss: 1.4927...  6.1417 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1437...  Training loss: 1.4462...  6.2195 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1438...  Training loss: 1.5155...  6.0555 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1439...  Training loss: 1.4711...  6.1852 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1440...  Training loss: 1.4825...  6.3117 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1441...  Training loss: 1.4651...  6.0884 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1442...  Training loss: 1.4802...  6.2025 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1443...  Training loss: 1.4988...  6.1026 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1444...  Training loss: 1.4518...  6.1734 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1445...  Training loss: 1.4505...  6.1178 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1446...  Training loss: 1.5083...  6.1976 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1447...  Training loss: 1.4712...  6.1326 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1448...  Training loss: 1.5312...  6.1648 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1449...  Training loss: 1.5053...  6.1698 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1450...  Training loss: 1.4869...  6.1779 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1451...  Training loss: 1.4677...  6.1109 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1452...  Training loss: 1.4940...  6.1846 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1453...  Training loss: 1.4848...  6.1050 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1454...  Training loss: 1.4448...  6.1236 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1455...  Training loss: 1.4757...  6.1493 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1456...  Training loss: 1.4708...  6.1551 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1457...  Training loss: 1.5193...  6.1481 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1458...  Training loss: 1.4889...  6.1513 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1459...  Training loss: 1.5028...  6.1332 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1460...  Training loss: 1.4513...  6.2007 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1461...  Training loss: 1.4655...  6.8870 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1462...  Training loss: 1.4905...  6.2827 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1463...  Training loss: 1.4668...  6.2744 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1464...  Training loss: 1.4580...  777.4878 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1465...  Training loss: 1.4306...  8.1252 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1466...  Training loss: 1.4757...  6.9943 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1467...  Training loss: 1.4254...  6.4385 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1468...  Training loss: 1.4753...  6.2053 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1469...  Training loss: 1.4222...  6.1744 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1470...  Training loss: 1.4630...  6.2209 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1471...  Training loss: 1.4402...  6.1876 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1472...  Training loss: 1.4550...  6.1736 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1473...  Training loss: 1.4397...  6.3932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1474...  Training loss: 1.4490...  6.2041 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1475...  Training loss: 1.4283...  6.1955 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1476...  Training loss: 1.4719...  6.2031 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1477...  Training loss: 1.4390...  6.0840 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1478...  Training loss: 1.4425...  6.3584 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1479...  Training loss: 1.4397...  6.1619 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1480...  Training loss: 1.4385...  6.1972 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1481...  Training loss: 1.4333...  6.1468 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1482...  Training loss: 1.4766...  6.1443 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1483...  Training loss: 1.4718...  6.1598 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1484...  Training loss: 1.4291...  6.1243 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1485...  Training loss: 1.4470...  6.2639 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1486...  Training loss: 1.4322...  6.1949 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1487...  Training loss: 1.4678...  6.0764 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1488...  Training loss: 1.4511...  6.2865 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1489...  Training loss: 1.4468...  8.8676 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1490...  Training loss: 1.4543...  6.2363 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1491...  Training loss: 1.4545...  6.1779 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1492...  Training loss: 1.4504...  6.2265 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1493...  Training loss: 1.4605...  8.7834 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1494...  Training loss: 1.4664...  9.3281 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1495...  Training loss: 1.4609...  7.5274 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1496...  Training loss: 1.4813...  6.1610 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20...  Training Step: 1497...  Training loss: 1.4382...  6.2409 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1498...  Training loss: 1.4598...  6.1569 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1499...  Training loss: 1.4581...  7.5981 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1500...  Training loss: 1.4437...  6.7763 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1501...  Training loss: 1.4267...  6.9863 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1502...  Training loss: 1.4129...  6.1937 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1503...  Training loss: 1.4609...  6.2147 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1504...  Training loss: 1.4611...  6.1589 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1505...  Training loss: 1.4537...  6.1942 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1506...  Training loss: 1.4339...  6.1665 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1507...  Training loss: 1.4459...  6.1688 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1508...  Training loss: 1.4208...  6.2141 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1509...  Training loss: 1.4043...  6.1806 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1510...  Training loss: 1.4674...  6.2120 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1511...  Training loss: 1.4535...  7.0487 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1512...  Training loss: 1.4105...  6.2428 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1513...  Training loss: 1.4604...  6.1367 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1514...  Training loss: 1.4698...  6.3067 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1515...  Training loss: 1.4481...  6.1937 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1516...  Training loss: 1.4245...  6.1748 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1517...  Training loss: 1.4102...  6.1955 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1518...  Training loss: 1.4313...  6.2191 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1519...  Training loss: 1.4670...  6.1413 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1520...  Training loss: 1.4597...  6.1843 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1521...  Training loss: 1.4609...  6.0924 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1522...  Training loss: 1.4547...  6.2846 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1523...  Training loss: 1.4867...  6.1575 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1524...  Training loss: 1.4781...  6.1147 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1525...  Training loss: 1.4731...  6.1923 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1526...  Training loss: 1.4547...  6.1735 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1527...  Training loss: 1.5031...  6.1420 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1528...  Training loss: 1.4517...  6.1902 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1529...  Training loss: 1.4484...  6.2702 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1530...  Training loss: 1.4788...  6.1437 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1531...  Training loss: 1.4413...  6.2309 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1532...  Training loss: 1.4787...  6.2597 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1533...  Training loss: 1.4622...  6.1482 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1534...  Training loss: 1.4882...  6.3603 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1535...  Training loss: 1.4829...  6.2250 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1536...  Training loss: 1.4416...  6.2180 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1537...  Training loss: 1.4165...  6.1860 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1538...  Training loss: 1.4440...  6.1957 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1539...  Training loss: 1.4663...  6.1870 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1540...  Training loss: 1.4416...  6.1986 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1541...  Training loss: 1.4453...  6.2015 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1542...  Training loss: 1.4483...  6.1918 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1543...  Training loss: 1.4611...  6.1356 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1544...  Training loss: 1.4474...  6.2310 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1545...  Training loss: 1.4096...  6.1552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1546...  Training loss: 1.4726...  6.1635 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1547...  Training loss: 1.4885...  6.1289 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1548...  Training loss: 1.4539...  6.1466 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1549...  Training loss: 1.4578...  6.2061 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1550...  Training loss: 1.4485...  6.1460 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1551...  Training loss: 1.4563...  6.1957 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1552...  Training loss: 1.4472...  6.1489 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1553...  Training loss: 1.4667...  6.3740 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1554...  Training loss: 1.5255...  6.2229 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1555...  Training loss: 1.4440...  6.1099 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1556...  Training loss: 1.4523...  6.2115 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1557...  Training loss: 1.4316...  6.2112 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1558...  Training loss: 1.4289...  6.1706 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1559...  Training loss: 1.4699...  6.2133 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1560...  Training loss: 1.4547...  6.1411 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1561...  Training loss: 1.4606...  6.2438 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1562...  Training loss: 1.4319...  6.2195 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1563...  Training loss: 1.4289...  6.2298 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1564...  Training loss: 1.4706...  6.6231 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1565...  Training loss: 1.4240...  6.2265 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1566...  Training loss: 1.4222...  6.1705 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1567...  Training loss: 1.4239...  6.1188 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1568...  Training loss: 1.4415...  6.2833 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1569...  Training loss: 1.4458...  6.1559 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1570...  Training loss: 1.4461...  6.5652 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1571...  Training loss: 1.4398...  6.4186 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1572...  Training loss: 1.4310...  6.3015 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1573...  Training loss: 1.4589...  6.2364 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1574...  Training loss: 1.4323...  6.2587 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1575...  Training loss: 1.4480...  6.2026 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1576...  Training loss: 1.4468...  6.1060 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1577...  Training loss: 1.4308...  6.2112 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1578...  Training loss: 1.4169...  6.1813 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1579...  Training loss: 1.4382...  6.1686 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1580...  Training loss: 1.4281...  6.1802 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1581...  Training loss: 1.4098...  6.0466 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1582...  Training loss: 1.4492...  6.1700 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1583...  Training loss: 1.4343...  6.1155 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1584...  Training loss: 1.4372...  6.1445 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1585...  Training loss: 1.5811...  6.1710 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1586...  Training loss: 1.4484...  6.1571 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1587...  Training loss: 1.4439...  6.2667 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1588...  Training loss: 1.4562...  6.1158 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1589...  Training loss: 1.4315...  6.0713 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1590...  Training loss: 1.4158...  6.1450 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1591...  Training loss: 1.4355...  6.1174 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1592...  Training loss: 1.4355...  6.2966 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1593...  Training loss: 1.4520...  6.1422 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1594...  Training loss: 1.4308...  6.1487 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1595...  Training loss: 1.4237...  6.1453 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20...  Training Step: 1596...  Training loss: 1.4282...  6.1858 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1597...  Training loss: 1.4384...  6.1394 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1598...  Training loss: 1.4673...  6.1208 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1599...  Training loss: 1.4320...  6.0915 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1600...  Training loss: 1.4255...  6.2074 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1601...  Training loss: 1.4513...  6.1494 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1602...  Training loss: 1.4706...  6.2084 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1603...  Training loss: 1.4465...  6.2202 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1604...  Training loss: 1.4663...  6.1699 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1605...  Training loss: 1.4294...  6.2059 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1606...  Training loss: 1.4599...  6.1408 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1607...  Training loss: 1.4316...  6.1593 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1608...  Training loss: 1.4423...  6.9563 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1609...  Training loss: 1.4412...  6.1834 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1610...  Training loss: 1.4037...  6.1488 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1611...  Training loss: 1.3996...  6.2995 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1612...  Training loss: 1.4662...  6.1850 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1613...  Training loss: 1.4472...  6.1927 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1614...  Training loss: 1.4554...  6.2528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1615...  Training loss: 1.4251...  6.5394 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1616...  Training loss: 1.4038...  6.1448 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1617...  Training loss: 1.4404...  6.9348 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1618...  Training loss: 1.4369...  6.2212 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1619...  Training loss: 1.4206...  6.1068 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1620...  Training loss: 1.4335...  6.1477 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1621...  Training loss: 1.4062...  48.4583 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1622...  Training loss: 1.3891...  18.3084 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1623...  Training loss: 1.3910...  6.2859 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1624...  Training loss: 1.4211...  127.3170 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1625...  Training loss: 1.4048...  7.1421 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1626...  Training loss: 1.4588...  6.3238 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1627...  Training loss: 1.4117...  8.5681 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1628...  Training loss: 1.4049...  9.3135 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1629...  Training loss: 1.4334...  1984.1090 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1630...  Training loss: 1.4019...  7.5059 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1631...  Training loss: 1.4181...  6.4656 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1632...  Training loss: 1.4291...  6.1927 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1633...  Training loss: 1.4157...  6.3410 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1634...  Training loss: 1.4507...  6.2026 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1635...  Training loss: 1.4017...  6.1362 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1636...  Training loss: 1.4712...  6.1533 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1637...  Training loss: 1.4384...  6.0776 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1638...  Training loss: 1.4419...  6.2860 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1639...  Training loss: 1.4229...  6.1769 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1640...  Training loss: 1.4285...  6.1322 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1641...  Training loss: 1.4553...  6.2030 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1642...  Training loss: 1.4143...  6.2811 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1643...  Training loss: 1.4032...  7.2103 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1644...  Training loss: 1.4628...  6.3381 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1645...  Training loss: 1.4300...  6.3960 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1646...  Training loss: 1.4757...  6.2534 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1647...  Training loss: 1.4575...  6.2395 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1648...  Training loss: 1.4363...  6.2547 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1649...  Training loss: 1.4260...  6.1643 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1650...  Training loss: 1.4396...  6.1312 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1651...  Training loss: 1.4513...  6.2005 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1652...  Training loss: 1.4056...  6.2177 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1653...  Training loss: 1.4246...  6.1324 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1654...  Training loss: 1.4164...  6.1265 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1655...  Training loss: 1.4775...  6.1037 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1656...  Training loss: 1.4506...  6.2033 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1657...  Training loss: 1.4682...  6.1232 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1658...  Training loss: 1.4154...  6.1999 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1659...  Training loss: 1.4238...  6.3062 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1660...  Training loss: 1.4504...  6.1402 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1661...  Training loss: 1.4297...  6.2524 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1662...  Training loss: 1.4229...  6.1838 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1663...  Training loss: 1.3797...  6.1293 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1664...  Training loss: 1.4374...  6.1865 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1665...  Training loss: 1.3830...  6.1651 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1666...  Training loss: 1.4390...  6.2079 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1667...  Training loss: 1.3838...  6.2246 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1668...  Training loss: 1.4246...  6.2063 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1669...  Training loss: 1.3995...  6.1835 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1670...  Training loss: 1.4247...  6.2264 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1671...  Training loss: 1.3942...  6.2213 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1672...  Training loss: 1.4070...  6.1767 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1673...  Training loss: 1.3929...  6.2042 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1674...  Training loss: 1.4385...  6.1460 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1675...  Training loss: 1.4038...  6.1855 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1676...  Training loss: 1.4205...  6.2203 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1677...  Training loss: 1.3959...  6.1950 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1678...  Training loss: 1.4085...  6.1766 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1679...  Training loss: 1.3983...  6.6354 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1680...  Training loss: 1.4269...  6.2063 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1681...  Training loss: 1.4273...  6.1952 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1682...  Training loss: 1.3876...  6.1668 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1683...  Training loss: 1.3988...  6.1616 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1684...  Training loss: 1.3831...  6.1499 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1685...  Training loss: 1.4240...  6.1602 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1686...  Training loss: 1.4101...  6.1738 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1687...  Training loss: 1.4191...  6.1709 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1688...  Training loss: 1.4232...  6.2110 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1689...  Training loss: 1.4110...  6.1993 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1690...  Training loss: 1.4184...  6.1398 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1691...  Training loss: 1.4310...  6.1732 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1692...  Training loss: 1.4220...  6.2191 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1693...  Training loss: 1.4067...  6.1291 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1694...  Training loss: 1.4325...  6.1567 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20...  Training Step: 1695...  Training loss: 1.4034...  6.1416 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1696...  Training loss: 1.4104...  6.2026 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1697...  Training loss: 1.4101...  6.1723 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1698...  Training loss: 1.4045...  6.1338 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1699...  Training loss: 1.3908...  6.2069 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1700...  Training loss: 1.3760...  6.1544 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1701...  Training loss: 1.4194...  6.1477 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1702...  Training loss: 1.4160...  6.1722 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1703...  Training loss: 1.4083...  6.1733 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1704...  Training loss: 1.4012...  6.1793 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1705...  Training loss: 1.4151...  6.1723 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1706...  Training loss: 1.3693...  6.2521 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1707...  Training loss: 1.3627...  6.1904 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1708...  Training loss: 1.4201...  6.2434 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1709...  Training loss: 1.4002...  6.1569 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1710...  Training loss: 1.3773...  6.2125 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1711...  Training loss: 1.4189...  6.2041 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1712...  Training loss: 1.4303...  6.2005 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1713...  Training loss: 1.4062...  6.1628 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1714...  Training loss: 1.3835...  6.1909 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1715...  Training loss: 1.3698...  6.1927 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1716...  Training loss: 1.3837...  6.2037 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1717...  Training loss: 1.4288...  6.1882 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1718...  Training loss: 1.4135...  6.1058 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1719...  Training loss: 1.4226...  6.2410 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1720...  Training loss: 1.4145...  6.2446 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1721...  Training loss: 1.4412...  6.1564 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1722...  Training loss: 1.4266...  6.1787 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1723...  Training loss: 1.4192...  6.1817 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1724...  Training loss: 1.4104...  6.1901 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1725...  Training loss: 1.4714...  6.2133 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1726...  Training loss: 1.4185...  6.1817 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1727...  Training loss: 1.4110...  6.1673 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1728...  Training loss: 1.4468...  6.2575 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1729...  Training loss: 1.4074...  6.1279 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1730...  Training loss: 1.4364...  6.2514 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1731...  Training loss: 1.4188...  6.4742 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1732...  Training loss: 1.4432...  6.4556 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1733...  Training loss: 1.4324...  6.3884 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1734...  Training loss: 1.4000...  6.1828 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1735...  Training loss: 1.3764...  6.1926 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1736...  Training loss: 1.3887...  6.1702 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1737...  Training loss: 1.4239...  6.1548 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1738...  Training loss: 1.4176...  6.2096 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1739...  Training loss: 1.4093...  6.1961 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1740...  Training loss: 1.4201...  6.2362 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1741...  Training loss: 1.4248...  6.1597 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1742...  Training loss: 1.4003...  6.2053 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1743...  Training loss: 1.3720...  6.2083 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1744...  Training loss: 1.4286...  6.1622 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1745...  Training loss: 1.4408...  6.1801 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1746...  Training loss: 1.4094...  6.1880 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1747...  Training loss: 1.4161...  6.1921 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1748...  Training loss: 1.4089...  6.1552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1749...  Training loss: 1.4042...  6.2269 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1750...  Training loss: 1.4032...  6.1302 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1751...  Training loss: 1.4371...  6.1388 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1752...  Training loss: 1.4831...  6.1843 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1753...  Training loss: 1.4096...  6.1595 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1754...  Training loss: 1.4132...  6.1845 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1755...  Training loss: 1.4072...  6.1234 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1756...  Training loss: 1.3934...  6.2673 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1757...  Training loss: 1.4379...  6.1607 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1758...  Training loss: 1.4176...  6.1948 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1759...  Training loss: 1.4363...  6.1565 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1760...  Training loss: 1.3952...  6.2001 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1761...  Training loss: 1.3915...  6.1952 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1762...  Training loss: 1.4317...  6.1936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1763...  Training loss: 1.3849...  6.2063 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1764...  Training loss: 1.3745...  6.1970 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1765...  Training loss: 1.3754...  6.2176 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1766...  Training loss: 1.4041...  6.2100 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1767...  Training loss: 1.4137...  6.1474 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1768...  Training loss: 1.3990...  6.2868 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1769...  Training loss: 1.4023...  6.1444 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1770...  Training loss: 1.3914...  6.2345 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1771...  Training loss: 1.4314...  6.1821 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1772...  Training loss: 1.4015...  6.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1773...  Training loss: 1.3934...  6.2377 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1774...  Training loss: 1.4120...  6.1475 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1775...  Training loss: 1.3873...  6.1765 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1776...  Training loss: 1.3851...  6.1600 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1777...  Training loss: 1.4175...  6.1866 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1778...  Training loss: 1.3857...  6.2027 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1779...  Training loss: 1.3719...  6.1941 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1780...  Training loss: 1.4165...  6.1854 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1781...  Training loss: 1.3932...  6.2434 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1782...  Training loss: 1.3848...  6.2650 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1783...  Training loss: 1.5541...  6.2162 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1784...  Training loss: 1.4125...  6.1926 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1785...  Training loss: 1.4160...  6.1480 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1786...  Training loss: 1.4215...  6.5686 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1787...  Training loss: 1.3837...  6.1748 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1788...  Training loss: 1.3844...  6.2503 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1789...  Training loss: 1.4062...  6.2114 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1790...  Training loss: 1.3994...  6.1910 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1791...  Training loss: 1.4143...  6.1741 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1792...  Training loss: 1.4049...  6.2250 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1793...  Training loss: 1.3939...  6.2016 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20...  Training Step: 1794...  Training loss: 1.4000...  6.2352 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1795...  Training loss: 1.4029...  6.1333 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1796...  Training loss: 1.4307...  6.1969 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1797...  Training loss: 1.3897...  6.1380 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1798...  Training loss: 1.3812...  6.1612 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1799...  Training loss: 1.4163...  6.1859 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1800...  Training loss: 1.4329...  6.1392 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1801...  Training loss: 1.4095...  6.1800 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1802...  Training loss: 1.4326...  6.1827 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1803...  Training loss: 1.3939...  6.1979 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1804...  Training loss: 1.4162...  6.2862 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1805...  Training loss: 1.4026...  6.2211 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1806...  Training loss: 1.4237...  6.2038 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1807...  Training loss: 1.4048...  6.2080 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1808...  Training loss: 1.3606...  6.1784 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1809...  Training loss: 1.3710...  6.1310 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1810...  Training loss: 1.4106...  6.2005 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1811...  Training loss: 1.4133...  6.1911 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1812...  Training loss: 1.4195...  6.1945 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1813...  Training loss: 1.3892...  6.1337 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1814...  Training loss: 1.3729...  6.1391 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1815...  Training loss: 1.4115...  6.2209 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1816...  Training loss: 1.4098...  6.1363 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1817...  Training loss: 1.3870...  6.1497 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1818...  Training loss: 1.4012...  6.1511 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1819...  Training loss: 1.3676...  6.1834 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1820...  Training loss: 1.3635...  6.1850 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1821...  Training loss: 1.3422...  6.1774 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1822...  Training loss: 1.3787...  6.1538 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1823...  Training loss: 1.3688...  6.1768 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1824...  Training loss: 1.4319...  6.1856 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1825...  Training loss: 1.3757...  6.2008 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1826...  Training loss: 1.3713...  6.2327 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1827...  Training loss: 1.4141...  6.1610 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1828...  Training loss: 1.3707...  6.1687 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1829...  Training loss: 1.3867...  6.1556 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1830...  Training loss: 1.3925...  6.1830 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1831...  Training loss: 1.3939...  6.2286 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1832...  Training loss: 1.4144...  6.1753 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1833...  Training loss: 1.3622...  6.2330 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1834...  Training loss: 1.4459...  6.1694 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1835...  Training loss: 1.3991...  6.1457 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1836...  Training loss: 1.3970...  6.1622 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1837...  Training loss: 1.3840...  6.2528 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1838...  Training loss: 1.4019...  6.1147 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1839...  Training loss: 1.4161...  6.6050 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1840...  Training loss: 1.3839...  6.2131 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1841...  Training loss: 1.3767...  6.2563 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1842...  Training loss: 1.4209...  6.2711 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1843...  Training loss: 1.3960...  6.1895 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1844...  Training loss: 1.4341...  6.1705 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1845...  Training loss: 1.4244...  6.1906 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1846...  Training loss: 1.4001...  6.1857 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1847...  Training loss: 1.3937...  6.1642 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1848...  Training loss: 1.4180...  6.1237 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1849...  Training loss: 1.4113...  6.1830 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1850...  Training loss: 1.3812...  6.1160 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1851...  Training loss: 1.3972...  6.2232 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1852...  Training loss: 1.3770...  6.1827 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1853...  Training loss: 1.4467...  6.1867 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1854...  Training loss: 1.4186...  6.2329 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1855...  Training loss: 1.4324...  6.2024 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1856...  Training loss: 1.3832...  6.1462 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1857...  Training loss: 1.4025...  6.1988 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1858...  Training loss: 1.4159...  6.2047 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1859...  Training loss: 1.3934...  6.1265 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1860...  Training loss: 1.3783...  6.2145 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1861...  Training loss: 1.3531...  6.1385 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1862...  Training loss: 1.4039...  6.2257 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1863...  Training loss: 1.3537...  6.1905 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1864...  Training loss: 1.3953...  6.1779 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1865...  Training loss: 1.3471...  6.1833 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1866...  Training loss: 1.3848...  6.1934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1867...  Training loss: 1.3677...  6.2342 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1868...  Training loss: 1.3936...  6.1913 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1869...  Training loss: 1.3709...  6.1843 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1870...  Training loss: 1.3728...  6.1615 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1871...  Training loss: 1.3585...  6.1761 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1872...  Training loss: 1.4073...  6.1489 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1873...  Training loss: 1.3690...  6.1884 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1874...  Training loss: 1.3806...  6.1858 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1875...  Training loss: 1.3638...  6.1154 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1876...  Training loss: 1.3716...  6.1387 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1877...  Training loss: 1.3634...  6.2033 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1878...  Training loss: 1.3965...  6.1378 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1879...  Training loss: 1.3895...  6.1594 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1880...  Training loss: 1.3582...  6.1459 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1881...  Training loss: 1.3616...  6.1732 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1882...  Training loss: 1.3524...  6.2199 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1883...  Training loss: 1.3821...  6.1578 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1884...  Training loss: 1.3816...  6.1737 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1885...  Training loss: 1.3839...  6.1659 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1886...  Training loss: 1.3778...  6.1809 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1887...  Training loss: 1.3748...  6.1694 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1888...  Training loss: 1.3896...  6.1747 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1889...  Training loss: 1.3899...  6.1349 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1890...  Training loss: 1.3840...  6.2020 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1891...  Training loss: 1.3799...  6.5569 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20...  Training Step: 1892...  Training loss: 1.3988...  7.9619 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1893...  Training loss: 1.3678...  6.2288 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1894...  Training loss: 1.3829...  6.1143 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1895...  Training loss: 1.3889...  6.2365 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1896...  Training loss: 1.3733...  6.2046 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1897...  Training loss: 1.3596...  6.1974 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1898...  Training loss: 1.3448...  6.2066 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1899...  Training loss: 1.3854...  6.2427 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1900...  Training loss: 1.3929...  6.1270 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1901...  Training loss: 1.3792...  6.1647 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1902...  Training loss: 1.3659...  6.1809 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1903...  Training loss: 1.3754...  6.1555 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1904...  Training loss: 1.3419...  6.2473 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1905...  Training loss: 1.3321...  6.1341 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1906...  Training loss: 1.3940...  6.1547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1907...  Training loss: 1.3743...  6.2696 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1908...  Training loss: 1.3340...  6.1677 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1909...  Training loss: 1.3934...  6.1485 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1910...  Training loss: 1.3926...  6.2357 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1911...  Training loss: 1.3664...  6.2058 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1912...  Training loss: 1.3393...  6.2177 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1913...  Training loss: 1.3394...  6.1925 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1914...  Training loss: 1.3672...  6.1484 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1915...  Training loss: 1.4101...  6.1819 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1916...  Training loss: 1.3919...  6.1665 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1917...  Training loss: 1.3889...  6.1717 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1918...  Training loss: 1.3847...  6.2044 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1919...  Training loss: 1.4202...  6.1623 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1920...  Training loss: 1.3994...  6.1802 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1921...  Training loss: 1.3879...  6.1779 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1922...  Training loss: 1.3801...  6.1963 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1923...  Training loss: 1.4424...  6.2083 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1924...  Training loss: 1.3842...  6.2567 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1925...  Training loss: 1.3751...  6.1699 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1926...  Training loss: 1.4034...  6.1874 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1927...  Training loss: 1.3721...  6.1846 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1928...  Training loss: 1.4034...  6.3990 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1929...  Training loss: 1.3905...  6.6727 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1930...  Training loss: 1.4318...  6.1846 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1931...  Training loss: 1.4149...  6.2491 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1932...  Training loss: 1.3768...  6.1725 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1933...  Training loss: 1.3439...  6.2108 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1934...  Training loss: 1.3612...  6.1657 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1935...  Training loss: 1.3936...  6.1424 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1936...  Training loss: 1.3770...  6.2027 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1937...  Training loss: 1.3733...  6.1318 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1938...  Training loss: 1.3789...  6.2073 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1939...  Training loss: 1.3910...  6.2196 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1940...  Training loss: 1.3702...  6.1999 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1941...  Training loss: 1.3399...  6.1666 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1942...  Training loss: 1.4062...  6.2214 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1943...  Training loss: 1.4180...  6.1709 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1944...  Training loss: 1.3808...  6.2011 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1945...  Training loss: 1.3874...  6.6391 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1946...  Training loss: 1.3778...  6.2473 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1947...  Training loss: 1.3686...  6.2118 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1948...  Training loss: 1.3753...  6.1780 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1949...  Training loss: 1.4008...  6.2281 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1950...  Training loss: 1.4527...  6.1477 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1951...  Training loss: 1.3837...  6.3728 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1952...  Training loss: 1.3840...  6.1759 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1953...  Training loss: 1.3699...  6.0989 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1954...  Training loss: 1.3637...  6.2198 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1955...  Training loss: 1.4140...  6.2407 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1956...  Training loss: 1.3886...  6.1368 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1957...  Training loss: 1.3893...  6.2060 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1958...  Training loss: 1.3470...  6.1795 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1959...  Training loss: 1.3624...  6.1876 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1960...  Training loss: 1.4050...  6.2650 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1961...  Training loss: 1.3521...  6.1793 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1962...  Training loss: 1.3458...  6.2036 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1963...  Training loss: 1.3522...  6.1733 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1964...  Training loss: 1.3679...  6.2256 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1965...  Training loss: 1.3763...  6.1576 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1966...  Training loss: 1.3761...  6.2173 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1967...  Training loss: 1.3670...  6.1082 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1968...  Training loss: 1.3654...  6.1997 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1969...  Training loss: 1.3919...  6.1573 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1970...  Training loss: 1.3786...  6.1705 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1971...  Training loss: 1.3700...  6.2119 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1972...  Training loss: 1.3739...  6.2079 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1973...  Training loss: 1.3526...  6.1021 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1974...  Training loss: 1.3597...  6.1125 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1975...  Training loss: 1.3820...  6.2055 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1976...  Training loss: 1.3541...  6.1724 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1977...  Training loss: 1.3458...  6.2014 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1978...  Training loss: 1.3848...  6.1546 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1979...  Training loss: 1.3699...  6.1728 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1980...  Training loss: 1.3575...  6.2237 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1981...  Training loss: 1.5137...  6.2163 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1982...  Training loss: 1.3890...  6.2109 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1983...  Training loss: 1.3757...  6.1716 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1984...  Training loss: 1.3854...  6.1529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1985...  Training loss: 1.3557...  6.1812 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1986...  Training loss: 1.3469...  6.2049 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1987...  Training loss: 1.3812...  6.2469 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1988...  Training loss: 1.3773...  6.1983 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1989...  Training loss: 1.3818...  6.2344 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 1990...  Training loss: 1.3729...  6.1567 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1991...  Training loss: 1.3669...  6.2079 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1992...  Training loss: 1.3657...  6.1518 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1993...  Training loss: 1.3701...  6.3778 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1994...  Training loss: 1.3939...  6.4505 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1995...  Training loss: 1.3765...  6.4638 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1996...  Training loss: 1.3573...  6.4017 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1997...  Training loss: 1.3856...  6.3656 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1998...  Training loss: 1.3978...  6.8182 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1999...  Training loss: 1.3738...  6.4983 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2000...  Training loss: 1.4069...  6.4569 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2001...  Training loss: 1.3810...  6.3989 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2002...  Training loss: 1.3896...  6.6506 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2003...  Training loss: 1.3683...  6.2429 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2004...  Training loss: 1.3816...  65.7492 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2005...  Training loss: 1.3778...  9.5668 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2006...  Training loss: 1.3427...  7.8259 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2007...  Training loss: 1.3380...  7.6866 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2008...  Training loss: 1.3806...  8.8769 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2009...  Training loss: 1.3881...  7.3902 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2010...  Training loss: 1.3935...  7.5674 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2011...  Training loss: 1.3602...  7.3728 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2012...  Training loss: 1.3496...  7.3152 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2013...  Training loss: 1.3707...  6.8700 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2014...  Training loss: 1.3753...  8.5331 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2015...  Training loss: 1.3636...  8.8959 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2016...  Training loss: 1.3776...  8.5735 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2017...  Training loss: 1.3352...  7.1986 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2018...  Training loss: 1.3213...  6.4954 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2019...  Training loss: 1.3202...  7.2001 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2020...  Training loss: 1.3491...  7.1512 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2021...  Training loss: 1.3480...  6.8530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2022...  Training loss: 1.3984...  6.8652 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2023...  Training loss: 1.3482...  7.3624 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2024...  Training loss: 1.3440...  6.9394 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2025...  Training loss: 1.3771...  7.6569 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2026...  Training loss: 1.3479...  8.1345 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2027...  Training loss: 1.3583...  9.0505 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2028...  Training loss: 1.3577...  6.9587 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2029...  Training loss: 1.3543...  7.0433 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2030...  Training loss: 1.3873...  6.2619 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2031...  Training loss: 1.3415...  6.3081 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2032...  Training loss: 1.3996...  6.5373 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2033...  Training loss: 1.3683...  7.0387 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2034...  Training loss: 1.3854...  7.4414 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2035...  Training loss: 1.3583...  7.2461 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2036...  Training loss: 1.3746...  6.2364 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2037...  Training loss: 1.3902...  6.9445 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2038...  Training loss: 1.3593...  6.7474 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2039...  Training loss: 1.3501...  8.4883 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2040...  Training loss: 1.3972...  6.5439 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2041...  Training loss: 1.3688...  6.4034 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2042...  Training loss: 1.4146...  6.3195 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2043...  Training loss: 1.3901...  6.3790 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2044...  Training loss: 1.3759...  6.3569 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2045...  Training loss: 1.3580...  7.2306 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2046...  Training loss: 1.3844...  6.1420 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2047...  Training loss: 1.3891...  6.1470 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2048...  Training loss: 1.3511...  6.1529 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2049...  Training loss: 1.3703...  6.1943 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2050...  Training loss: 1.3439...  6.2191 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2051...  Training loss: 1.4149...  6.2336 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2052...  Training loss: 1.3866...  6.1231 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2053...  Training loss: 1.4059...  6.1928 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2054...  Training loss: 1.3423...  6.2404 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2055...  Training loss: 1.3703...  6.2068 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2056...  Training loss: 1.3863...  6.1427 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2057...  Training loss: 1.3711...  6.1335 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2058...  Training loss: 1.3533...  6.2330 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2059...  Training loss: 1.3191...  6.1958 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2060...  Training loss: 1.3704...  6.1840 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2061...  Training loss: 1.3207...  6.4100 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2062...  Training loss: 1.3738...  6.3309 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2063...  Training loss: 1.3338...  6.1887 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2064...  Training loss: 1.3595...  6.1998 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2065...  Training loss: 1.3370...  6.4144 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2066...  Training loss: 1.3598...  6.4351 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2067...  Training loss: 1.3397...  7.7461 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2068...  Training loss: 1.3418...  7.0567 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2069...  Training loss: 1.3314...  6.8099 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2070...  Training loss: 1.3732...  6.4996 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2071...  Training loss: 1.3438...  7.4682 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2072...  Training loss: 1.3516...  7.4621 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2073...  Training loss: 1.3436...  7.8014 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2074...  Training loss: 1.3302...  6.2079 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2075...  Training loss: 1.3420...  7.3031 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2076...  Training loss: 1.3719...  7.1323 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2077...  Training loss: 1.3664...  6.2053 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2078...  Training loss: 1.3365...  6.2134 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2079...  Training loss: 1.3397...  6.1834 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2080...  Training loss: 1.3305...  6.1831 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2081...  Training loss: 1.3657...  6.2605 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2082...  Training loss: 1.3486...  6.1727 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2083...  Training loss: 1.3668...  6.7023 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2084...  Training loss: 1.3511...  7.3779 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2085...  Training loss: 1.3485...  8.7307 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2086...  Training loss: 1.3553...  7.2188 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2087...  Training loss: 1.3599...  8.1164 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 2088...  Training loss: 1.3593...  6.5957 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2089...  Training loss: 1.3489...  6.1043 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2090...  Training loss: 1.3716...  6.1480 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2091...  Training loss: 1.3407...  6.1135 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2092...  Training loss: 1.3553...  6.1689 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2093...  Training loss: 1.3589...  6.2171 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2094...  Training loss: 1.3445...  6.2141 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2095...  Training loss: 1.3342...  6.1690 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2096...  Training loss: 1.3240...  6.5191 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2097...  Training loss: 1.3571...  6.1200 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2098...  Training loss: 1.3687...  6.2499 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2099...  Training loss: 1.3450...  6.1798 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2100...  Training loss: 1.3527...  6.1749 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2101...  Training loss: 1.3464...  6.1373 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2102...  Training loss: 1.3229...  6.1449 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2103...  Training loss: 1.3090...  6.1540 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2104...  Training loss: 1.3596...  6.1693 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2105...  Training loss: 1.3542...  6.2066 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2106...  Training loss: 1.3157...  6.1413 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2107...  Training loss: 1.3642...  6.1617 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2108...  Training loss: 1.3620...  6.1526 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2109...  Training loss: 1.3456...  7.1364 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2110...  Training loss: 1.3136...  6.5516 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2111...  Training loss: 1.3079...  6.3349 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2112...  Training loss: 1.3286...  6.2300 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2113...  Training loss: 1.3696...  6.2136 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2114...  Training loss: 1.3638...  6.5615 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2115...  Training loss: 1.3604...  7.2595 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2116...  Training loss: 1.3469...  8.8800 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2117...  Training loss: 1.3912...  6.2070 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2118...  Training loss: 1.3709...  6.2328 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2119...  Training loss: 1.3650...  6.2234 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2120...  Training loss: 1.3609...  6.2922 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2121...  Training loss: 1.4084...  6.2446 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2122...  Training loss: 1.3577...  6.2827 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2123...  Training loss: 1.3458...  6.4190 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2124...  Training loss: 1.3820...  6.2518 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2125...  Training loss: 1.3432...  6.2002 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2126...  Training loss: 1.3772...  6.2742 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2127...  Training loss: 1.3681...  6.1767 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2128...  Training loss: 1.3879...  6.1651 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2129...  Training loss: 1.3812...  6.4809 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2130...  Training loss: 1.3497...  6.1938 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2131...  Training loss: 1.3254...  6.3332 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2132...  Training loss: 1.3299...  6.1896 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2133...  Training loss: 1.3695...  6.2379 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2134...  Training loss: 1.3531...  6.3430 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2135...  Training loss: 1.3458...  6.1703 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2136...  Training loss: 1.3592...  6.3092 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2137...  Training loss: 1.3702...  6.7947 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2138...  Training loss: 1.3447...  6.1941 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2139...  Training loss: 1.3151...  6.2121 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2140...  Training loss: 1.3743...  6.1720 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2141...  Training loss: 1.3859...  6.2048 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2142...  Training loss: 1.3587...  6.3901 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2143...  Training loss: 1.3528...  6.1811 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2144...  Training loss: 1.3514...  6.2901 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2145...  Training loss: 1.3559...  6.1438 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2146...  Training loss: 1.3417...  6.2109 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2147...  Training loss: 1.3756...  6.6184 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2148...  Training loss: 1.4214...  6.3938 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2149...  Training loss: 1.3600...  7.5537 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2150...  Training loss: 1.3524...  6.7577 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2151...  Training loss: 1.3445...  6.9104 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2152...  Training loss: 1.3311...  9.6592 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2153...  Training loss: 1.3892...  7.5381 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2154...  Training loss: 1.3609...  6.4409 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2155...  Training loss: 1.3665...  6.4949 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2156...  Training loss: 1.3341...  8.6491 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2157...  Training loss: 1.3400...  6.8357 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2158...  Training loss: 1.3824...  6.8810 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2159...  Training loss: 1.3267...  6.6655 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2160...  Training loss: 1.3175...  6.8124 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2161...  Training loss: 1.3245...  8.4217 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2162...  Training loss: 1.3440...  8.8477 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2163...  Training loss: 1.3507...  7.8709 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2164...  Training loss: 1.3362...  7.4766 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2165...  Training loss: 1.3507...  7.4689 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2166...  Training loss: 1.3357...  7.2997 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2167...  Training loss: 1.3772...  7.3784 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2168...  Training loss: 1.3425...  7.0010 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2169...  Training loss: 1.3409...  7.0135 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2170...  Training loss: 1.3427...  7.1966 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2171...  Training loss: 1.3319...  7.0970 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2172...  Training loss: 1.3322...  7.0481 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2173...  Training loss: 1.3557...  7.7585 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2174...  Training loss: 1.3314...  7.2257 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2175...  Training loss: 1.3163...  7.0162 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2176...  Training loss: 1.3493...  7.7559 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2177...  Training loss: 1.3418...  7.5829 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2178...  Training loss: 1.3339...  7.5016 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2179...  Training loss: 1.4739...  7.8728 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2180...  Training loss: 1.3658...  7.8406 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2181...  Training loss: 1.3411...  8.1187 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2182...  Training loss: 1.3633...  7.7445 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2183...  Training loss: 1.3308...  8.8483 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2184...  Training loss: 1.3239...  7.7091 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2185...  Training loss: 1.3587...  7.2579 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20...  Training Step: 2186...  Training loss: 1.3429...  7.1569 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2187...  Training loss: 1.3556...  7.5608 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2188...  Training loss: 1.3513...  7.9615 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2189...  Training loss: 1.3343...  9.8146 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2190...  Training loss: 1.3461...  9.3824 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2191...  Training loss: 1.3543...  8.5609 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2192...  Training loss: 1.3673...  8.5192 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2193...  Training loss: 1.3387...  9.2103 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2194...  Training loss: 1.3177...  8.2089 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2195...  Training loss: 1.3696...  7.5188 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2196...  Training loss: 1.3731...  7.4512 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2197...  Training loss: 1.3535...  6.6981 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2198...  Training loss: 1.3798...  6.4023 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2199...  Training loss: 1.3430...  6.3838 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2200...  Training loss: 1.3686...  7.1187 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2201...  Training loss: 1.3505...  6.4124 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2202...  Training loss: 1.3634...  7.2904 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2203...  Training loss: 1.3580...  7.2795 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2204...  Training loss: 1.3082...  6.8817 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2205...  Training loss: 1.3091...  6.3921 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2206...  Training loss: 1.3590...  6.2645 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2207...  Training loss: 1.3561...  6.4976 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2208...  Training loss: 1.3633...  6.4511 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2209...  Training loss: 1.3235...  6.2205 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2210...  Training loss: 1.3216...  7.3936 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2211...  Training loss: 1.3513...  6.4572 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2212...  Training loss: 1.3522...  6.4326 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2213...  Training loss: 1.3304...  7.0763 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2214...  Training loss: 1.3501...  7.5218 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2215...  Training loss: 1.3161...  7.8915 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2216...  Training loss: 1.3082...  7.4121 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2217...  Training loss: 1.3016...  7.3203 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2218...  Training loss: 1.3225...  7.5663 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2219...  Training loss: 1.3214...  7.3848 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2220...  Training loss: 1.3745...  7.1472 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2221...  Training loss: 1.3310...  6.7719 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2222...  Training loss: 1.3164...  6.5839 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2223...  Training loss: 1.3549...  6.7653 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2224...  Training loss: 1.3130...  6.9017 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2225...  Training loss: 1.3328...  6.7992 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2226...  Training loss: 1.3344...  7.2643 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2227...  Training loss: 1.3323...  7.7926 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2228...  Training loss: 1.3620...  7.6576 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2229...  Training loss: 1.3165...  7.8704 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2230...  Training loss: 1.3788...  7.5538 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2231...  Training loss: 1.3413...  7.6769 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2232...  Training loss: 1.3494...  8.2713 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2233...  Training loss: 1.3328...  7.6289 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2234...  Training loss: 1.3460...  6.6411 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2235...  Training loss: 1.3624...  7.2607 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2236...  Training loss: 1.3267...  7.0300 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2237...  Training loss: 1.3156...  6.2260 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2238...  Training loss: 1.3774...  6.4173 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2239...  Training loss: 1.3478...  6.5249 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2240...  Training loss: 1.3921...  6.1705 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2241...  Training loss: 1.3637...  6.2634 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2242...  Training loss: 1.3509...  6.6266 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2243...  Training loss: 1.3443...  7.4113 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2244...  Training loss: 1.3559...  7.8249 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2245...  Training loss: 1.3574...  7.4377 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2246...  Training loss: 1.3187...  7.9838 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2247...  Training loss: 1.3418...  6.7076 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2248...  Training loss: 1.3266...  7.3818 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2249...  Training loss: 1.3828...  7.0463 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2250...  Training loss: 1.3596...  8.0528 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2251...  Training loss: 1.3753...  7.1062 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2252...  Training loss: 1.3270...  7.2365 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2253...  Training loss: 1.3416...  6.9008 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2254...  Training loss: 1.3609...  6.8533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2255...  Training loss: 1.3466...  6.9911 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2256...  Training loss: 1.3270...  6.5787 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2257...  Training loss: 1.2999...  6.5817 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2258...  Training loss: 1.3432...  6.3834 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2259...  Training loss: 1.2959...  6.6338 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2260...  Training loss: 1.3449...  6.6631 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2261...  Training loss: 1.2995...  6.6403 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2262...  Training loss: 1.3323...  6.4116 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2263...  Training loss: 1.3116...  6.6095 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2264...  Training loss: 1.3411...  7.1094 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2265...  Training loss: 1.3103...  6.2774 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2266...  Training loss: 1.3085...  6.5507 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2267...  Training loss: 1.3115...  6.7512 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2268...  Training loss: 1.3476...  6.8367 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2269...  Training loss: 1.3184...  6.9353 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2270...  Training loss: 1.3263...  7.0043 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2271...  Training loss: 1.3184...  6.7006 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2272...  Training loss: 1.3125...  7.5298 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2273...  Training loss: 1.3147...  6.6332 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2274...  Training loss: 1.3485...  6.7472 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2275...  Training loss: 1.3501...  7.0788 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2276...  Training loss: 1.3061...  7.5109 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2277...  Training loss: 1.3158...  7.0067 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2278...  Training loss: 1.3134...  7.1273 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2279...  Training loss: 1.3420...  6.5413 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2280...  Training loss: 1.3199...  6.4431 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2281...  Training loss: 1.3333...  6.9790 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2282...  Training loss: 1.3241...  6.6639 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2283...  Training loss: 1.3289...  6.9423 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20...  Training Step: 2284...  Training loss: 1.3301...  7.3405 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2285...  Training loss: 1.3420...  7.5643 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2286...  Training loss: 1.3410...  7.2125 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2287...  Training loss: 1.3237...  6.9137 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2288...  Training loss: 1.3527...  6.3738 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2289...  Training loss: 1.3263...  6.1621 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2290...  Training loss: 1.3389...  6.1436 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2291...  Training loss: 1.3397...  6.1644 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2292...  Training loss: 1.3297...  6.1358 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2293...  Training loss: 1.3166...  6.2827 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2294...  Training loss: 1.2964...  6.6041 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2295...  Training loss: 1.3376...  6.4414 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2296...  Training loss: 1.3386...  7.0952 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2297...  Training loss: 1.3309...  6.6080 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2298...  Training loss: 1.3347...  6.7426 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2299...  Training loss: 1.3363...  6.3627 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2300...  Training loss: 1.2983...  6.1339 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2301...  Training loss: 1.2874...  6.1538 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2302...  Training loss: 1.3396...  7.2024 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2303...  Training loss: 1.3249...  6.7412 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2304...  Training loss: 1.2946...  6.9645 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2305...  Training loss: 1.3415...  6.2894 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2306...  Training loss: 1.3449...  6.2455 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2307...  Training loss: 1.3136...  8.6206 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2308...  Training loss: 1.2987...  9.4904 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2309...  Training loss: 1.2812...  7.1886 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2310...  Training loss: 1.3180...  7.3963 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2311...  Training loss: 1.3588...  7.4733 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2312...  Training loss: 1.3405...  7.2243 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2313...  Training loss: 1.3361...  7.2542 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2314...  Training loss: 1.3317...  8.0630 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2315...  Training loss: 1.3582...  6.9306 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2316...  Training loss: 1.3377...  6.5352 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2317...  Training loss: 1.3384...  6.6692 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2318...  Training loss: 1.3383...  8.1431 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2319...  Training loss: 1.3866...  7.1134 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2320...  Training loss: 1.3409...  7.0604 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2321...  Training loss: 1.3300...  6.7706 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2322...  Training loss: 1.3589...  7.4190 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2323...  Training loss: 1.3189...  6.8936 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2324...  Training loss: 1.3513...  6.8818 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2325...  Training loss: 1.3384...  8.0877 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2326...  Training loss: 1.3705...  7.0097 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2327...  Training loss: 1.3565...  7.2277 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2328...  Training loss: 1.3355...  7.9527 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2329...  Training loss: 1.3065...  6.9145 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2330...  Training loss: 1.3159...  6.7441 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2331...  Training loss: 1.3479...  7.5582 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2332...  Training loss: 1.3333...  7.6103 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2333...  Training loss: 1.3312...  8.1532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2334...  Training loss: 1.3325...  8.5266 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2335...  Training loss: 1.3290...  8.0858 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2336...  Training loss: 1.3290...  8.3601 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2337...  Training loss: 1.2927...  9.0209 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2338...  Training loss: 1.3459...  8.2531 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2339...  Training loss: 1.3602...  7.1987 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2340...  Training loss: 1.3363...  7.0253 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2341...  Training loss: 1.3356...  6.7366 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2342...  Training loss: 1.3294...  6.7873 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2343...  Training loss: 1.3266...  6.6610 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2344...  Training loss: 1.3241...  7.0004 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2345...  Training loss: 1.3579...  7.0615 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2346...  Training loss: 1.4044...  8.7497 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2347...  Training loss: 1.3344...  7.2986 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2348...  Training loss: 1.3405...  7.4439 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2349...  Training loss: 1.3192...  8.5551 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2350...  Training loss: 1.3149...  6.7296 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2351...  Training loss: 1.3687...  9.1323 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2352...  Training loss: 1.3310...  7.0633 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2353...  Training loss: 1.3459...  6.1615 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2354...  Training loss: 1.3082...  6.6121 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2355...  Training loss: 1.3099...  7.2535 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2356...  Training loss: 1.3584...  6.7081 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2357...  Training loss: 1.3023...  6.0875 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2358...  Training loss: 1.2954...  6.3364 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2359...  Training loss: 1.3060...  7.0743 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2360...  Training loss: 1.3291...  8.8482 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2361...  Training loss: 1.3248...  9.3208 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2362...  Training loss: 1.3187...  8.6290 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2363...  Training loss: 1.3199...  6.4268 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2364...  Training loss: 1.3055...  7.6694 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2365...  Training loss: 1.3531...  8.4557 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2366...  Training loss: 1.3192...  9.5637 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2367...  Training loss: 1.3172...  6.8585 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2368...  Training loss: 1.3228...  6.5943 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2369...  Training loss: 1.3048...  6.2385 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2370...  Training loss: 1.3110...  6.2146 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2371...  Training loss: 1.3298...  6.6551 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2372...  Training loss: 1.3014...  6.6527 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2373...  Training loss: 1.2945...  6.8491 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2374...  Training loss: 1.3292...  6.5631 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2375...  Training loss: 1.3099...  6.6360 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2376...  Training loss: 1.3044...  6.3836 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2377...  Training loss: 1.4602...  6.8045 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2378...  Training loss: 1.3367...  6.4958 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2379...  Training loss: 1.3289...  6.4414 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2380...  Training loss: 1.3412...  6.6431 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2381...  Training loss: 1.3165...  6.5883 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20...  Training Step: 2382...  Training loss: 1.2996...  6.6668 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2383...  Training loss: 1.3399...  7.4574 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2384...  Training loss: 1.3167...  8.5192 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2385...  Training loss: 1.3415...  6.7569 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2386...  Training loss: 1.3324...  6.8155 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2387...  Training loss: 1.3139...  7.2927 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2388...  Training loss: 1.3192...  7.0693 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2389...  Training loss: 1.3269...  6.5888 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2390...  Training loss: 1.3383...  6.2639 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2391...  Training loss: 1.3084...  6.2071 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2392...  Training loss: 1.3033...  6.6015 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2393...  Training loss: 1.3307...  7.0197 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2394...  Training loss: 1.3422...  6.9385 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2395...  Training loss: 1.3277...  6.7896 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2396...  Training loss: 1.3540...  6.7888 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2397...  Training loss: 1.3259...  6.9869 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2398...  Training loss: 1.3476...  6.8322 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2399...  Training loss: 1.3122...  8.0039 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2400...  Training loss: 1.3517...  8.2931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2401...  Training loss: 1.3257...  7.9695 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2402...  Training loss: 1.2843...  7.5318 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2403...  Training loss: 1.2900...  7.5887 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2404...  Training loss: 1.3395...  6.5637 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2405...  Training loss: 1.3287...  6.4885 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2406...  Training loss: 1.3406...  6.8988 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2407...  Training loss: 1.3010...  7.5497 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2408...  Training loss: 1.2915...  7.2860 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2409...  Training loss: 1.3397...  7.7239 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2410...  Training loss: 1.3330...  7.5556 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2411...  Training loss: 1.3076...  7.2409 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2412...  Training loss: 1.3265...  6.9075 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2413...  Training loss: 1.2978...  7.9871 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2414...  Training loss: 1.2767...  6.8389 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2415...  Training loss: 1.2742...  8.0938 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2416...  Training loss: 1.3034...  8.1709 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2417...  Training loss: 1.2955...  6.9787 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2418...  Training loss: 1.3656...  6.4038 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2419...  Training loss: 1.3040...  6.3183 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2420...  Training loss: 1.2976...  6.3320 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2421...  Training loss: 1.3324...  6.3636 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2422...  Training loss: 1.2990...  6.5859 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2423...  Training loss: 1.3251...  6.3508 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2424...  Training loss: 1.3188...  6.4260 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2425...  Training loss: 1.3198...  6.2978 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2426...  Training loss: 1.3329...  6.2623 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2427...  Training loss: 1.2951...  6.3312 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2428...  Training loss: 1.3663...  6.3737 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2429...  Training loss: 1.3211...  6.5869 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2430...  Training loss: 1.3401...  6.4989 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2431...  Training loss: 1.3143...  6.3828 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2432...  Training loss: 1.3159...  6.4372 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2433...  Training loss: 1.3399...  6.3916 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2434...  Training loss: 1.3004...  6.6932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2435...  Training loss: 1.2997...  6.7028 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2436...  Training loss: 1.3653...  7.3137 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2437...  Training loss: 1.3293...  7.3840 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2438...  Training loss: 1.3619...  6.5376 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2439...  Training loss: 1.3526...  6.9960 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2440...  Training loss: 1.3368...  6.6232 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2441...  Training loss: 1.3192...  7.0758 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2442...  Training loss: 1.3341...  6.7280 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2443...  Training loss: 1.3365...  6.9899 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2444...  Training loss: 1.3109...  7.6420 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2445...  Training loss: 1.3205...  6.7207 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2446...  Training loss: 1.3074...  7.2640 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2447...  Training loss: 1.3639...  7.2119 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2448...  Training loss: 1.3500...  7.5058 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2449...  Training loss: 1.3563...  7.4882 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2450...  Training loss: 1.3059...  6.9748 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2451...  Training loss: 1.3240...  6.4426 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2452...  Training loss: 1.3378...  6.3058 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2453...  Training loss: 1.3207...  6.7403 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2454...  Training loss: 1.3038...  7.3165 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2455...  Training loss: 1.2735...  7.7083 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2456...  Training loss: 1.3247...  7.1514 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2457...  Training loss: 1.2758...  7.6102 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2458...  Training loss: 1.3167...  6.2097 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2459...  Training loss: 1.2872...  6.3248 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2460...  Training loss: 1.3155...  6.3588 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2461...  Training loss: 1.2950...  7.6789 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2462...  Training loss: 1.3152...  7.7658 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2463...  Training loss: 1.2961...  7.6251 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2464...  Training loss: 1.2948...  7.1303 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2465...  Training loss: 1.2942...  7.9029 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2466...  Training loss: 1.3256...  6.3374 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2467...  Training loss: 1.2918...  6.8036 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2468...  Training loss: 1.3108...  7.7554 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2469...  Training loss: 1.2922...  7.6790 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2470...  Training loss: 1.2968...  6.8830 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2471...  Training loss: 1.3043...  6.4362 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2472...  Training loss: 1.3234...  7.7521 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2473...  Training loss: 1.3288...  8.9584 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2474...  Training loss: 1.2906...  9.0431 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2475...  Training loss: 1.2912...  7.3145 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2476...  Training loss: 1.2867...  7.1862 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2477...  Training loss: 1.3279...  7.6407 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2478...  Training loss: 1.3072...  7.8739 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2479...  Training loss: 1.3083...  7.8261 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20...  Training Step: 2480...  Training loss: 1.3036...  7.4084 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2481...  Training loss: 1.3078...  7.0867 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2482...  Training loss: 1.3108...  6.4762 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2483...  Training loss: 1.3170...  7.5394 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2484...  Training loss: 1.3139...  11.0419 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2485...  Training loss: 1.3074...  8.6519 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2486...  Training loss: 1.3215...  7.0812 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2487...  Training loss: 1.2966...  6.4143 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2488...  Training loss: 1.3181...  6.2173 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2489...  Training loss: 1.3221...  6.3176 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2490...  Training loss: 1.3056...  6.2290 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2491...  Training loss: 1.2925...  6.1304 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2492...  Training loss: 1.2704...  6.1749 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2493...  Training loss: 1.3178...  6.4322 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2494...  Training loss: 1.3242...  6.1216 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2495...  Training loss: 1.3109...  6.3137 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2496...  Training loss: 1.3085...  6.1340 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2497...  Training loss: 1.3116...  6.1843 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2498...  Training loss: 1.2806...  6.1309 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2499...  Training loss: 1.2656...  6.3184 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2500...  Training loss: 1.3255...  6.1691 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2501...  Training loss: 1.2997...  6.1508 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2502...  Training loss: 1.2728...  6.1806 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2503...  Training loss: 1.3218...  6.1326 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2504...  Training loss: 1.3088...  6.3506 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2505...  Training loss: 1.2980...  6.8354 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2506...  Training loss: 1.2701...  6.2173 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2507...  Training loss: 1.2618...  6.1911 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2508...  Training loss: 1.2996...  6.2292 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2509...  Training loss: 1.3277...  6.1721 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2510...  Training loss: 1.3211...  6.2185 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2511...  Training loss: 1.3110...  6.1383 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2512...  Training loss: 1.3169...  6.1946 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2513...  Training loss: 1.3458...  6.1623 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2514...  Training loss: 1.3292...  6.2339 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2515...  Training loss: 1.3250...  6.2003 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2516...  Training loss: 1.3264...  6.2020 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2517...  Training loss: 1.3604...  6.1898 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2518...  Training loss: 1.3162...  6.1772 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2519...  Training loss: 1.3006...  6.2324 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2520...  Training loss: 1.3489...  6.0844 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2521...  Training loss: 1.3003...  6.1856 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2522...  Training loss: 1.3359...  6.0940 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2523...  Training loss: 1.3229...  6.2206 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2524...  Training loss: 1.3484...  6.1485 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2525...  Training loss: 1.3404...  6.2070 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2526...  Training loss: 1.3050...  6.0789 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2527...  Training loss: 1.2809...  6.2520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2528...  Training loss: 1.2916...  6.2419 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2529...  Training loss: 1.3169...  6.1630 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2530...  Training loss: 1.3058...  6.2290 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2531...  Training loss: 1.3101...  6.1476 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2532...  Training loss: 1.3079...  6.1901 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2533...  Training loss: 1.3153...  6.1719 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2534...  Training loss: 1.3084...  6.1911 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2535...  Training loss: 1.2798...  6.1573 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2536...  Training loss: 1.3275...  6.1916 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2537...  Training loss: 1.3497...  6.1270 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2538...  Training loss: 1.3221...  6.2846 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2539...  Training loss: 1.3086...  6.0688 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2540...  Training loss: 1.3130...  6.1012 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2541...  Training loss: 1.3104...  6.1407 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2542...  Training loss: 1.3226...  6.2040 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2543...  Training loss: 1.3288...  6.2421 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2544...  Training loss: 1.3733...  6.0880 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2545...  Training loss: 1.3239...  6.2209 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2546...  Training loss: 1.3095...  6.1035 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2547...  Training loss: 1.3051...  6.0913 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2548...  Training loss: 1.2991...  6.1462 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2549...  Training loss: 1.3496...  6.2229 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2550...  Training loss: 1.3186...  6.1795 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2551...  Training loss: 1.3264...  6.0563 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2552...  Training loss: 1.2884...  6.2786 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2553...  Training loss: 1.2996...  6.0995 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2554...  Training loss: 1.3464...  6.2182 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2555...  Training loss: 1.2983...  6.1431 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2556...  Training loss: 1.2834...  6.1564 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2557...  Training loss: 1.2902...  6.4999 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2558...  Training loss: 1.3070...  6.1617 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2559...  Training loss: 1.3041...  6.3475 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2560...  Training loss: 1.3048...  6.1310 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2561...  Training loss: 1.2993...  6.2059 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2562...  Training loss: 1.2903...  6.0686 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2563...  Training loss: 1.3353...  6.0797 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2564...  Training loss: 1.2977...  6.2401 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2565...  Training loss: 1.2989...  6.2055 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2566...  Training loss: 1.2906...  6.2246 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2567...  Training loss: 1.2819...  6.2524 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2568...  Training loss: 1.2910...  6.2964 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2569...  Training loss: 1.3078...  6.0747 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2570...  Training loss: 1.2880...  6.2087 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2571...  Training loss: 1.2733...  6.2420 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2572...  Training loss: 1.3066...  6.0994 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2573...  Training loss: 1.2912...  6.2073 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2574...  Training loss: 1.2936...  6.0962 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2575...  Training loss: 1.4136...  6.1850 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2576...  Training loss: 1.3215...  6.1441 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2577...  Training loss: 1.3018...  6.1329 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20...  Training Step: 2578...  Training loss: 1.3286...  6.2448 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2579...  Training loss: 1.2975...  6.1071 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2580...  Training loss: 1.2696...  6.1997 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2581...  Training loss: 1.3101...  6.1589 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2582...  Training loss: 1.2981...  6.2475 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2583...  Training loss: 1.3013...  6.1501 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2584...  Training loss: 1.3061...  6.0928 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2585...  Training loss: 1.2926...  6.2796 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2586...  Training loss: 1.3086...  6.1318 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2587...  Training loss: 1.3037...  6.0726 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2588...  Training loss: 1.3166...  6.3607 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2589...  Training loss: 1.2863...  6.2785 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2590...  Training loss: 1.2800...  6.1276 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2591...  Training loss: 1.3246...  21.9747 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2592...  Training loss: 1.3203...  18.3023 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2593...  Training loss: 1.3033...  6.9072 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2594...  Training loss: 1.3308...  6.2541 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2595...  Training loss: 1.3166...  6.2215 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2596...  Training loss: 1.3275...  6.1879 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2597...  Training loss: 1.3051...  778.0133 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2598...  Training loss: 1.3190...  6.5110 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2599...  Training loss: 1.3180...  10.7128 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2600...  Training loss: 1.2661...  11.4512 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2601...  Training loss: 1.2794...  8.3536 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2602...  Training loss: 1.3231...  8.5290 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2603...  Training loss: 1.3151...  8.7602 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2604...  Training loss: 1.3308...  8.6104 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2605...  Training loss: 1.2984...  8.7299 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2606...  Training loss: 1.2838...  743.7092 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2607...  Training loss: 1.3095...  6.5470 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2608...  Training loss: 1.3220...  7.1604 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2609...  Training loss: 1.2921...  8.6623 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2610...  Training loss: 1.3071...  8.5026 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2611...  Training loss: 1.2880...  8.4035 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2612...  Training loss: 1.2634...  775.1064 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2613...  Training loss: 1.2550...  6.8714 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2614...  Training loss: 1.2915...  7.0951 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2615...  Training loss: 1.2705...  8.5222 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2616...  Training loss: 1.3462...  8.6407 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2617...  Training loss: 1.2853...  8.3254 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2618...  Training loss: 1.2777...  8.3942 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2619...  Training loss: 1.3167...  8.5339 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2620...  Training loss: 1.2733...  8.9174 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2621...  Training loss: 1.2934...  8.5384 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2622...  Training loss: 1.2911...  8.6886 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2623...  Training loss: 1.3011...  8.7086 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2624...  Training loss: 1.3214...  8.6919 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2625...  Training loss: 1.2720...  8.5858 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2626...  Training loss: 1.3374...  8.7422 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2627...  Training loss: 1.3079...  8.7177 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2628...  Training loss: 1.3088...  8.8242 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2629...  Training loss: 1.2897...  8.6922 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2630...  Training loss: 1.3056...  8.6734 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2631...  Training loss: 1.3234...  8.7169 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2632...  Training loss: 1.2906...  8.6950 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2633...  Training loss: 1.2817...  8.8985 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2634...  Training loss: 1.3382...  8.8147 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2635...  Training loss: 1.3048...  9.3267 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2636...  Training loss: 1.3425...  8.7873 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2637...  Training loss: 1.3322...  8.9314 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2638...  Training loss: 1.3045...  8.9249 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2639...  Training loss: 1.2979...  10.6497 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2640...  Training loss: 1.3106...  23.6291 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2641...  Training loss: 1.3177...  40.4854 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2642...  Training loss: 1.2892...  43.1092 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2643...  Training loss: 1.3098...  39.6160 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2644...  Training loss: 1.2879...  39.8493 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2645...  Training loss: 1.3573...  41.4241 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2646...  Training loss: 1.3285...  39.7643 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2647...  Training loss: 1.3405...  39.3577 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2648...  Training loss: 1.2895...  41.0591 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2649...  Training loss: 1.3004...  40.3574 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2650...  Training loss: 1.3198...  41.6964 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2651...  Training loss: 1.3076...  41.9435 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2652...  Training loss: 1.2881...  40.5232 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2653...  Training loss: 1.2519...  40.0948 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2654...  Training loss: 1.3036...  41.6651 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2655...  Training loss: 1.2668...  40.2574 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2656...  Training loss: 1.3080...  41.2696 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2657...  Training loss: 1.2724...  41.0126 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2658...  Training loss: 1.2920...  42.6879 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2659...  Training loss: 1.2853...  40.2577 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2660...  Training loss: 1.3013...  40.5498 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2661...  Training loss: 1.2852...  39.1315 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2662...  Training loss: 1.2853...  39.6824 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2663...  Training loss: 1.2637...  41.0529 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2664...  Training loss: 1.3073...  41.1272 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2665...  Training loss: 1.2833...  39.6844 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2666...  Training loss: 1.2904...  43.1729 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2667...  Training loss: 1.2789...  39.7570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2668...  Training loss: 1.2722...  40.1757 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2669...  Training loss: 1.2823...  41.6899 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2670...  Training loss: 1.3055...  40.4354 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2671...  Training loss: 1.2998...  40.3587 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2672...  Training loss: 1.2784...  40.9342 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2673...  Training loss: 1.2734...  39.3758 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2674...  Training loss: 1.2808...  41.7399 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20...  Training Step: 2675...  Training loss: 1.3042...  41.5146 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2676...  Training loss: 1.2851...  39.7088 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2677...  Training loss: 1.2966...  40.3652 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2678...  Training loss: 1.2892...  41.4755 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2679...  Training loss: 1.2926...  39.4293 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2680...  Training loss: 1.2861...  39.9943 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2681...  Training loss: 1.3020...  41.1428 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2682...  Training loss: 1.3036...  40.3946 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2683...  Training loss: 1.2815...  43.2634 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2684...  Training loss: 1.2989...  40.7817 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2685...  Training loss: 1.2810...  40.6678 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2686...  Training loss: 1.2991...  40.8959 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2687...  Training loss: 1.2973...  40.4683 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2688...  Training loss: 1.2844...  39.4040 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2689...  Training loss: 1.2813...  41.1894 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2690...  Training loss: 1.2597...  41.3935 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2691...  Training loss: 1.3048...  42.4354 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2692...  Training loss: 1.3044...  40.2753 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2693...  Training loss: 1.2947...  40.8669 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2694...  Training loss: 1.2915...  40.4226 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2695...  Training loss: 1.2839...  40.8491 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2696...  Training loss: 1.2682...  41.9666 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2697...  Training loss: 1.2576...  39.6725 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2698...  Training loss: 1.3025...  40.6452 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2699...  Training loss: 1.2774...  43.8180 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2700...  Training loss: 1.2522...  40.5381 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2701...  Training loss: 1.3001...  40.3775 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2702...  Training loss: 1.2928...  40.1784 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2703...  Training loss: 1.2732...  40.1684 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2704...  Training loss: 1.2573...  40.7129 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2705...  Training loss: 1.2489...  40.3900 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2706...  Training loss: 1.2786...  39.5406 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2707...  Training loss: 1.3182...  43.4501 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2708...  Training loss: 1.3022...  39.5791 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2709...  Training loss: 1.2997...  40.3598 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2710...  Training loss: 1.2909...  41.4041 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2711...  Training loss: 1.3277...  40.0955 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2712...  Training loss: 1.3114...  40.1414 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2713...  Training loss: 1.3018...  40.8259 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2714...  Training loss: 1.3048...  41.4072 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2715...  Training loss: 1.3426...  43.1664 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2716...  Training loss: 1.3068...  40.6228 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2717...  Training loss: 1.2936...  40.0905 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2718...  Training loss: 1.3244...  40.2229 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2719...  Training loss: 1.2850...  40.3063 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2720...  Training loss: 1.3148...  40.7517 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2721...  Training loss: 1.2989...  40.1701 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2722...  Training loss: 1.3349...  40.2409 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2723...  Training loss: 1.3209...  42.7071 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2724...  Training loss: 1.2867...  41.3341 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2725...  Training loss: 1.2632...  40.8544 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2726...  Training loss: 1.2749...  40.5358 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2727...  Training loss: 1.3056...  40.1050 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2728...  Training loss: 1.2946...  41.3328 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2729...  Training loss: 1.2843...  40.6641 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2730...  Training loss: 1.2936...  40.0272 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2731...  Training loss: 1.2896...  42.5820 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2732...  Training loss: 1.2819...  40.4320 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2733...  Training loss: 1.2602...  39.7038 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2734...  Training loss: 1.3161...  40.7094 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2735...  Training loss: 1.3184...  40.0088 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2736...  Training loss: 1.2967...  39.5674 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2737...  Training loss: 1.2918...  40.8762 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2738...  Training loss: 1.2949...  39.8597 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2739...  Training loss: 1.2972...  42.5661 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2740...  Training loss: 1.2857...  41.6885 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2741...  Training loss: 1.3127...  41.6700 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2742...  Training loss: 1.3575...  39.5444 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2743...  Training loss: 1.3050...  41.1840 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2744...  Training loss: 1.2953...  40.6342 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2745...  Training loss: 1.2829...  40.3213 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2746...  Training loss: 1.2844...  40.4376 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2747...  Training loss: 1.3276...  41.8149 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2748...  Training loss: 1.3053...  40.6451 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2749...  Training loss: 1.3104...  40.9892 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2750...  Training loss: 1.2640...  39.9062 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2751...  Training loss: 1.2852...  40.0529 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2752...  Training loss: 1.3199...  40.9698 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2753...  Training loss: 1.2767...  39.5696 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2754...  Training loss: 1.2589...  39.8217 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2755...  Training loss: 1.2769...  42.7744 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2756...  Training loss: 1.2911...  40.1343 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2757...  Training loss: 1.2960...  39.6891 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2758...  Training loss: 1.2918...  41.7418 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2759...  Training loss: 1.2807...  39.9445 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2760...  Training loss: 1.2756...  39.8887 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2761...  Training loss: 1.3111...  40.2657 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2762...  Training loss: 1.2899...  41.0598 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2763...  Training loss: 1.2907...  40.6122 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2764...  Training loss: 1.2877...  42.8086 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2765...  Training loss: 1.2626...  40.4042 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2766...  Training loss: 1.2747...  40.2304 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2767...  Training loss: 1.2937...  40.5818 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2768...  Training loss: 1.2699...  40.1029 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2769...  Training loss: 1.2552...  39.5527 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2770...  Training loss: 1.2943...  40.3832 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2771...  Training loss: 1.2757...  40.6684 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20...  Training Step: 2772...  Training loss: 1.2741...  42.7433 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2773...  Training loss: 1.4152...  41.0659 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2774...  Training loss: 1.3079...  40.7462 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2775...  Training loss: 1.2978...  39.6571 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2776...  Training loss: 1.3182...  41.0867 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2777...  Training loss: 1.2746...  41.0785 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2778...  Training loss: 1.2601...  39.6383 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2779...  Training loss: 1.3041...  41.2252 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2780...  Training loss: 1.2859...  43.2539 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2781...  Training loss: 1.3041...  40.1954 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2782...  Training loss: 1.2906...  40.2769 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2783...  Training loss: 1.2813...  41.5717 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2784...  Training loss: 1.2864...  41.0319 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2785...  Training loss: 1.2944...  41.5315 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2786...  Training loss: 1.2998...  40.0926 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2787...  Training loss: 1.2786...  40.5256 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2788...  Training loss: 1.2726...  43.7043 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2789...  Training loss: 1.3072...  39.6519 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2790...  Training loss: 1.3049...  39.6069 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2791...  Training loss: 1.2944...  40.9511 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2792...  Training loss: 1.3073...  39.6811 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2793...  Training loss: 1.2948...  39.8307 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2794...  Training loss: 1.3029...  41.6000 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2795...  Training loss: 1.2901...  39.6668 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2796...  Training loss: 1.3013...  43.0281 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2797...  Training loss: 1.2929...  40.5114 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2798...  Training loss: 1.2528...  40.1736 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2799...  Training loss: 1.2668...  39.7688 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2800...  Training loss: 1.3120...  41.0261 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2801...  Training loss: 1.3024...  39.2892 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2802...  Training loss: 1.3120...  40.0043 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2803...  Training loss: 1.2751...  40.7349 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2804...  Training loss: 1.2676...  42.6647 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2805...  Training loss: 1.2999...  39.7467 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2806...  Training loss: 1.2917...  41.1274 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2807...  Training loss: 1.2755...  40.4463 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2808...  Training loss: 1.3052...  39.7243 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2809...  Training loss: 1.2667...  40.9243 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2810...  Training loss: 1.2469...  39.5958 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2811...  Training loss: 1.2433...  39.4178 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2812...  Training loss: 1.2710...  43.7004 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2813...  Training loss: 1.2629...  40.9072 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2814...  Training loss: 1.3187...  39.6531 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2815...  Training loss: 1.2728...  41.7539 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2816...  Training loss: 1.2526...  39.9578 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2817...  Training loss: 1.2972...  40.1603 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2818...  Training loss: 1.2617...  41.5025 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2819...  Training loss: 1.2762...  39.8300 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2820...  Training loss: 1.2773...  42.1936 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2821...  Training loss: 1.2811...  41.4460 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2822...  Training loss: 1.2965...  40.0174 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2823...  Training loss: 1.2610...  39.9559 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2824...  Training loss: 1.3185...  40.9068 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2825...  Training loss: 1.2852...  39.9653 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2826...  Training loss: 1.2888...  39.8636 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2827...  Training loss: 1.2792...  41.7843 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2828...  Training loss: 1.2833...  41.8806 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2829...  Training loss: 1.3003...  41.1803 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2830...  Training loss: 1.2739...  40.0702 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2831...  Training loss: 1.2684...  40.5671 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2832...  Training loss: 1.3200...  40.6083 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2833...  Training loss: 1.2911...  41.5391 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2834...  Training loss: 1.3207...  40.3468 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2835...  Training loss: 1.3091...  39.9828 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2836...  Training loss: 1.2944...  43.3695 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2837...  Training loss: 1.2802...  41.1594 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2838...  Training loss: 1.2955...  40.2221 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2839...  Training loss: 1.3011...  40.4423 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2840...  Training loss: 1.2643...  40.1150 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2841...  Training loss: 1.2937...  40.3962 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2842...  Training loss: 1.2712...  40.4132 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2843...  Training loss: 1.3360...  39.8865 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2844...  Training loss: 1.3058...  41.9321 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2845...  Training loss: 1.3175...  41.7580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2846...  Training loss: 1.2623...  40.5585 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2847...  Training loss: 1.2873...  41.1764 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2848...  Training loss: 1.2977...  39.9178 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2849...  Training loss: 1.2946...  39.9837 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2850...  Training loss: 1.2745...  39.8272 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2851...  Training loss: 1.2450...  41.7319 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2852...  Training loss: 1.2936...  41.3375 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2853...  Training loss: 1.2413...  42.1784 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2854...  Training loss: 1.2873...  41.0705 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2855...  Training loss: 1.2544...  39.9879 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2856...  Training loss: 1.2890...  40.4461 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2857...  Training loss: 1.2674...  39.7094 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2858...  Training loss: 1.2833...  40.0440 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2859...  Training loss: 1.2664...  40.5323 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2860...  Training loss: 1.2632...  40.0707 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2861...  Training loss: 1.2500...  43.0212 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2862...  Training loss: 1.2943...  41.9571 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2863...  Training loss: 1.2496...  40.2849 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2864...  Training loss: 1.2704...  40.0144 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2865...  Training loss: 1.2501...  41.1842 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2866...  Training loss: 1.2556...  40.8603 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2867...  Training loss: 1.2643...  40.3964 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2868...  Training loss: 1.2907...  40.6871 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20...  Training Step: 2869...  Training loss: 1.2900...  42.3379 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2870...  Training loss: 1.2584...  39.9937 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2871...  Training loss: 1.2649...  41.4673 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2872...  Training loss: 1.2537...  39.9464 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2873...  Training loss: 1.2885...  39.7667 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2874...  Training loss: 1.2785...  42.4474 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2875...  Training loss: 1.2777...  40.7781 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2876...  Training loss: 1.2671...  39.8710 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2877...  Training loss: 1.2833...  44.4926 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2878...  Training loss: 1.2780...  40.0024 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2879...  Training loss: 1.2856...  40.7519 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2880...  Training loss: 1.2992...  41.1888 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2881...  Training loss: 1.2747...  39.5462 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2882...  Training loss: 1.2992...  39.4810 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2883...  Training loss: 1.2714...  41.4695 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2884...  Training loss: 1.2894...  40.2233 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2885...  Training loss: 1.2883...  42.4435 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2886...  Training loss: 1.2632...  40.9840 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2887...  Training loss: 1.2569...  39.3070 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2888...  Training loss: 1.2451...  39.9862 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2889...  Training loss: 1.2939...  41.1942 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2890...  Training loss: 1.2869...  40.9126 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2891...  Training loss: 1.2759...  40.0919 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2892...  Training loss: 1.2796...  40.6147 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2893...  Training loss: 1.2789...  42.7051 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2894...  Training loss: 1.2518...  41.5908 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2895...  Training loss: 1.2346...  42.5098 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2896...  Training loss: 1.2794...  41.9263 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2897...  Training loss: 1.2736...  40.7846 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2898...  Training loss: 1.2406...  40.6452 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2899...  Training loss: 1.2825...  40.9196 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2900...  Training loss: 1.2753...  41.8989 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2901...  Training loss: 1.2625...  46.0463 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2902...  Training loss: 1.2417...  40.1367 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2903...  Training loss: 1.2346...  41.3361 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2904...  Training loss: 1.2569...  41.2288 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2905...  Training loss: 1.2865...  40.1084 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2906...  Training loss: 1.2791...  40.9780 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2907...  Training loss: 1.2889...  39.8521 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2908...  Training loss: 1.2780...  39.2953 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2909...  Training loss: 1.3078...  43.4656 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2910...  Training loss: 1.2887...  40.7412 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2911...  Training loss: 1.2884...  39.5731 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2912...  Training loss: 1.2969...  40.4013 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2913...  Training loss: 1.3251...  39.9867 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2914...  Training loss: 1.2783...  39.9998 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2915...  Training loss: 1.2727...  41.6934 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2916...  Training loss: 1.3041...  40.0571 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2917...  Training loss: 1.2711...  42.7694 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2918...  Training loss: 1.3029...  41.3204 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2919...  Training loss: 1.2818...  40.5598 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2920...  Training loss: 1.3171...  39.3713 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2921...  Training loss: 1.3125...  41.2744 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2922...  Training loss: 1.2811...  39.6611 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2923...  Training loss: 1.2510...  40.1352 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2924...  Training loss: 1.2562...  41.3499 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2925...  Training loss: 1.2963...  41.7154 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2926...  Training loss: 1.2731...  39.9481 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2927...  Training loss: 1.2690...  41.2520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2928...  Training loss: 1.2703...  40.4307 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2929...  Training loss: 1.2842...  39.7573 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2930...  Training loss: 1.2825...  40.9246 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2931...  Training loss: 1.2486...  39.8112 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2932...  Training loss: 1.2961...  39.5345 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2933...  Training loss: 1.3015...  42.8291 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2934...  Training loss: 1.2829...  41.1876 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2935...  Training loss: 1.2807...  40.5043 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2936...  Training loss: 1.2810...  42.4346 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2937...  Training loss: 1.2831...  39.8495 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2938...  Training loss: 1.2742...  39.4316 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2939...  Training loss: 1.3003...  41.6365 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2940...  Training loss: 1.3349...  39.8468 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2941...  Training loss: 1.2917...  42.5937 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2942...  Training loss: 1.2799...  41.2039 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2943...  Training loss: 1.2720...  39.7985 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2944...  Training loss: 1.2695...  40.1346 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2945...  Training loss: 1.3128...  41.0112 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2946...  Training loss: 1.2912...  40.3435 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2947...  Training loss: 1.2910...  39.9576 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2948...  Training loss: 1.2610...  40.5244 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2949...  Training loss: 1.2750...  42.1972 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2950...  Training loss: 1.3089...  41.2775 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2951...  Training loss: 1.2574...  41.5603 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2952...  Training loss: 1.2613...  40.6958 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2953...  Training loss: 1.2621...  40.3744 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2954...  Training loss: 1.2712...  40.8348 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2955...  Training loss: 1.2829...  40.0872 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2956...  Training loss: 1.2679...  40.2479 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2957...  Training loss: 1.2748...  42.3319 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2958...  Training loss: 1.2474...  40.4603 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2959...  Training loss: 1.3064...  39.9788 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2960...  Training loss: 1.2688...  40.8582 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2961...  Training loss: 1.2747...  39.6173 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2962...  Training loss: 1.2717...  40.7431 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2963...  Training loss: 1.2503...  41.8847 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2964...  Training loss: 1.2534...  39.9876 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2965...  Training loss: 1.2900...  41.1945 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20...  Training Step: 2966...  Training loss: 1.2488...  43.4288 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2967...  Training loss: 1.2343...  39.8457 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2968...  Training loss: 1.2787...  40.5354 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2969...  Training loss: 1.2672...  41.7157 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2970...  Training loss: 1.2623...  40.3333 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2971...  Training loss: 1.3969...  39.9105 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2972...  Training loss: 1.2836...  41.1361 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2973...  Training loss: 1.2741...  39.7395 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2974...  Training loss: 1.3039...  42.3729 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2975...  Training loss: 1.2581...  40.8980 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2976...  Training loss: 1.2417...  40.2941 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2977...  Training loss: 1.2884...  40.3051 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2978...  Training loss: 1.2768...  41.1256 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2979...  Training loss: 1.2854...  40.2345 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2980...  Training loss: 1.2654...  40.1509 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2981...  Training loss: 1.2622...  41.2114 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2982...  Training loss: 1.2758...  42.2688 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2983...  Training loss: 1.2814...  40.3612 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2984...  Training loss: 1.2799...  40.6831 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2985...  Training loss: 1.2572...  39.9536 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2986...  Training loss: 1.2593...  40.2067 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2987...  Training loss: 1.2981...  40.5244 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2988...  Training loss: 1.2944...  39.5478 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2989...  Training loss: 1.2754...  40.8179 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2990...  Training loss: 1.2912...  43.7927 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2991...  Training loss: 1.2689...  40.2638 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2992...  Training loss: 1.3019...  38.8215 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2993...  Training loss: 1.2774...  40.9375 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2994...  Training loss: 1.2851...  40.9336 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2995...  Training loss: 1.2921...  40.0203 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2996...  Training loss: 1.2304...  41.7672 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2997...  Training loss: 1.2470...  39.9581 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2998...  Training loss: 1.2860...  43.0245 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2999...  Training loss: 1.2824...  40.8875 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3000...  Training loss: 1.2959...  40.2204 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3001...  Training loss: 1.2586...  39.2923 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3002...  Training loss: 1.2464...  40.4955 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3003...  Training loss: 1.2816...  40.2251 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3004...  Training loss: 1.2788...  40.2708 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3005...  Training loss: 1.2621...  41.2381 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3006...  Training loss: 1.2816...  42.9789 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3007...  Training loss: 1.2523...  41.2526 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3008...  Training loss: 1.2316...  40.6818 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3009...  Training loss: 1.2318...  40.0411 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3010...  Training loss: 1.2639...  39.6750 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3011...  Training loss: 1.2449...  40.3682 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3012...  Training loss: 1.3170...  39.7983 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3013...  Training loss: 1.2571...  40.1128 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3014...  Training loss: 1.2431...  42.7401 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3015...  Training loss: 1.2763...  40.5342 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3016...  Training loss: 1.2419...  40.2194 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3017...  Training loss: 1.2651...  40.0014 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3018...  Training loss: 1.2657...  40.4443 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3019...  Training loss: 1.2565...  39.5805 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3020...  Training loss: 1.2974...  40.6766 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3021...  Training loss: 1.2368...  40.3661 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3022...  Training loss: 1.3098...  42.4973 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3023...  Training loss: 1.2692...  41.0707 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3024...  Training loss: 1.2794...  40.2705 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3025...  Training loss: 1.2690...  41.0369 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3026...  Training loss: 1.2669...  40.6366 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3027...  Training loss: 1.2896...  40.0137 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3028...  Training loss: 1.2626...  39.9386 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3029...  Training loss: 1.2513...  41.2969 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3030...  Training loss: 1.3119...  43.1866 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3031...  Training loss: 1.2834...  40.9547 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3032...  Training loss: 1.3074...  39.7646 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3033...  Training loss: 1.2900...  40.8307 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3034...  Training loss: 1.2872...  40.6625 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3035...  Training loss: 1.2773...  40.0494 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3036...  Training loss: 1.2826...  41.4607 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3037...  Training loss: 1.2982...  42.0533 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3038...  Training loss: 1.2643...  43.5195 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3039...  Training loss: 1.2851...  39.8831 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3040...  Training loss: 1.2686...  40.8658 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3041...  Training loss: 1.3239...  40.6444 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3042...  Training loss: 1.2949...  39.7747 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3043...  Training loss: 1.3083...  40.5435 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3044...  Training loss: 1.2574...  41.3941 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3045...  Training loss: 1.2750...  40.7771 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3046...  Training loss: 1.2890...  43.7633 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3047...  Training loss: 1.2777...  40.2536 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3048...  Training loss: 1.2583...  40.4670 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3049...  Training loss: 1.2321...  40.6158 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3050...  Training loss: 1.2701...  40.4827 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3051...  Training loss: 1.2314...  40.9392 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3052...  Training loss: 1.2654...  57.1786 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3053...  Training loss: 1.2376...  59.8477 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3054...  Training loss: 1.2736...  54.9223 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3055...  Training loss: 1.2503...  42.0023 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3056...  Training loss: 1.2709...  40.0329 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3057...  Training loss: 1.2526...  42.3251 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3058...  Training loss: 1.2521...  40.5707 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3059...  Training loss: 1.2329...  40.1151 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3060...  Training loss: 1.2707...  41.8690 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3061...  Training loss: 1.2474...  43.6188 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3062...  Training loss: 1.2570...  40.1575 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20...  Training Step: 3063...  Training loss: 1.2409...  40.3123 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3064...  Training loss: 1.2465...  40.9232 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3065...  Training loss: 1.2493...  40.1896 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3066...  Training loss: 1.2844...  40.8436 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3067...  Training loss: 1.2734...  41.1155 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3068...  Training loss: 1.2400...  40.7875 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3069...  Training loss: 1.2438...  42.9133 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3070...  Training loss: 1.2379...  39.8047 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3071...  Training loss: 1.2699...  40.3475 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3072...  Training loss: 1.2558...  40.5289 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3073...  Training loss: 1.2684...  40.0626 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3074...  Training loss: 1.2559...  40.2897 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3075...  Training loss: 1.2582...  40.9229 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3076...  Training loss: 1.2628...  40.2223 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3077...  Training loss: 1.2779...  42.2012 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3078...  Training loss: 1.2715...  41.5436 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3079...  Training loss: 1.2602...  40.3270 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3080...  Training loss: 1.2827...  40.5150 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3081...  Training loss: 1.2501...  41.5951 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3082...  Training loss: 1.2729...  39.5553 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3083...  Training loss: 1.2697...  39.5344 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3084...  Training loss: 1.2561...  41.6646 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3085...  Training loss: 1.2373...  42.4398 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3086...  Training loss: 1.2255...  40.7433 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3087...  Training loss: 1.2696...  41.0364 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3088...  Training loss: 1.2766...  39.8682 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3089...  Training loss: 1.2555...  41.0647 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3090...  Training loss: 1.2604...  40.3161 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3091...  Training loss: 1.2576...  39.8403 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3092...  Training loss: 1.2311...  40.2637 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3093...  Training loss: 1.2122...  42.3438 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3094...  Training loss: 1.2574...  41.2585 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3095...  Training loss: 1.2490...  41.2593 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3096...  Training loss: 1.2105...  40.5590 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3097...  Training loss: 1.2667...  40.0358 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3098...  Training loss: 1.2713...  40.3373 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3099...  Training loss: 1.2525...  40.5894 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3100...  Training loss: 1.2245...  39.6843 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3101...  Training loss: 1.2210...  40.0467 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3102...  Training loss: 1.2469...  42.8440 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3103...  Training loss: 1.2853...  39.6527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3104...  Training loss: 1.2676...  40.5406 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3105...  Training loss: 1.2664...  41.7277 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3106...  Training loss: 1.2604...  40.2882 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3107...  Training loss: 1.2987...  41.4810 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3108...  Training loss: 1.2753...  40.3692 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3109...  Training loss: 1.2804...  39.5648 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3110...  Training loss: 1.2725...  44.2451 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3111...  Training loss: 1.3100...  40.3801 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3112...  Training loss: 1.2819...  40.7272 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3113...  Training loss: 1.2571...  40.2833 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3114...  Training loss: 1.2988...  41.4532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3115...  Training loss: 1.2492...  39.6944 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3116...  Training loss: 1.2876...  40.7849 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3117...  Training loss: 1.2730...  39.9941 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3118...  Training loss: 1.3062...  42.4664 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3119...  Training loss: 1.2952...  41.3600 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3120...  Training loss: 1.2522...  40.3551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3121...  Training loss: 1.2410...  39.8640 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3122...  Training loss: 1.2405...  41.0326 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3123...  Training loss: 1.2819...  39.4349 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3124...  Training loss: 1.2592...  40.9108 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3125...  Training loss: 1.2541...  41.5100 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3126...  Training loss: 1.2581...  42.1462 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3127...  Training loss: 1.2820...  40.3618 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3128...  Training loss: 1.2569...  42.0327 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3129...  Training loss: 1.2309...  40.1034 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3130...  Training loss: 1.2875...  40.1029 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3131...  Training loss: 1.2868...  42.1680 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3132...  Training loss: 1.2642...  39.9143 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3133...  Training loss: 1.2605...  39.5680 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3134...  Training loss: 1.2664...  44.2577 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3135...  Training loss: 1.2648...  40.1492 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3136...  Training loss: 1.2579...  39.4379 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3137...  Training loss: 1.2918...  42.0399 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3138...  Training loss: 1.3244...  41.5294 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3139...  Training loss: 1.2718...  39.3845 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3140...  Training loss: 1.2702...  41.0669 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3141...  Training loss: 1.2541...  40.2208 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3142...  Training loss: 1.2545...  42.1917 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3143...  Training loss: 1.3030...  42.1751 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3144...  Training loss: 1.2696...  39.4632 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3145...  Training loss: 1.2769...  40.0843 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3146...  Training loss: 1.2369...  42.0230 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3147...  Training loss: 1.2527...  39.8462 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3148...  Training loss: 1.2904...  39.4489 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3149...  Training loss: 1.2478...  41.4827 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3150...  Training loss: 1.2358...  42.5100 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3151...  Training loss: 1.2471...  39.5711 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3152...  Training loss: 1.2567...  41.4934 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3153...  Training loss: 1.2695...  39.8305 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3154...  Training loss: 1.2542...  40.7743 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3155...  Training loss: 1.2601...  41.6535 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3156...  Training loss: 1.2427...  39.5698 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3157...  Training loss: 1.2844...  39.6026 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3158...  Training loss: 1.2582...  44.0585 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3159...  Training loss: 1.2544...  40.1683 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20...  Training Step: 3160...  Training loss: 1.2587...  40.1372 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3161...  Training loss: 1.2403...  40.7255 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3162...  Training loss: 1.2509...  40.2668 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3163...  Training loss: 1.2673...  40.4210 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3164...  Training loss: 1.2458...  41.7561 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3165...  Training loss: 1.2260...  39.9195 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3166...  Training loss: 1.2632...  42.1902 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3167...  Training loss: 1.2538...  41.3814 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3168...  Training loss: 1.2550...  40.2978 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3169...  Training loss: 1.3731...  41.1701 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3170...  Training loss: 1.2700...  40.7485 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3171...  Training loss: 1.2651...  40.2333 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3172...  Training loss: 1.2743...  41.4454 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3173...  Training loss: 1.2450...  40.7630 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3174...  Training loss: 1.2303...  42.4591 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3175...  Training loss: 1.2684...  41.0206 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3176...  Training loss: 1.2630...  40.3168 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3177...  Training loss: 1.2718...  40.2747 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3178...  Training loss: 1.2551...  40.2867 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3179...  Training loss: 1.2547...  40.0549 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3180...  Training loss: 1.2653...  40.1815 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3181...  Training loss: 1.2657...  42.7493 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3182...  Training loss: 1.2777...  42.1703 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3183...  Training loss: 1.2451...  40.1126 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3184...  Training loss: 1.2481...  40.7941 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3185...  Training loss: 1.2797...  40.0446 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3186...  Training loss: 1.2829...  39.7624 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3187...  Training loss: 1.2599...  40.6143 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3188...  Training loss: 1.2843...  40.7101 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3189...  Training loss: 1.2601...  40.0241 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3190...  Training loss: 1.2768...  43.2436 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3191...  Training loss: 1.2539...  41.8545 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3192...  Training loss: 1.2743...  39.5719 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3193...  Training loss: 1.2725...  40.7001 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3194...  Training loss: 1.2233...  40.9664 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3195...  Training loss: 1.2353...  39.4570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3196...  Training loss: 1.2786...  40.2303 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3197...  Training loss: 1.2741...  40.9768 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3198...  Training loss: 1.2840...  41.9964 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3199...  Training loss: 1.2468...  43.9845 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3200...  Training loss: 1.2336...  40.3989 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3201...  Training loss: 1.2774...  40.0865 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3202...  Training loss: 1.2584...  41.3079 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3203...  Training loss: 1.2475...  40.9445 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3204...  Training loss: 1.2542...  39.2812 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3205...  Training loss: 1.2445...  41.1137 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3206...  Training loss: 1.2222...  40.6654 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3207...  Training loss: 1.2219...  42.1940 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3208...  Training loss: 1.2455...  40.6793 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3209...  Training loss: 1.2359...  40.3293 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3210...  Training loss: 1.2942...  40.3742 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3211...  Training loss: 1.2437...  41.0982 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3212...  Training loss: 1.2231...  40.5568 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3213...  Training loss: 1.2685...  41.5759 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3214...  Training loss: 1.2350...  41.8598 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3215...  Training loss: 1.2457...  41.9669 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3216...  Training loss: 1.2547...  40.0352 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3217...  Training loss: 1.2633...  40.7258 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3218...  Training loss: 1.2759...  40.1491 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3219...  Training loss: 1.2341...  40.6708 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3220...  Training loss: 1.3088...  40.5281 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3221...  Training loss: 1.2563...  40.1300 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3222...  Training loss: 1.2692...  39.9279 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3223...  Training loss: 1.2548...  43.7014 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3224...  Training loss: 1.2622...  41.0849 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3225...  Training loss: 1.2768...  41.1879 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3226...  Training loss: 1.2408...  40.6340 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3227...  Training loss: 1.2469...  41.2076 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3228...  Training loss: 1.2975...  41.1034 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3229...  Training loss: 1.2720...  40.9958 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3230...  Training loss: 1.3047...  39.4774 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3231...  Training loss: 1.2831...  16.0762 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3232...  Training loss: 1.2639...  16.6143 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3233...  Training loss: 1.2609...  9.0072 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3234...  Training loss: 1.2770...  7.4079 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3235...  Training loss: 1.2689...  7.0812 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3236...  Training loss: 1.2471...  6.2591 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3237...  Training loss: 1.2620...  6.7768 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3238...  Training loss: 1.2459...  6.2869 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3239...  Training loss: 1.3077...  6.4428 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3240...  Training loss: 1.2821...  6.7700 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3241...  Training loss: 1.2994...  7.6370 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3242...  Training loss: 1.2392...  9.9180 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3243...  Training loss: 1.2696...  7.4483 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3244...  Training loss: 1.2774...  7.1374 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3245...  Training loss: 1.2605...  6.4285 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3246...  Training loss: 1.2427...  6.5990 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3247...  Training loss: 1.2149...  6.4013 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3248...  Training loss: 1.2609...  6.4177 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3249...  Training loss: 1.2324...  6.2577 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3250...  Training loss: 1.2623...  6.3630 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3251...  Training loss: 1.2278...  6.7184 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3252...  Training loss: 1.2544...  6.1935 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3253...  Training loss: 1.2419...  743.9494 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3254...  Training loss: 1.2540...  112.0321 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3255...  Training loss: 1.2334...  6.5074 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3256...  Training loss: 1.2339...  9.0806 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20...  Training Step: 3257...  Training loss: 1.2237...  9.7024 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3258...  Training loss: 1.2588...  36.5451 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3259...  Training loss: 1.2338...  7.1616 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3260...  Training loss: 1.2472...  8.9008 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3261...  Training loss: 1.2381...  10.1148 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3262...  Training loss: 1.2302...  27.6461 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3263...  Training loss: 1.2375...  7.4371 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3264...  Training loss: 1.2642...  9.9415 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3265...  Training loss: 1.2641...  10.0567 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3266...  Training loss: 1.2238...  47.5462 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3267...  Training loss: 1.2376...  7.9733 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3268...  Training loss: 1.2294...  11.7219 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3269...  Training loss: 1.2597...  222.8096 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3270...  Training loss: 1.2437...  7.0089 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3271...  Training loss: 1.2478...  8.4287 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3272...  Training loss: 1.2506...  10.2636 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3273...  Training loss: 1.2432...  504.5449 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3274...  Training loss: 1.2464...  6.9753 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3275...  Training loss: 1.2601...  9.7232 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3276...  Training loss: 1.2579...  9.7109 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3277...  Training loss: 1.2417...  56.2568 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3278...  Training loss: 1.2661...  7.8967 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3279...  Training loss: 1.2363...  9.9141 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3280...  Training loss: 1.2490...  13.6874 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3281...  Training loss: 1.2555...  8.1128 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3282...  Training loss: 1.2418...  7.1166 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3283...  Training loss: 1.2259...  8.9196 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3284...  Training loss: 1.2175...  7.7291 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3285...  Training loss: 1.2700...  7.8673 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3286...  Training loss: 1.2498...  7.8504 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3287...  Training loss: 1.2512...  6.6502 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3288...  Training loss: 1.2462...  7.1685 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3289...  Training loss: 1.2500...  8.0819 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3290...  Training loss: 1.2276...  7.9218 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3291...  Training loss: 1.2054...  8.0711 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3292...  Training loss: 1.2468...  8.6192 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3293...  Training loss: 1.2328...  7.4287 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3294...  Training loss: 1.2061...  7.7253 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3295...  Training loss: 1.2529...  7.4279 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3296...  Training loss: 1.2494...  7.5513 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3297...  Training loss: 1.2316...  7.7085 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3298...  Training loss: 1.2087...  7.7412 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3299...  Training loss: 1.2025...  7.5059 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3300...  Training loss: 1.2385...  7.1762 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3301...  Training loss: 1.2677...  6.8142 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3302...  Training loss: 1.2544...  7.1525 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3303...  Training loss: 1.2636...  7.1330 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3304...  Training loss: 1.2578...  8.2015 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3305...  Training loss: 1.2884...  7.3462 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3306...  Training loss: 1.2681...  6.5667 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3307...  Training loss: 1.2530...  6.7094 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3308...  Training loss: 1.2644...  8.4176 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3309...  Training loss: 1.3119...  7.4914 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3310...  Training loss: 1.2727...  6.6547 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3311...  Training loss: 1.2403...  7.3490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3312...  Training loss: 1.2829...  7.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3313...  Training loss: 1.2390...  7.8457 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3314...  Training loss: 1.2695...  6.2098 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3315...  Training loss: 1.2641...  6.1841 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3316...  Training loss: 1.2909...  6.2619 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3317...  Training loss: 1.2797...  6.2996 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3318...  Training loss: 1.2471...  7.8444 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3319...  Training loss: 1.2232...  8.5284 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3320...  Training loss: 1.2307...  7.5653 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3321...  Training loss: 1.2629...  10.6587 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3322...  Training loss: 1.2457...  8.3339 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3323...  Training loss: 1.2378...  7.8957 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3324...  Training loss: 1.2520...  10.7230 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3325...  Training loss: 1.2545...  7.9915 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3326...  Training loss: 1.2421...  7.0475 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3327...  Training loss: 1.2216...  8.1904 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3328...  Training loss: 1.2687...  8.0115 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3329...  Training loss: 1.2778...  6.7299 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3330...  Training loss: 1.2549...  6.9781 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3331...  Training loss: 1.2496...  8.2522 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3332...  Training loss: 1.2518...  7.2507 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3333...  Training loss: 1.2535...  7.1893 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3334...  Training loss: 1.2548...  7.1062 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3335...  Training loss: 1.2692...  7.3997 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3336...  Training loss: 1.3123...  6.8940 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3337...  Training loss: 1.2719...  7.4339 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3338...  Training loss: 1.2529...  7.1005 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3339...  Training loss: 1.2474...  6.8284 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3340...  Training loss: 1.2445...  6.7038 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3341...  Training loss: 1.2945...  7.9079 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3342...  Training loss: 1.2634...  7.4714 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3343...  Training loss: 1.2660...  7.0916 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3344...  Training loss: 1.2275...  7.2832 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3345...  Training loss: 1.2558...  7.1924 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3346...  Training loss: 1.2915...  6.9954 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3347...  Training loss: 1.2441...  6.9006 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3348...  Training loss: 1.2291...  7.0847 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3349...  Training loss: 1.2321...  7.3533 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3350...  Training loss: 1.2495...  7.7170 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3351...  Training loss: 1.2473...  8.5863 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3352...  Training loss: 1.2465...  9.8164 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3353...  Training loss: 1.2434...  9.4708 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3354...  Training loss: 1.2410...  15.6962 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20...  Training Step: 3355...  Training loss: 1.2727...  12.1656 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3356...  Training loss: 1.2429...  12.4114 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3357...  Training loss: 1.2491...  12.1941 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3358...  Training loss: 1.2487...  12.0414 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3359...  Training loss: 1.2224...  16.2235 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3360...  Training loss: 1.2407...  12.0725 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3361...  Training loss: 1.2608...  12.1585 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3362...  Training loss: 1.2463...  12.1350 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3363...  Training loss: 1.2060...  12.0311 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3364...  Training loss: 1.2503...  12.1049 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3365...  Training loss: 1.2357...  11.9610 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3366...  Training loss: 1.2367...  12.0533 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3367...  Training loss: 1.3612...  12.2098 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3368...  Training loss: 1.2579...  12.0711 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3369...  Training loss: 1.2489...  12.1630 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3370...  Training loss: 1.2733...  12.2421 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3371...  Training loss: 1.2214...  12.0809 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3372...  Training loss: 1.2105...  12.1166 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3373...  Training loss: 1.2525...  11.8071 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3374...  Training loss: 1.2504...  12.0652 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3375...  Training loss: 1.2616...  11.8468 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3376...  Training loss: 1.2365...  6.9126 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3377...  Training loss: 1.2375...  7.3609 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3378...  Training loss: 1.2475...  9.5462 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3379...  Training loss: 1.2453...  7.1639 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3380...  Training loss: 1.2671...  7.1888 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3381...  Training loss: 1.2349...  10.4913 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3382...  Training loss: 1.2204...  11.7273 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3383...  Training loss: 1.2693...  12.1206 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3384...  Training loss: 1.2721...  12.1278 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3385...  Training loss: 1.2550...  12.0146 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3386...  Training loss: 1.2727...  12.1974 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3387...  Training loss: 1.2544...  12.0954 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3388...  Training loss: 1.2632...  12.4415 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3389...  Training loss: 1.2490...  11.9640 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3390...  Training loss: 1.2704...  12.1711 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3391...  Training loss: 1.2552...  12.0407 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3392...  Training loss: 1.2168...  12.0733 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3393...  Training loss: 1.2123...  11.9487 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3394...  Training loss: 1.2682...  12.1496 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3395...  Training loss: 1.2647...  12.1487 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3396...  Training loss: 1.2671...  12.1118 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3397...  Training loss: 1.2305...  12.0731 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3398...  Training loss: 1.2217...  12.0675 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3399...  Training loss: 1.2582...  12.0319 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3400...  Training loss: 1.2471...  12.0925 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3401...  Training loss: 1.2354...  12.0852 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3402...  Training loss: 1.2484...  12.1725 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3403...  Training loss: 1.2219...  12.0434 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3404...  Training loss: 1.2029...  12.0432 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3405...  Training loss: 1.2011...  11.9318 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3406...  Training loss: 1.2365...  12.1158 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3407...  Training loss: 1.2265...  12.2532 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3408...  Training loss: 1.2865...  12.9858 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3409...  Training loss: 1.2399...  11.9793 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3410...  Training loss: 1.2258...  12.2606 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3411...  Training loss: 1.2585...  12.1780 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3412...  Training loss: 1.2242...  11.9224 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3413...  Training loss: 1.2319...  12.1624 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3414...  Training loss: 1.2417...  12.2430 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3415...  Training loss: 1.2430...  12.4004 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3416...  Training loss: 1.2606...  12.0699 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3417...  Training loss: 1.2131...  12.0545 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3418...  Training loss: 1.2823...  12.1083 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3419...  Training loss: 1.2460...  11.9750 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3420...  Training loss: 1.2587...  12.1523 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3421...  Training loss: 1.2475...  12.1229 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3422...  Training loss: 1.2440...  11.9612 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3423...  Training loss: 1.2602...  13.9271 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3424...  Training loss: 1.2301...  14.9743 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3425...  Training loss: 1.2272...  15.1449 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3426...  Training loss: 1.2946...  12.4731 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3427...  Training loss: 1.2529...  12.6289 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3428...  Training loss: 1.2935...  12.1318 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3429...  Training loss: 1.2733...  12.8012 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3430...  Training loss: 1.2575...  12.2186 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3431...  Training loss: 1.2519...  12.1091 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3432...  Training loss: 1.2520...  12.0834 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3433...  Training loss: 1.2692...  12.1207 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3434...  Training loss: 1.2337...  12.4553 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3435...  Training loss: 1.2492...  14.8518 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3436...  Training loss: 1.2390...  15.6034 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3437...  Training loss: 1.2931...  12.9097 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3438...  Training loss: 1.2695...  12.5815 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3439...  Training loss: 1.2797...  12.6564 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3440...  Training loss: 1.2221...  12.8348 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3441...  Training loss: 1.2432...  12.6218 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3442...  Training loss: 1.2554...  12.5947 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3443...  Training loss: 1.2507...  11.9477 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3444...  Training loss: 1.2271...  12.0570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3445...  Training loss: 1.2027...  11.9910 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3446...  Training loss: 1.2559...  12.1554 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3447...  Training loss: 1.2116...  12.1948 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3448...  Training loss: 1.2529...  12.0082 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3449...  Training loss: 1.2171...  12.0674 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3450...  Training loss: 1.2409...  11.9938 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3451...  Training loss: 1.2205...  12.0157 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20...  Training Step: 3452...  Training loss: 1.2418...  13.3403 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3453...  Training loss: 1.2258...  13.5892 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3454...  Training loss: 1.2203...  15.0724 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3455...  Training loss: 1.2179...  15.6718 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3456...  Training loss: 1.2471...  14.3535 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3457...  Training loss: 1.2142...  13.9965 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3458...  Training loss: 1.2357...  16.7808 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3459...  Training loss: 1.2223...  16.7524 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3460...  Training loss: 1.2159...  14.4169 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3461...  Training loss: 1.2242...  20.3330 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3462...  Training loss: 1.2473...  18.5765 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3463...  Training loss: 1.2557...  15.5119 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3464...  Training loss: 1.2209...  17.3689 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3465...  Training loss: 1.2255...  15.2676 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3466...  Training loss: 1.2153...  14.6984 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3467...  Training loss: 1.2499...  17.4171 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3468...  Training loss: 1.2300...  18.9140 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3469...  Training loss: 1.2414...  15.7102 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3470...  Training loss: 1.2264...  17.1415 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3471...  Training loss: 1.2329...  16.3666 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3472...  Training loss: 1.2389...  14.1776 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3473...  Training loss: 1.2556...  16.6043 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3474...  Training loss: 1.2398...  15.2784 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3475...  Training loss: 1.2370...  14.8006 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3476...  Training loss: 1.2573...  15.3354 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3477...  Training loss: 1.2281...  14.9970 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3478...  Training loss: 1.2425...  14.1689 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3479...  Training loss: 1.2494...  13.4748 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3480...  Training loss: 1.2282...  13.7453 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3481...  Training loss: 1.2241...  12.7018 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3482...  Training loss: 1.2098...  14.1482 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3483...  Training loss: 1.2475...  15.4859 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3484...  Training loss: 1.2442...  12.6303 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3485...  Training loss: 1.2395...  13.9584 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3486...  Training loss: 1.2330...  14.6172 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3487...  Training loss: 1.2372...  13.4815 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3488...  Training loss: 1.2093...  7.8786 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3489...  Training loss: 1.1957...  16.0170 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3490...  Training loss: 1.2408...  12.9453 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3491...  Training loss: 1.2281...  12.4492 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3492...  Training loss: 1.1959...  12.5424 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3493...  Training loss: 1.2474...  12.6531 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3494...  Training loss: 1.2420...  12.5225 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3495...  Training loss: 1.2283...  12.4005 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3496...  Training loss: 1.1984...  12.5235 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3497...  Training loss: 1.2023...  12.6335 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3498...  Training loss: 1.2275...  12.7163 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3499...  Training loss: 1.2475...  12.6279 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3500...  Training loss: 1.2438...  12.5864 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3501...  Training loss: 1.2516...  12.5906 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3502...  Training loss: 1.2328...  12.4056 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3503...  Training loss: 1.2661...  12.6131 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3504...  Training loss: 1.2559...  12.8676 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3505...  Training loss: 1.2486...  13.2275 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3506...  Training loss: 1.2462...  12.6330 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3507...  Training loss: 1.2842...  14.2030 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3508...  Training loss: 1.2581...  12.8160 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3509...  Training loss: 1.2396...  12.9592 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3510...  Training loss: 1.2723...  12.5370 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3511...  Training loss: 1.2314...  13.7086 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3512...  Training loss: 1.2708...  13.8287 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3513...  Training loss: 1.2604...  12.6219 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3514...  Training loss: 1.2713...  12.6030 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3515...  Training loss: 1.2692...  12.4290 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3516...  Training loss: 1.2268...  13.0443 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3517...  Training loss: 1.2041...  13.0402 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3518...  Training loss: 1.2253...  12.5120 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3519...  Training loss: 1.2559...  12.4317 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3520...  Training loss: 1.2325...  12.3257 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3521...  Training loss: 1.2267...  12.1237 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3522...  Training loss: 1.2349...  12.6035 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3523...  Training loss: 1.2362...  12.0117 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3524...  Training loss: 1.2295...  12.1366 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3525...  Training loss: 1.2077...  11.9782 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3526...  Training loss: 1.2566...  11.9480 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3527...  Training loss: 1.2524...  12.1055 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3528...  Training loss: 1.2566...  12.0541 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3529...  Training loss: 1.2403...  11.9798 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3530...  Training loss: 1.2407...  14.5886 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3531...  Training loss: 1.2354...  14.3441 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3532...  Training loss: 1.2424...  12.7051 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3533...  Training loss: 1.2597...  13.5702 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3534...  Training loss: 1.3031...  15.2812 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3535...  Training loss: 1.2432...  15.5575 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3536...  Training loss: 1.2477...  12.8357 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3537...  Training loss: 1.2493...  12.1642 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3538...  Training loss: 1.2276...  12.1953 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3539...  Training loss: 1.2799...  11.9633 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3540...  Training loss: 1.2533...  11.9804 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3541...  Training loss: 1.2555...  12.0240 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3542...  Training loss: 1.2146...  12.1058 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3543...  Training loss: 1.2371...  12.1882 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3544...  Training loss: 1.2775...  14.4224 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3545...  Training loss: 1.2217...  12.7649 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3546...  Training loss: 1.2127...  12.0712 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3547...  Training loss: 1.2255...  12.0868 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3548...  Training loss: 1.2283...  12.1226 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20...  Training Step: 3549...  Training loss: 1.2368...  12.0161 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3550...  Training loss: 1.2303...  12.6360 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3551...  Training loss: 1.2344...  12.7596 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3552...  Training loss: 1.2202...  12.0228 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3553...  Training loss: 1.2725...  12.0717 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3554...  Training loss: 1.2310...  12.2832 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3555...  Training loss: 1.2397...  12.0673 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3556...  Training loss: 1.2429...  12.0844 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3557...  Training loss: 1.2182...  12.8491 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3558...  Training loss: 1.2212...  12.2941 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3559...  Training loss: 1.2476...  12.0556 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3560...  Training loss: 1.2268...  12.3543 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3561...  Training loss: 1.2027...  11.9676 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3562...  Training loss: 1.2453...  11.9893 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3563...  Training loss: 1.2341...  12.1214 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3564...  Training loss: 1.2232...  12.1492 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3565...  Training loss: 1.3601...  12.0609 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3566...  Training loss: 1.2481...  12.0231 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3567...  Training loss: 1.2406...  12.0564 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3568...  Training loss: 1.2609...  11.9478 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3569...  Training loss: 1.2210...  11.9117 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3570...  Training loss: 1.1984...  11.8879 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3571...  Training loss: 1.2477...  12.9380 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3572...  Training loss: 1.2468...  13.7004 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3573...  Training loss: 1.2517...  11.9126 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3574...  Training loss: 1.2354...  12.1177 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3575...  Training loss: 1.2238...  13.3105 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3576...  Training loss: 1.2444...  12.9621 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3577...  Training loss: 1.2385...  12.0308 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3578...  Training loss: 1.2513...  12.0489 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3579...  Training loss: 1.2253...  12.0866 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3580...  Training loss: 1.2175...  11.9492 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3581...  Training loss: 1.2544...  12.0885 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3582...  Training loss: 1.2557...  11.8356 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3583...  Training loss: 1.2373...  12.1014 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3584...  Training loss: 1.2701...  12.8383 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3585...  Training loss: 1.2375...  12.2526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3586...  Training loss: 1.2685...  11.9791 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3587...  Training loss: 1.2323...  12.1795 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3588...  Training loss: 1.2630...  11.9399 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3589...  Training loss: 1.2395...  11.8821 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3590...  Training loss: 1.2007...  12.1114 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3591...  Training loss: 1.2106...  11.8913 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3592...  Training loss: 1.2431...  12.2042 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3593...  Training loss: 1.2474...  12.1146 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3594...  Training loss: 1.2695...  11.9924 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3595...  Training loss: 1.2199...  12.2411 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3596...  Training loss: 1.2133...  14.5723 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3597...  Training loss: 1.2346...  15.1396 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3598...  Training loss: 1.2469...  15.1666 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3599...  Training loss: 1.2184...  14.0890 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3600...  Training loss: 1.2369...  13.4829 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3601...  Training loss: 1.2236...  13.8927 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3602...  Training loss: 1.1925...  14.7868 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3603...  Training loss: 1.1963...  14.5559 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3604...  Training loss: 1.2217...  16.2373 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3605...  Training loss: 1.2163...  15.9528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3606...  Training loss: 1.2772...  15.5916 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3607...  Training loss: 1.2313...  14.0515 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3608...  Training loss: 1.2032...  15.1351 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3609...  Training loss: 1.2471...  15.4788 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3610...  Training loss: 1.2134...  15.2683 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3611...  Training loss: 1.2176...  14.0498 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3612...  Training loss: 1.2194...  15.7792 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3613...  Training loss: 1.2332...  14.5296 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3614...  Training loss: 1.2537...  14.6358 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3615...  Training loss: 1.2125...  12.9309 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3616...  Training loss: 1.2715...  12.1968 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3617...  Training loss: 1.2381...  11.9701 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3618...  Training loss: 1.2496...  12.2455 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3619...  Training loss: 1.2278...  11.8683 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3620...  Training loss: 1.2409...  12.2785 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3621...  Training loss: 1.2406...  12.5389 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3622...  Training loss: 1.2309...  12.0065 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3623...  Training loss: 1.2188...  8.6598 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3624...  Training loss: 1.2725...  15.3375 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3625...  Training loss: 1.2508...  13.5003 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3626...  Training loss: 1.2757...  11.9934 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3627...  Training loss: 1.2523...  11.8686 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3628...  Training loss: 1.2464...  12.2462 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3629...  Training loss: 1.2340...  12.1221 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3630...  Training loss: 1.2545...  11.8713 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3631...  Training loss: 1.2578...  12.2019 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3632...  Training loss: 1.2305...  12.0402 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3633...  Training loss: 1.2481...  11.9707 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3634...  Training loss: 1.2290...  12.7873 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3635...  Training loss: 1.2831...  12.5004 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3636...  Training loss: 1.2593...  12.6993 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3637...  Training loss: 1.2687...  12.6445 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3638...  Training loss: 1.2171...  12.0542 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3639...  Training loss: 1.2490...  11.9858 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3640...  Training loss: 1.2497...  12.4654 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3641...  Training loss: 1.2326...  12.1373 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3642...  Training loss: 1.2254...  12.0377 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3643...  Training loss: 1.1906...  12.5092 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3644...  Training loss: 1.2477...  11.8435 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3645...  Training loss: 1.2017...  11.8828 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20...  Training Step: 3646...  Training loss: 1.2401...  12.1131 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3647...  Training loss: 1.2073...  11.8869 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3648...  Training loss: 1.2186...  12.0003 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3649...  Training loss: 1.2120...  12.2223 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3650...  Training loss: 1.2282...  11.9688 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3651...  Training loss: 1.2099...  12.1398 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3652...  Training loss: 1.2211...  11.9932 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3653...  Training loss: 1.2037...  12.0801 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3654...  Training loss: 1.2396...  12.0177 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3655...  Training loss: 1.2146...  14.2306 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3656...  Training loss: 1.2146...  13.0429 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3657...  Training loss: 1.2187...  11.9037 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3658...  Training loss: 1.2031...  11.8319 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3659...  Training loss: 1.2175...  12.3618 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3660...  Training loss: 1.2424...  11.9681 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3661...  Training loss: 1.2376...  15.4481 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3662...  Training loss: 1.2059...  14.1726 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3663...  Training loss: 1.2122...  12.9516 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3664...  Training loss: 1.2154...  12.2767 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3665...  Training loss: 1.2332...  12.0937 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3666...  Training loss: 1.2329...  13.0476 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3667...  Training loss: 1.2386...  11.9854 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3668...  Training loss: 1.2198...  12.0918 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3669...  Training loss: 1.2345...  13.1190 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3670...  Training loss: 1.2267...  11.9157 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3671...  Training loss: 1.2384...  11.9637 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3672...  Training loss: 1.2330...  12.0867 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3673...  Training loss: 1.2207...  11.9280 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3674...  Training loss: 1.2504...  11.9844 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3675...  Training loss: 1.2194...  11.9902 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3676...  Training loss: 1.2370...  12.2009 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3677...  Training loss: 1.2393...  11.8549 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3678...  Training loss: 1.2156...  12.0598 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3679...  Training loss: 1.2094...  13.5821 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3680...  Training loss: 1.1946...  12.7298 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3681...  Training loss: 1.2413...  13.8015 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3682...  Training loss: 1.2416...  13.4275 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3683...  Training loss: 1.2280...  12.7434 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3684...  Training loss: 1.2247...  12.2170 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3685...  Training loss: 1.2284...  12.0117 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3686...  Training loss: 1.1959...  12.0289 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3687...  Training loss: 1.1942...  12.9146 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3688...  Training loss: 1.2243...  12.4067 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3689...  Training loss: 1.2244...  13.3712 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3690...  Training loss: 1.1849...  13.7055 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3691...  Training loss: 1.2302...  13.1197 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3692...  Training loss: 1.2261...  12.6499 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3693...  Training loss: 1.2222...  12.0406 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3694...  Training loss: 1.1811...  12.2575 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3695...  Training loss: 1.1851...  11.9156 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3696...  Training loss: 1.2136...  12.1037 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3697...  Training loss: 1.2563...  11.8405 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3698...  Training loss: 1.2392...  12.1848 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3699...  Training loss: 1.2393...  12.0715 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3700...  Training loss: 1.2227...  12.0678 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3701...  Training loss: 1.2646...  12.1072 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3702...  Training loss: 1.2504...  12.0036 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3703...  Training loss: 1.2409...  12.0932 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3704...  Training loss: 1.2413...  11.8703 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3705...  Training loss: 1.2815...  11.9139 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3706...  Training loss: 1.2419...  11.9748 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3707...  Training loss: 1.2321...  12.4296 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3708...  Training loss: 1.2611...  12.1136 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3709...  Training loss: 1.2220...  12.3879 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3710...  Training loss: 1.2542...  11.7982 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3711...  Training loss: 1.2386...  12.0306 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3712...  Training loss: 1.2691...  12.0217 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3713...  Training loss: 1.2577...  12.0204 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3714...  Training loss: 1.2244...  12.6423 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3715...  Training loss: 1.2096...  12.1357 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3716...  Training loss: 1.2023...  11.9781 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3717...  Training loss: 1.2466...  12.0973 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3718...  Training loss: 1.2225...  11.8786 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3719...  Training loss: 1.2154...  12.2180 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3720...  Training loss: 1.2248...  12.0280 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3721...  Training loss: 1.2360...  12.0769 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3722...  Training loss: 1.2173...  12.2633 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3723...  Training loss: 1.1905...  12.1998 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3724...  Training loss: 1.2469...  12.2208 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3725...  Training loss: 1.2521...  12.0676 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3726...  Training loss: 1.2310...  11.9861 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3727...  Training loss: 1.2329...  11.9812 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3728...  Training loss: 1.2333...  12.1977 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3729...  Training loss: 1.2341...  11.8572 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3730...  Training loss: 1.2261...  12.2158 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3731...  Training loss: 1.2601...  11.8751 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3732...  Training loss: 1.2847...  11.9861 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3733...  Training loss: 1.2487...  12.0900 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3734...  Training loss: 1.2328...  12.1978 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3735...  Training loss: 1.2252...  12.0110 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3736...  Training loss: 1.2190...  11.9627 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3737...  Training loss: 1.2531...  11.8565 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3738...  Training loss: 1.2287...  12.1499 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3739...  Training loss: 1.2401...  11.8960 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3740...  Training loss: 1.2104...  12.2265 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3741...  Training loss: 1.2241...  12.7368 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3742...  Training loss: 1.2607...  12.0755 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20...  Training Step: 3743...  Training loss: 1.2101...  11.8892 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3744...  Training loss: 1.2040...  12.0970 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3745...  Training loss: 1.2271...  11.9087 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3746...  Training loss: 1.2321...  12.0251 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3747...  Training loss: 1.2276...  11.9808 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3748...  Training loss: 1.2256...  11.9221 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3749...  Training loss: 1.2191...  12.1664 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3750...  Training loss: 1.2038...  12.0404 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3751...  Training loss: 1.2582...  11.8560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3752...  Training loss: 1.2239...  12.0373 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3753...  Training loss: 1.2189...  12.0167 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3754...  Training loss: 1.2279...  12.1858 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3755...  Training loss: 1.2052...  11.9004 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3756...  Training loss: 1.2135...  12.0687 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3757...  Training loss: 1.2360...  12.1559 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3758...  Training loss: 1.2072...  11.9248 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3759...  Training loss: 1.1955...  12.2877 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3760...  Training loss: 1.2290...  12.0038 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3761...  Training loss: 1.2194...  12.0962 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3762...  Training loss: 1.2199...  12.0083 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3763...  Training loss: 1.3365...  12.0938 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3764...  Training loss: 1.2482...  12.2337 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3765...  Training loss: 1.2203...  11.8316 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3766...  Training loss: 1.2460...  12.1483 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3767...  Training loss: 1.2174...  12.1677 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3768...  Training loss: 1.1896...  12.2039 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3769...  Training loss: 1.2322...  12.3442 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3770...  Training loss: 1.2181...  12.0194 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3771...  Training loss: 1.2337...  11.8093 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3772...  Training loss: 1.2224...  12.2025 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3773...  Training loss: 1.2123...  12.0502 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3774...  Training loss: 1.2256...  12.1496 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3775...  Training loss: 1.2337...  12.3345 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3776...  Training loss: 1.2369...  11.9234 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3777...  Training loss: 1.2157...  12.0883 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3778...  Training loss: 1.2048...  11.8774 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3779...  Training loss: 1.2508...  12.1070 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3780...  Training loss: 1.2439...  11.9553 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3781...  Training loss: 1.2235...  11.9255 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3782...  Training loss: 1.2625...  11.8638 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3783...  Training loss: 1.2313...  12.2050 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3784...  Training loss: 1.2484...  12.2629 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3785...  Training loss: 1.2299...  12.1146 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3786...  Training loss: 1.2498...  11.9929 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3787...  Training loss: 1.2353...  12.0152 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3788...  Training loss: 1.1902...  12.4592 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3789...  Training loss: 1.1969...  12.0713 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3790...  Training loss: 1.2437...  91.3971 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3791...  Training loss: 1.2355...  16.3613 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3792...  Training loss: 1.2513...  13.7789 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3793...  Training loss: 1.2112...  13.5544 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3794...  Training loss: 1.2003...  13.7342 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3795...  Training loss: 1.2399...  12.9819 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3796...  Training loss: 1.2332...  12.4432 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3797...  Training loss: 1.2182...  12.1543 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3798...  Training loss: 1.2306...  12.0103 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3799...  Training loss: 1.1995...  11.9495 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3800...  Training loss: 1.1857...  11.9853 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3801...  Training loss: 1.1813...  12.0662 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3802...  Training loss: 1.2165...  11.8353 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3803...  Training loss: 1.1948...  11.9912 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3804...  Training loss: 1.2661...  12.2604 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3805...  Training loss: 1.2232...  12.0309 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3806...  Training loss: 1.1975...  12.0229 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3807...  Training loss: 1.2331...  12.0549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3808...  Training loss: 1.2066...  11.8819 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3809...  Training loss: 1.2059...  11.9866 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3810...  Training loss: 1.2206...  12.0421 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3811...  Training loss: 1.2179...  12.0090 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3812...  Training loss: 1.2435...  11.8307 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3813...  Training loss: 1.1920...  11.9272 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3814...  Training loss: 1.2607...  12.0513 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3815...  Training loss: 1.2285...  12.1545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3816...  Training loss: 1.2372...  14.4454 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3817...  Training loss: 1.2094...  13.6347 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3818...  Training loss: 1.2140...  12.3267 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3819...  Training loss: 1.2267...  39.3723 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3820...  Training loss: 1.2141...  12.4733 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3821...  Training loss: 1.2142...  79.3260 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3822...  Training loss: 1.2703...  38.4106 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3823...  Training loss: 1.2460...  19.3245 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3824...  Training loss: 1.2735...  105.2403 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3825...  Training loss: 1.2536...  15.7534 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3826...  Training loss: 1.2326...  256.2622 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3827...  Training loss: 1.2184...  12.9080 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3828...  Training loss: 1.2412...  12.4606 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3829...  Training loss: 1.2433...  12.7782 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3830...  Training loss: 1.2142...  13.6571 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3831...  Training loss: 1.2325...  13.2303 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3832...  Training loss: 1.2171...  12.3038 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3833...  Training loss: 1.2758...  14.3882 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3834...  Training loss: 1.2432...  14.6367 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3835...  Training loss: 1.2548...  14.1354 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3836...  Training loss: 1.2099...  15.0234 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3837...  Training loss: 1.2303...  14.4633 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3838...  Training loss: 1.2280...  13.6073 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3839...  Training loss: 1.2299...  14.6052 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 3840...  Training loss: 1.2096...  13.9281 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3841...  Training loss: 1.1821...  13.5955 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3842...  Training loss: 1.2383...  13.4626 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3843...  Training loss: 1.1840...  13.5730 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3844...  Training loss: 1.2335...  14.1840 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3845...  Training loss: 1.1976...  14.0467 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3846...  Training loss: 1.2140...  14.3208 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3847...  Training loss: 1.1930...  13.8428 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3848...  Training loss: 1.2177...  13.6184 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3849...  Training loss: 1.1885...  14.6241 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3850...  Training loss: 1.2049...  14.1863 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3851...  Training loss: 1.1943...  14.9684 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3852...  Training loss: 1.2256...  16.6821 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3853...  Training loss: 1.2004...  13.5379 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3854...  Training loss: 1.2122...  15.0245 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3855...  Training loss: 1.1934...  14.4223 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3856...  Training loss: 1.1938...  14.8134 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3857...  Training loss: 1.2116...  15.5348 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3858...  Training loss: 1.2327...  14.3511 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3859...  Training loss: 1.2313...  14.5681 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3860...  Training loss: 1.2002...  12.7976 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3861...  Training loss: 1.2083...  14.8122 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3862...  Training loss: 1.1985...  15.8603 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3863...  Training loss: 1.2271...  12.4444 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3864...  Training loss: 1.2196...  15.5223 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3865...  Training loss: 1.2274...  15.3545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3866...  Training loss: 1.2131...  13.3831 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3867...  Training loss: 1.2097...  12.3055 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3868...  Training loss: 1.2134...  12.0493 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3869...  Training loss: 1.2278...  11.9349 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3870...  Training loss: 1.2277...  12.5163 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3871...  Training loss: 1.2161...  12.9956 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3872...  Training loss: 1.2446...  14.5231 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3873...  Training loss: 1.2097...  12.9598 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3874...  Training loss: 1.2258...  12.1262 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3875...  Training loss: 1.2200...  12.0557 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3876...  Training loss: 1.2102...  12.2860 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3877...  Training loss: 1.1998...  12.1027 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3878...  Training loss: 1.1836...  12.2849 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3879...  Training loss: 1.2268...  12.8757 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3880...  Training loss: 1.2299...  11.9671 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3881...  Training loss: 1.2182...  12.0023 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3882...  Training loss: 1.2226...  11.3483 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3883...  Training loss: 1.2220...  15.9905 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3884...  Training loss: 1.1842...  13.8552 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3885...  Training loss: 1.1771...  12.0957 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3886...  Training loss: 1.2202...  12.1790 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3887...  Training loss: 1.2172...  12.1305 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3888...  Training loss: 1.1730...  14.6284 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3889...  Training loss: 1.2231...  16.9033 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3890...  Training loss: 1.2217...  13.9015 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3891...  Training loss: 1.2005...  14.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3892...  Training loss: 1.1759...  13.5846 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3893...  Training loss: 1.1816...  14.0171 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3894...  Training loss: 1.2056...  15.6281 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3895...  Training loss: 1.2410...  16.7110 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3896...  Training loss: 1.2262...  16.3314 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3897...  Training loss: 1.2304...  16.3535 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3898...  Training loss: 1.2074...  14.3014 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3899...  Training loss: 1.2439...  14.9656 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3900...  Training loss: 1.2355...  14.3621 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3901...  Training loss: 1.2308...  15.0861 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3902...  Training loss: 1.2248...  14.7670 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3903...  Training loss: 1.2721...  13.6405 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3904...  Training loss: 1.2398...  14.6429 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3905...  Training loss: 1.2122...  15.6315 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3906...  Training loss: 1.2585...  15.6601 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3907...  Training loss: 1.2115...  15.5341 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3908...  Training loss: 1.2388...  15.4203 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3909...  Training loss: 1.2232...  16.4936 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3910...  Training loss: 1.2489...  15.3952 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3911...  Training loss: 1.2500...  14.8299 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3912...  Training loss: 1.2189...  14.3036 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3913...  Training loss: 1.1916...  12.4248 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3914...  Training loss: 1.2002...  12.4929 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3915...  Training loss: 1.2285...  12.7449 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3916...  Training loss: 1.2108...  13.0626 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3917...  Training loss: 1.2154...  12.5871 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3918...  Training loss: 1.2133...  13.6710 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3919...  Training loss: 1.2270...  17.2605 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3920...  Training loss: 1.2208...  13.0639 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3921...  Training loss: 1.1876...  14.2177 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3922...  Training loss: 1.2398...  13.6209 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3923...  Training loss: 1.2312...  13.5573 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3924...  Training loss: 1.2282...  13.3010 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3925...  Training loss: 1.2218...  14.4684 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3926...  Training loss: 1.2305...  14.0151 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3927...  Training loss: 1.2170...  15.5690 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3928...  Training loss: 1.2166...  14.3656 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3929...  Training loss: 1.2392...  14.3995 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3930...  Training loss: 1.2823...  13.7379 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3931...  Training loss: 1.2308...  13.2828 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3932...  Training loss: 1.2264...  13.7364 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3933...  Training loss: 1.2122...  12.1613 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3934...  Training loss: 1.2155...  13.7460 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3935...  Training loss: 1.2528...  13.1632 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3936...  Training loss: 1.2250...  13.2643 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 3937...  Training loss: 1.2333...  13.6795 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3938...  Training loss: 1.1919...  13.3156 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3939...  Training loss: 1.2157...  13.6288 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3940...  Training loss: 1.2571...  14.4118 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3941...  Training loss: 1.2004...  12.8330 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3942...  Training loss: 1.1982...  12.9520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3943...  Training loss: 1.2150...  13.0021 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3944...  Training loss: 1.2177...  14.5905 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3945...  Training loss: 1.2117...  15.5497 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3946...  Training loss: 1.2160...  14.1428 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3947...  Training loss: 1.2102...  14.5266 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3948...  Training loss: 1.2070...  14.6587 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3949...  Training loss: 1.2463...  15.1743 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3950...  Training loss: 1.2130...  15.8912 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3951...  Training loss: 1.2140...  15.7118 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3952...  Training loss: 1.2143...  13.7815 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3953...  Training loss: 1.2010...  14.2775 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3954...  Training loss: 1.2033...  14.4184 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3955...  Training loss: 1.2238...  14.1105 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3956...  Training loss: 1.2045...  14.3158 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3957...  Training loss: 1.1803...  14.9879 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3958...  Training loss: 1.2205...  15.9470 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3959...  Training loss: 1.2072...  14.3248 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3960...  Training loss: 1.2034...  14.3832 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i3960_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3960_l512.ckpt\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/i3960_l512.ckpt'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i3960_l512.ckpt\n",
      "Farred of them,\n",
      "she said that she could not have liked to say, having not baren\n",
      "and shameful to his short of the position. She was in the table\n",
      "and started, and hid because she her husband was not all so seeing, but\n",
      "he came back to the best forese and many sound of anticipalies, but she\n",
      "would so lively, that he did not see the same afternation. And with a\n",
      "creature to this way of his happiness in the moment.\n",
      "\n",
      "\"What are you someone for? The prince was a long while that's a minute to\n",
      "think is at the coupt of the same at all,\" she answered.\n",
      "\n",
      "\"All I am so song of your honest.\"\n",
      "\n",
      "And he should not care to see him when he had to go to her.\n",
      "\n",
      "\"Yes, I'm never been work and more attracting her of it,\" said Stepan\n",
      "Arkadyevitch, still half-past the princess. \"I shall be in a country\n",
      "done,\" he thought that the corrept and a shrewd of the conversation of\n",
      "the book in the doorway. \"In it and has stood it.\"\n",
      "\n",
      "\"Well, and would be a saying any sort. And they write to me and that\n",
      "I can't dinn,\" thought Anna, answering him, with a stepar for her to the\n",
      "plans to she had at a pretate with him, and his higher shooking starse of\n",
      "the self-possession of which he had a side that they were not always feel.\n",
      "Had the pastion of the conversation that was that her such in the\n",
      "particilors were talking of the bearth.\n",
      "\n",
      "\"What an order of the some men?\" he said, afmerially with a common and what\n",
      "she did not stay. \"That is no one force a minute that is it all, to\n",
      "be thought in his son in a dispecubour crest, when he has been, when you\n",
      "could not have been treated and had all about it of the counting of the\n",
      "clushed. But it was so thinking of your position; but that's all the\n",
      "cheaker.\n",
      "\n",
      "I shall gave him the peasants in the country.\"\n",
      "\n",
      "\"Well, when she's so monty or to meet me! And we all at all. I's so lately....\"\n",
      "\n",
      "\"You're been walking for yourself for a subject of the crest for your\n",
      "chance in the whole of it.\"\n",
      "\n",
      "And at the sight of the strained children saed all her face that it was\n",
      "a little arranged firmersing happy,\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i200_l512.ckpt\n",
      "Farg,.\n",
      "\n",
      "\"W\"hh sos\n",
      "sean thed the with winte wo he his ane ad th ant wate thed an theres thor th wimhe he wotes.\n",
      "\n",
      "\"Wh t ald tho shen tor he whos anes of hos he th siton ot the won the hersos one the she warigind sor ho te whed an oto sor he thim wasesid andind art ot tore ang an ot has\n",
      "eranded sou thit he hons ofes wond the sing the asd aned ther thas at as ar hire ha he han this toring ant an or hin the timense the\n",
      "sane tathe ante ad al oo wher whe had\n",
      "at at oo los tire an thim he whar thit\n",
      "he thet to the and and the he wes tas hes and he han ad ar te hissas, the he her on tete the and onging on tor that asd ang ond hin that to win athe whe wot ta the woris hir seasd se and tis he hers hans wang and oun and, he anten as on thin shared ar herans ond ong than an sont ond an hot tes he sher tha we and the hun sher, sane has ood tat an he an hit asd or wint tho ther wan sasd, wad the han than he ale watit\n",
      "\n",
      "ateten, win hat on the heranse ant he terer othere sotond wo she an oud th sas on hint an\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i200_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i600_l512.ckpt\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i600_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'checkpoints/i1200_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
