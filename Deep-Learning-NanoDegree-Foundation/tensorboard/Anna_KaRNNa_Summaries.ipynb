{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we'll load the text file and convert it into integers for our network to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\r\\n\\r\\n\\r\\nHappy families are all alike; every unhappy family is unhappy in its own\\r\\nway.\\r\\n\\r\\nEve'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31, 66, 57, 74, 78, 61, 76,  3, 16,  1,  0,  1,  0,  1,  0, 38, 57,\n",
       "       74, 74, 81,  3, 64, 57, 69, 65, 70, 65, 61, 75,  3, 57, 76, 61,  3,\n",
       "       57, 70, 70,  3, 57, 70, 65, 67, 61, 26,  3, 61, 80, 61, 76, 81,  3,\n",
       "       77, 72, 66, 57, 74, 74, 81,  3, 64, 57, 69, 65, 70, 81,  3, 65, 75,\n",
       "        3, 77, 72, 66, 57, 74, 74, 81,  3, 65, 72,  3, 65, 78, 75,  3, 71,\n",
       "       79, 72,  1,  0, 79, 57, 81, 15,  1,  0,  1,  0, 33, 80, 61], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the number of batches. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 182000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[31, 66, 57, 74, 78, 61, 76,  3, 16,  1],\n",
       "       [ 3, 74, 76, 61, 75, 75,  3, 66, 61, 76],\n",
       "       [72,  3, 78, 71,  3, 75, 74, 61, 57, 67],\n",
       "       [61, 69, 74, 71, 76, 57, 76, 81,  3, 61],\n",
       "       [ 3,  4, 50, 66, 61, 81,  7, 76, 61,  3],\n",
       "       [72, 72, 71, 78, 65, 59, 61, 62, 15,  3],\n",
       "       [65, 72, 63, 61, 76, 75,  3, 79, 65, 78],\n",
       "       [ 3, 69, 71, 69, 61, 72, 78,  3, 79, 66],\n",
       "       [65, 64, 78,  3, 71, 64,  3, 78, 66, 61],\n",
       "       [70,  3, 62, 57, 81, 13,  3, 57, 72, 62]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I'll write another function to grab batches out of the arrays made by split data. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "        x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "    \n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "        y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "        y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Build the RNN layers\n",
    "    with tf.name_scope(\"RNN_cells\"):\n",
    "        def build_cell(lstm_size, keep_prob):\n",
    "            lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "            drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "            return drop\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    \n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each cell output\n",
    "    with tf.name_scope('sequence_reshape'):\n",
    "        seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "        output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer and calculate the cost\n",
    "    with tf.name_scope('logits'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                               name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        tf.summary.histogram('softmax_w', softmax_w)\n",
    "        tf.summary.histogram('softmax_b', softmax_b)\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        preds = tf.nn.softmax(logits, name='predictions')\n",
    "        tf.summary.histogram('predictions', preds)\n",
    "    \n",
    "    with tf.name_scope('cost'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "        cost = tf.reduce_mean(loss, name='cost')\n",
    "        tf.summary.scalar('cost', cost)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    with tf.name_scope('train'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer', 'merged']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. The two you probably haven't seen before are `lstm_size` and `num_layers`. These set the number of hidden units in the LSTM layers and the number of LSTM layers, respectively. Of course, making these bigger will improve the network's performance but you'll have to watch out for overfitting. If your validation loss is much larger than the training loss, you're probably overfitting. Decrease the size of the network or decrease the dropout keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p checkpoints/anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch 1/10 ', 'Iteration 1/1810', 'Training loss: 4.4329', '7.4568 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 2/1810', 'Training loss: 4.3932', '6.9325 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 3/1810', 'Training loss: 4.2420', '7.1841 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 4/1810', 'Training loss: 4.6982', '7.1308 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 5/1810', 'Training loss: 4.6089', '6.9516 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 6/1810', 'Training loss: 4.5035', '7.0568 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 7/1810', 'Training loss: 4.4129', '7.6510 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 8/1810', 'Training loss: 4.3263', '8.3086 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 9/1810', 'Training loss: 4.2446', '6.7482 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 10/1810', 'Training loss: 4.1654', '6.4873 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 11/1810', 'Training loss: 4.0965', '6.4245 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 12/1810', 'Training loss: 4.0392', '6.2811 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 13/1810', 'Training loss: 3.9883', '6.4515 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 14/1810', 'Training loss: 3.9442', '7.1231 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 15/1810', 'Training loss: 3.9060', '7.2260 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 16/1810', 'Training loss: 3.8705', '8.9545 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 17/1810', 'Training loss: 3.8387', '7.3703 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 18/1810', 'Training loss: 3.8111', '9.2454 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 19/1810', 'Training loss: 3.7853', '8.8076 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 20/1810', 'Training loss: 3.7612', '8.1489 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 21/1810', 'Training loss: 3.7386', '8.3313 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 22/1810', 'Training loss: 3.7179', '8.3997 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 23/1810', 'Training loss: 3.6989', '7.4782 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 24/1810', 'Training loss: 3.6813', '7.9802 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 25/1810', 'Training loss: 3.6646', '7.1104 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 26/1810', 'Training loss: 3.6490', '6.9735 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 27/1810', 'Training loss: 3.6349', '7.1704 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 28/1810', 'Training loss: 3.6211', '7.1228 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 29/1810', 'Training loss: 3.6083', '6.9644 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 30/1810', 'Training loss: 3.5956', '7.8675 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 31/1810', 'Training loss: 3.5840', '9.6608 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 32/1810', 'Training loss: 3.5735', '8.7209 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 33/1810', 'Training loss: 3.5639', '7.9319 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 34/1810', 'Training loss: 3.5538', '7.3536 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 35/1810', 'Training loss: 3.5440', '7.6170 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 36/1810', 'Training loss: 3.5359', '7.6431 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 37/1810', 'Training loss: 3.5276', '7.7731 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 38/1810', 'Training loss: 3.5191', '8.1304 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 39/1810', 'Training loss: 3.5110', '6.5328 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 40/1810', 'Training loss: 3.5032', '7.5722 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 41/1810', 'Training loss: 3.4955', '7.6961 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 42/1810', 'Training loss: 3.4887', '8.6757 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 43/1810', 'Training loss: 3.4821', '8.4464 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 44/1810', 'Training loss: 3.4754', '8.1508 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 45/1810', 'Training loss: 3.4689', '7.8119 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 46/1810', 'Training loss: 3.4630', '7.5951 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 47/1810', 'Training loss: 3.4572', '7.7269 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 48/1810', 'Training loss: 3.4516', '7.5523 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 49/1810', 'Training loss: 3.4466', '7.7067 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 50/1810', 'Training loss: 3.4418', '7.5110 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 51/1810', 'Training loss: 3.4368', '8.1533 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 52/1810', 'Training loss: 3.4318', '8.1062 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 53/1810', 'Training loss: 3.4271', '7.1419 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 54/1810', 'Training loss: 3.4224', '8.5355 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 55/1810', 'Training loss: 3.4180', '7.1575 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 56/1810', 'Training loss: 3.4138', '7.0860 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 57/1810', 'Training loss: 3.4096', '6.8943 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 58/1810', 'Training loss: 3.4054', '7.1481 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 59/1810', 'Training loss: 3.4014', '8.0729 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 60/1810', 'Training loss: 3.3979', '7.1889 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 61/1810', 'Training loss: 3.3941', '7.1693 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 62/1810', 'Training loss: 3.3903', '7.0010 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 63/1810', 'Training loss: 3.3870', '6.9651 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 64/1810', 'Training loss: 3.3836', '6.9917 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 65/1810', 'Training loss: 3.3806', '6.9627 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 66/1810', 'Training loss: 3.3775', '7.0279 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 67/1810', 'Training loss: 3.3742', '7.0380 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 68/1810', 'Training loss: 3.3710', '7.0733 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 69/1810', 'Training loss: 3.3681', '6.9571 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 70/1810', 'Training loss: 3.3650', '9.1943 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 71/1810', 'Training loss: 3.3620', '7.8830 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 72/1810', 'Training loss: 3.3591', '7.8606 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 73/1810', 'Training loss: 3.3561', '8.0488 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 74/1810', 'Training loss: 3.3534', '8.2435 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 75/1810', 'Training loss: 3.3510', '8.2744 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 76/1810', 'Training loss: 3.3485', '8.9537 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 77/1810', 'Training loss: 3.3462', '8.2767 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 78/1810', 'Training loss: 3.3442', '8.3370 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 79/1810', 'Training loss: 3.3420', '8.1250 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 80/1810', 'Training loss: 3.3401', '7.3856 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 81/1810', 'Training loss: 3.3378', '7.3894 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 82/1810', 'Training loss: 3.3354', '7.3306 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 83/1810', 'Training loss: 3.3331', '7.3499 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 84/1810', 'Training loss: 3.3312', '7.6265 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 85/1810', 'Training loss: 3.3290', '7.6806 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 86/1810', 'Training loss: 3.3270', '6.7632 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 87/1810', 'Training loss: 3.3249', '6.7577 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 88/1810', 'Training loss: 3.3225', '7.0410 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 89/1810', 'Training loss: 3.3204', '6.8751 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 90/1810', 'Training loss: 3.3182', '8.4398 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 91/1810', 'Training loss: 3.3159', '8.0985 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 92/1810', 'Training loss: 3.3142', '8.2791 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 93/1810', 'Training loss: 3.3122', '8.3739 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 94/1810', 'Training loss: 3.3103', '9.6422 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 95/1810', 'Training loss: 3.3083', '7.2278 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 96/1810', 'Training loss: 3.3063', '7.1268 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 97/1810', 'Training loss: 3.3043', '7.5670 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 98/1810', 'Training loss: 3.3022', '8.1704 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 99/1810', 'Training loss: 3.3003', '6.7534 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 100/1810', 'Training loss: 3.2984', '7.5899 sec/batch')\n",
      "('Validation loss:', 3.275836, 'Saving checkpoint!')\n",
      "('Epoch 1/10 ', 'Iteration 101/1810', 'Training loss: 3.2987', '7.1338 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 102/1810', 'Training loss: 3.2985', '6.9675 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 103/1810', 'Training loss: 3.2979', '7.1036 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 104/1810', 'Training loss: 3.2969', '7.1853 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 105/1810', 'Training loss: 3.2950', '7.1033 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 106/1810', 'Training loss: 3.2932', '7.3594 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 107/1810', 'Training loss: 3.2911', '7.5124 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 108/1810', 'Training loss: 3.2893', '8.7256 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 109/1810', 'Training loss: 3.2872', '10.1852 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 110/1810', 'Training loss: 3.2851', '8.9180 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 111/1810', 'Training loss: 3.2830', '7.4577 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 112/1810', 'Training loss: 3.2809', '7.6851 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 113/1810', 'Training loss: 3.2789', '7.6969 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 114/1810', 'Training loss: 3.2767', '8.1923 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 115/1810', 'Training loss: 3.2748', '8.6542 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 116/1810', 'Training loss: 3.2725', '9.4871 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 117/1810', 'Training loss: 3.2703', '9.6678 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 118/1810', 'Training loss: 3.2681', '8.7646 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 119/1810', 'Training loss: 3.2657', '9.0331 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 120/1810', 'Training loss: 3.2634', '8.3248 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 121/1810', 'Training loss: 3.2611', '8.4444 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 122/1810', 'Training loss: 3.2588', '8.5174 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 123/1810', 'Training loss: 3.2565', '8.4150 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 124/1810', 'Training loss: 3.2543', '8.2940 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 125/1810', 'Training loss: 3.2520', '8.4893 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 126/1810', 'Training loss: 3.2497', '8.4340 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 127/1810', 'Training loss: 3.2472', '8.7705 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 128/1810', 'Training loss: 3.2447', '8.7185 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 129/1810', 'Training loss: 3.2426', '9.3946 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 130/1810', 'Training loss: 3.2406', '8.2306 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 131/1810', 'Training loss: 3.2386', '9.0310 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 132/1810', 'Training loss: 3.2364', '8.3471 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 133/1810', 'Training loss: 3.2342', '8.2368 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 134/1810', 'Training loss: 3.2321', '8.0217 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 135/1810', 'Training loss: 3.2297', '7.9456 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 136/1810', 'Training loss: 3.2275', '9.5391 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 137/1810', 'Training loss: 3.2250', '8.4428 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 138/1810', 'Training loss: 3.2227', '7.8372 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 139/1810', 'Training loss: 3.2204', '7.6119 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 140/1810', 'Training loss: 3.2178', '8.0365 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 141/1810', 'Training loss: 3.2152', '8.0597 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 142/1810', 'Training loss: 3.2128', '8.5350 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 143/1810', 'Training loss: 3.2101', '7.8498 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 144/1810', 'Training loss: 3.2076', '7.1778 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 145/1810', 'Training loss: 3.2050', '7.5601 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 146/1810', 'Training loss: 3.2021', '7.3169 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 147/1810', 'Training loss: 3.1994', '6.6179 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 148/1810', 'Training loss: 3.1967', '6.9066 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 149/1810', 'Training loss: 3.1940', '7.5284 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 150/1810', 'Training loss: 3.1913', '7.1892 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 151/1810', 'Training loss: 3.1888', '8.1296 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 152/1810', 'Training loss: 3.1861', '8.1649 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 153/1810', 'Training loss: 3.1833', '8.3324 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 154/1810', 'Training loss: 3.1805', '7.5124 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 155/1810', 'Training loss: 3.1778', '6.5935 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 156/1810', 'Training loss: 3.1752', '6.5745 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 157/1810', 'Training loss: 3.1727', '6.7229 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 158/1810', 'Training loss: 3.1701', '6.6902 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 159/1810', 'Training loss: 3.1671', '6.6416 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 160/1810', 'Training loss: 3.1644', '6.8065 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 161/1810', 'Training loss: 3.1614', '6.6147 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 162/1810', 'Training loss: 3.1586', '7.0400 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 163/1810', 'Training loss: 3.1556', '6.5989 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 164/1810', 'Training loss: 3.1526', '7.9601 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 165/1810', 'Training loss: 3.1495', '6.6785 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 166/1810', 'Training loss: 3.1464', '6.4766 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 167/1810', 'Training loss: 3.1433', '6.4727 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 168/1810', 'Training loss: 3.1402', '6.3842 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 169/1810', 'Training loss: 3.1370', '6.3498 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 170/1810', 'Training loss: 3.1340', '6.4480 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 171/1810', 'Training loss: 3.1309', '6.3589 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 172/1810', 'Training loss: 3.1279', '6.3499 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 173/1810', 'Training loss: 3.1249', '6.5480 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 174/1810', 'Training loss: 3.1217', '6.3562 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 175/1810', 'Training loss: 3.1186', '6.4476 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 176/1810', 'Training loss: 3.1156', '6.4339 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 177/1810', 'Training loss: 3.1125', '6.5260 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 178/1810', 'Training loss: 3.1097', '7.7079 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 179/1810', 'Training loss: 3.1068', '8.7885 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 180/1810', 'Training loss: 3.1039', '7.7408 sec/batch')\n",
      "('Epoch 1/10 ', 'Iteration 181/1810', 'Training loss: 3.1008', '8.9564 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 182/1810', 'Training loss: 2.5884', '8.2696 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 183/1810', 'Training loss: 2.5528', '7.7538 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 184/1810', 'Training loss: 2.5276', '9.4785 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 185/1810', 'Training loss: 2.5209', '10.3023 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 186/1810', 'Training loss: 2.5148', '7.2041 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 187/1810', 'Training loss: 2.5160', '7.2814 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 188/1810', 'Training loss: 2.5118', '7.1568 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 189/1810', 'Training loss: 2.5066', '7.1810 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 190/1810', 'Training loss: 2.5050', '7.2715 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 191/1810', 'Training loss: 2.5011', '7.2148 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 192/1810', 'Training loss: 2.5002', '7.1795 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 193/1810', 'Training loss: 2.4985', '7.0326 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 194/1810', 'Training loss: 2.4963', '7.0745 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 195/1810', 'Training loss: 2.4929', '7.1870 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 196/1810', 'Training loss: 2.4928', '7.1023 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 197/1810', 'Training loss: 2.4899', '8.7729 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 198/1810', 'Training loss: 2.4871', '8.3619 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 199/1810', 'Training loss: 2.4856', '8.6631 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 200/1810', 'Training loss: 2.4855', '8.0140 sec/batch')\n",
      "('Validation loss:', 2.3573864, 'Saving checkpoint!')\n",
      "('Epoch 2/10 ', 'Iteration 201/1810', 'Training loss: 2.4849', '9.2120 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 202/1810', 'Training loss: 2.4842', '7.4212 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 203/1810', 'Training loss: 2.4809', '7.3207 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 204/1810', 'Training loss: 2.4786', '8.6983 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 205/1810', 'Training loss: 2.4766', '7.9249 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 206/1810', 'Training loss: 2.4744', '6.4299 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 207/1810', 'Training loss: 2.4728', '6.4515 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 208/1810', 'Training loss: 2.4711', '6.3539 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 209/1810', 'Training loss: 2.4680', '6.4891 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 210/1810', 'Training loss: 2.4661', '6.4851 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 211/1810', 'Training loss: 2.4642', '6.3406 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 212/1810', 'Training loss: 2.4621', '6.4851 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 213/1810', 'Training loss: 2.4604', '6.3999 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 214/1810', 'Training loss: 2.4581', '6.3711 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 215/1810', 'Training loss: 2.4566', '6.5095 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 216/1810', 'Training loss: 2.4543', '6.3634 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 217/1810', 'Training loss: 2.4528', '8.5304 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 218/1810', 'Training loss: 2.4510', '6.9191 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 219/1810', 'Training loss: 2.4484', '6.6006 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 220/1810', 'Training loss: 2.4466', '6.5319 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 221/1810', 'Training loss: 2.4444', '6.5994 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 222/1810', 'Training loss: 2.4414', '6.6766 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 223/1810', 'Training loss: 2.4394', '6.7663 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 224/1810', 'Training loss: 2.4372', '6.6697 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 225/1810', 'Training loss: 2.4350', '6.6738 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 226/1810', 'Training loss: 2.4329', '6.4153 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 227/1810', 'Training loss: 2.4305', '6.4678 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 228/1810', 'Training loss: 2.4285', '6.5333 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 229/1810', 'Training loss: 2.4260', '6.3075 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 230/1810', 'Training loss: 2.4241', '6.5180 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 231/1810', 'Training loss: 2.4224', '6.3437 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 232/1810', 'Training loss: 2.4207', '6.3713 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 233/1810', 'Training loss: 2.4188', '6.4084 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 234/1810', 'Training loss: 2.4174', '6.4347 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 235/1810', 'Training loss: 2.4156', '6.2980 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 236/1810', 'Training loss: 2.4140', '6.4533 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 237/1810', 'Training loss: 2.4123', '6.5336 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 238/1810', 'Training loss: 2.4106', '6.4486 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 239/1810', 'Training loss: 2.4089', '6.3313 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 240/1810', 'Training loss: 2.4072', '7.6734 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 241/1810', 'Training loss: 2.4058', '8.1419 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 242/1810', 'Training loss: 2.4047', '7.5235 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 243/1810', 'Training loss: 2.4030', '6.6709 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 244/1810', 'Training loss: 2.4014', '6.3596 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 245/1810', 'Training loss: 2.3998', '6.4704 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 246/1810', 'Training loss: 2.3986', '6.6264 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 247/1810', 'Training loss: 2.3977', '6.5554 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 248/1810', 'Training loss: 2.3962', '6.4342 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 249/1810', 'Training loss: 2.3948', '6.3631 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 250/1810', 'Training loss: 2.3930', '6.4085 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 251/1810', 'Training loss: 2.3918', '6.3279 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 252/1810', 'Training loss: 2.3904', '6.3889 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 253/1810', 'Training loss: 2.3889', '6.4212 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 254/1810', 'Training loss: 2.3875', '6.5143 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 255/1810', 'Training loss: 2.3864', '6.3408 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 256/1810', 'Training loss: 2.3852', '6.4533 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 257/1810', 'Training loss: 2.3837', '6.3040 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 258/1810', 'Training loss: 2.3822', '6.2820 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 259/1810', 'Training loss: 2.3810', '6.4473 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 260/1810', 'Training loss: 2.3797', '7.2691 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 261/1810', 'Training loss: 2.3787', '8.4275 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 262/1810', 'Training loss: 2.3772', '7.8426 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 263/1810', 'Training loss: 2.3757', '8.5123 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 264/1810', 'Training loss: 2.3743', '7.5084 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 265/1810', 'Training loss: 2.3731', '9.4128 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 266/1810', 'Training loss: 2.3715', '9.4352 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 267/1810', 'Training loss: 2.3699', '8.5277 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 268/1810', 'Training loss: 2.3681', '10.5963 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 269/1810', 'Training loss: 2.3665', '8.4670 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 270/1810', 'Training loss: 2.3651', '8.6886 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 271/1810', 'Training loss: 2.3638', '9.7195 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 272/1810', 'Training loss: 2.3622', '7.8887 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 273/1810', 'Training loss: 2.3607', '9.0319 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 274/1810', 'Training loss: 2.3593', '7.3651 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 275/1810', 'Training loss: 2.3580', '7.3451 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 276/1810', 'Training loss: 2.3566', '8.2408 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 277/1810', 'Training loss: 2.3552', '8.8261 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 278/1810', 'Training loss: 2.3539', '6.5962 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 279/1810', 'Training loss: 2.3525', '7.2511 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 280/1810', 'Training loss: 2.3508', '7.4108 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 281/1810', 'Training loss: 2.3495', '6.7529 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 282/1810', 'Training loss: 2.3480', '7.0090 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 283/1810', 'Training loss: 2.3466', '7.5255 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 284/1810', 'Training loss: 2.3452', '7.4343 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 285/1810', 'Training loss: 2.3436', '7.1850 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 286/1810', 'Training loss: 2.3426', '7.2662 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 287/1810', 'Training loss: 2.3412', '7.1800 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 288/1810', 'Training loss: 2.3398', '7.3599 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 289/1810', 'Training loss: 2.3387', '8.8935 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 290/1810', 'Training loss: 2.3373', '7.9271 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 291/1810', 'Training loss: 2.3362', '7.4467 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 292/1810', 'Training loss: 2.3350', '7.8320 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 293/1810', 'Training loss: 2.3340', '8.0140 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 294/1810', 'Training loss: 2.3328', '7.3100 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 295/1810', 'Training loss: 2.3317', '7.2393 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 296/1810', 'Training loss: 2.3307', '8.5437 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 297/1810', 'Training loss: 2.3294', '8.9259 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 298/1810', 'Training loss: 2.3281', '8.8261 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 299/1810', 'Training loss: 2.3268', '10.8588 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 300/1810', 'Training loss: 2.3255', '8.4233 sec/batch')\n",
      "('Validation loss:', 2.0968137, 'Saving checkpoint!')\n",
      "('Epoch 2/10 ', 'Iteration 301/1810', 'Training loss: 2.3244', '6.8335 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 302/1810', 'Training loss: 2.3231', '6.3777 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 303/1810', 'Training loss: 2.3221', '6.5322 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 304/1810', 'Training loss: 2.3207', '6.4094 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 305/1810', 'Training loss: 2.3198', '6.4377 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 306/1810', 'Training loss: 2.3187', '6.2386 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 307/1810', 'Training loss: 2.3175', '6.4274 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 308/1810', 'Training loss: 2.3161', '6.2458 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 309/1810', 'Training loss: 2.3150', '6.3949 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 310/1810', 'Training loss: 2.3139', '6.4097 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 311/1810', 'Training loss: 2.3128', '6.3563 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 312/1810', 'Training loss: 2.3118', '6.4516 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 313/1810', 'Training loss: 2.3108', '6.7897 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 314/1810', 'Training loss: 2.3098', '9.1119 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 315/1810', 'Training loss: 2.3087', '9.1567 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 316/1810', 'Training loss: 2.3073', '9.4586 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 317/1810', 'Training loss: 2.3063', '8.3981 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 318/1810', 'Training loss: 2.3050', '8.6058 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 319/1810', 'Training loss: 2.3042', '8.9291 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 320/1810', 'Training loss: 2.3031', '9.3175 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 321/1810', 'Training loss: 2.3020', '7.5812 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 322/1810', 'Training loss: 2.3009', '7.3989 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 323/1810', 'Training loss: 2.3001', '7.2801 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 324/1810', 'Training loss: 2.2990', '9.1389 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 325/1810', 'Training loss: 2.2980', '7.7827 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 326/1810', 'Training loss: 2.2970', '6.7830 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 327/1810', 'Training loss: 2.2958', '6.5473 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 328/1810', 'Training loss: 2.2948', '6.7154 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 329/1810', 'Training loss: 2.2938', '6.7592 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 330/1810', 'Training loss: 2.2929', '6.5352 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 331/1810', 'Training loss: 2.2919', '6.6966 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 332/1810', 'Training loss: 2.2910', '6.5069 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 333/1810', 'Training loss: 2.2901', '6.5271 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 334/1810', 'Training loss: 2.2893', '6.6132 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 335/1810', 'Training loss: 2.2883', '6.6302 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 336/1810', 'Training loss: 2.2872', '6.5944 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 337/1810', 'Training loss: 2.2865', '8.7963 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 338/1810', 'Training loss: 2.2855', '8.3757 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 339/1810', 'Training loss: 2.2847', '7.7985 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 340/1810', 'Training loss: 2.2836', '7.9584 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 341/1810', 'Training loss: 2.2827', '8.5940 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 342/1810', 'Training loss: 2.2817', '8.3313 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 343/1810', 'Training loss: 2.2808', '7.1777 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 344/1810', 'Training loss: 2.2797', '7.0136 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 345/1810', 'Training loss: 2.2786', '8.8320 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 346/1810', 'Training loss: 2.2776', '10.7373 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 347/1810', 'Training loss: 2.2766', '8.2703 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 348/1810', 'Training loss: 2.2759', '8.5694 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 349/1810', 'Training loss: 2.2749', '8.4341 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 350/1810', 'Training loss: 2.2739', '8.2628 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 351/1810', 'Training loss: 2.2730', '6.4039 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 352/1810', 'Training loss: 2.2721', '6.3875 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 353/1810', 'Training loss: 2.2714', '6.4606 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 354/1810', 'Training loss: 2.2705', '6.4076 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 355/1810', 'Training loss: 2.2694', '6.3582 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 356/1810', 'Training loss: 2.2683', '6.4583 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 357/1810', 'Training loss: 2.2673', '6.2691 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 358/1810', 'Training loss: 2.2663', '6.3701 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 359/1810', 'Training loss: 2.2656', '6.3191 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 360/1810', 'Training loss: 2.2647', '6.2451 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 361/1810', 'Training loss: 2.2639', '6.2761 sec/batch')\n",
      "('Epoch 2/10 ', 'Iteration 362/1810', 'Training loss: 2.2629', '6.2798 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 363/1810', 'Training loss: 2.1526', '6.3547 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 364/1810', 'Training loss: 2.1170', '6.2426 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 365/1810', 'Training loss: 2.0959', '6.2952 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 366/1810', 'Training loss: 2.0922', '6.3299 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 367/1810', 'Training loss: 2.0899', '6.2148 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 368/1810', 'Training loss: 2.0920', '6.2228 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 369/1810', 'Training loss: 2.0880', '6.4268 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 370/1810', 'Training loss: 2.0868', '6.2543 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 371/1810', 'Training loss: 2.0848', '6.3032 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 372/1810', 'Training loss: 2.0823', '6.3764 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 373/1810', 'Training loss: 2.0846', '6.3638 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 374/1810', 'Training loss: 2.0844', '6.2508 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 375/1810', 'Training loss: 2.0825', '6.2903 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 376/1810', 'Training loss: 2.0799', '6.5079 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 377/1810', 'Training loss: 2.0817', '6.2285 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 378/1810', 'Training loss: 2.0790', '6.4230 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 379/1810', 'Training loss: 2.0780', '6.2645 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 380/1810', 'Training loss: 2.0773', '6.2018 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 381/1810', 'Training loss: 2.0781', '6.2936 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 382/1810', 'Training loss: 2.0784', '6.3281 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 383/1810', 'Training loss: 2.0789', '8.4968 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 384/1810', 'Training loss: 2.0774', '7.7516 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 385/1810', 'Training loss: 2.0761', '8.1294 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 386/1810', 'Training loss: 2.0760', '7.0846 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 387/1810', 'Training loss: 2.0744', '7.5462 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 388/1810', 'Training loss: 2.0742', '7.5421 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 389/1810', 'Training loss: 2.0737', '7.9472 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 390/1810', 'Training loss: 2.0713', '7.3847 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 391/1810', 'Training loss: 2.0712', '6.9191 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 392/1810', 'Training loss: 2.0709', '6.5491 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 393/1810', 'Training loss: 2.0703', '7.1769 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 394/1810', 'Training loss: 2.0702', '6.3362 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 395/1810', 'Training loss: 2.0691', '6.3719 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 396/1810', 'Training loss: 2.0685', '6.3277 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 397/1810', 'Training loss: 2.0677', '6.2138 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 398/1810', 'Training loss: 2.0679', '6.2798 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 399/1810', 'Training loss: 2.0670', '6.3953 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 400/1810', 'Training loss: 2.0652', '6.2618 sec/batch')\n",
      "('Validation loss:', 1.9337404, 'Saving checkpoint!')\n",
      "('Epoch 3/10 ', 'Iteration 401/1810', 'Training loss: 2.0652', '6.3636 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 402/1810', 'Training loss: 2.0640', '6.2933 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 403/1810', 'Training loss: 2.0621', '6.2611 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 404/1810', 'Training loss: 2.0609', '6.3113 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 405/1810', 'Training loss: 2.0600', '6.2983 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 406/1810', 'Training loss: 2.0588', '6.3163 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 407/1810', 'Training loss: 2.0581', '6.2780 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 408/1810', 'Training loss: 2.0570', '6.2442 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 409/1810', 'Training loss: 2.0557', '6.3884 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 410/1810', 'Training loss: 2.0539', '6.2600 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 411/1810', 'Training loss: 2.0533', '6.2181 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 412/1810', 'Training loss: 2.0525', '8.2298 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 413/1810', 'Training loss: 2.0518', '8.0759 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 414/1810', 'Training loss: 2.0509', '8.4291 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 415/1810', 'Training loss: 2.0507', '6.4439 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 416/1810', 'Training loss: 2.0498', '6.6127 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 417/1810', 'Training loss: 2.0491', '8.8268 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 418/1810', 'Training loss: 2.0487', '9.1277 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 419/1810', 'Training loss: 2.0481', '11.8889 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 420/1810', 'Training loss: 2.0474', '9.9960 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 421/1810', 'Training loss: 2.0466', '8.7059 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 422/1810', 'Training loss: 2.0460', '8.5227 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 423/1810', 'Training loss: 2.0457', '11.5035 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 424/1810', 'Training loss: 2.0448', '10.7924 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 425/1810', 'Training loss: 2.0444', '7.4941 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 426/1810', 'Training loss: 2.0438', '9.3768 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 427/1810', 'Training loss: 2.0437', '9.0210 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 428/1810', 'Training loss: 2.0437', '8.8859 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 429/1810', 'Training loss: 2.0428', '8.7647 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 430/1810', 'Training loss: 2.0423', '9.3029 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 431/1810', 'Training loss: 2.0413', '9.4172 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 432/1810', 'Training loss: 2.0409', '8.6864 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 433/1810', 'Training loss: 2.0403', '8.6273 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 434/1810', 'Training loss: 2.0393', '9.8951 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 435/1810', 'Training loss: 2.0387', '6.9436 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 436/1810', 'Training loss: 2.0385', '7.1711 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 437/1810', 'Training loss: 2.0381', '11.2666 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 438/1810', 'Training loss: 2.0373', '10.3502 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 439/1810', 'Training loss: 2.0368', '8.7577 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 440/1810', 'Training loss: 2.0362', '8.9590 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 441/1810', 'Training loss: 2.0354', '8.8536 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 442/1810', 'Training loss: 2.0351', '9.0062 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 443/1810', 'Training loss: 2.0343', '8.4742 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 444/1810', 'Training loss: 2.0336', '8.7278 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 445/1810', 'Training loss: 2.0329', '8.5981 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 446/1810', 'Training loss: 2.0323', '8.6262 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 447/1810', 'Training loss: 2.0313', '8.9346 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 448/1810', 'Training loss: 2.0302', '8.6579 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 449/1810', 'Training loss: 2.0292', '8.6564 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 450/1810', 'Training loss: 2.0283', '8.5908 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 451/1810', 'Training loss: 2.0277', '8.6056 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 452/1810', 'Training loss: 2.0269', '8.8006 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 453/1810', 'Training loss: 2.0259', '8.6522 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 454/1810', 'Training loss: 2.0249', '8.5762 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 455/1810', 'Training loss: 2.0241', '8.7156 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 456/1810', 'Training loss: 2.0233', '8.4864 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 457/1810', 'Training loss: 2.0226', '8.5969 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 458/1810', 'Training loss: 2.0217', '8.5447 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 459/1810', 'Training loss: 2.0209', '8.4605 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 460/1810', 'Training loss: 2.0202', '8.3354 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 461/1810', 'Training loss: 2.0191', '8.7559 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 462/1810', 'Training loss: 2.0184', '8.5094 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 463/1810', 'Training loss: 2.0173', '8.6856 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 464/1810', 'Training loss: 2.0163', '8.4729 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 465/1810', 'Training loss: 2.0151', '8.5852 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 466/1810', 'Training loss: 2.0141', '8.6035 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 467/1810', 'Training loss: 2.0138', '8.6987 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 468/1810', 'Training loss: 2.0130', '9.4913 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 469/1810', 'Training loss: 2.0121', '8.5020 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 470/1810', 'Training loss: 2.0115', '8.5300 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 471/1810', 'Training loss: 2.0106', '8.4407 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 472/1810', 'Training loss: 2.0099', '7.9138 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 473/1810', 'Training loss: 2.0092', '7.8282 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 474/1810', 'Training loss: 2.0089', '7.8729 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 475/1810', 'Training loss: 2.0084', '7.8746 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 476/1810', 'Training loss: 2.0080', '7.8318 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 477/1810', 'Training loss: 2.0073', '7.7745 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 478/1810', 'Training loss: 2.0064', '7.6382 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 479/1810', 'Training loss: 2.0057', '7.8399 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 480/1810', 'Training loss: 2.0049', '7.7053 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 481/1810', 'Training loss: 2.0042', '7.7915 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 482/1810', 'Training loss: 2.0035', '7.8947 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 483/1810', 'Training loss: 2.0028', '7.8528 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 484/1810', 'Training loss: 2.0022', '7.8810 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 485/1810', 'Training loss: 2.0013', '7.8991 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 486/1810', 'Training loss: 2.0010', '7.8548 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 487/1810', 'Training loss: 2.0002', '7.7138 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 488/1810', 'Training loss: 1.9995', '7.7530 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 489/1810', 'Training loss: 1.9987', '7.8740 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 490/1810', 'Training loss: 1.9981', '7.7019 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 491/1810', 'Training loss: 1.9974', '8.0233 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 492/1810', 'Training loss: 1.9968', '7.7670 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 493/1810', 'Training loss: 1.9961', '7.9643 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 494/1810', 'Training loss: 1.9957', '7.9716 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 495/1810', 'Training loss: 1.9951', '7.7665 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 496/1810', 'Training loss: 1.9945', '7.7244 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 497/1810', 'Training loss: 1.9937', '7.7677 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 498/1810', 'Training loss: 1.9932', '7.8392 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 499/1810', 'Training loss: 1.9924', '7.6093 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 500/1810', 'Training loss: 1.9920', '7.7108 sec/batch')\n",
      "('Validation loss:', 1.8096597, 'Saving checkpoint!')\n",
      "('Epoch 3/10 ', 'Iteration 501/1810', 'Training loss: 1.9918', '7.8051 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 502/1810', 'Training loss: 1.9912', '7.7075 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 503/1810', 'Training loss: 1.9905', '8.6645 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 504/1810', 'Training loss: 1.9901', '7.6252 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 505/1810', 'Training loss: 1.9896', '7.7352 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 506/1810', 'Training loss: 1.9891', '7.9172 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 507/1810', 'Training loss: 1.9886', '7.8537 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 508/1810', 'Training loss: 1.9880', '7.9023 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 509/1810', 'Training loss: 1.9874', '7.7904 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 510/1810', 'Training loss: 1.9869', '7.7979 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 511/1810', 'Training loss: 1.9865', '7.6817 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 512/1810', 'Training loss: 1.9859', '7.7014 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 513/1810', 'Training loss: 1.9854', '7.7338 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 514/1810', 'Training loss: 1.9850', '7.8945 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 515/1810', 'Training loss: 1.9848', '7.8955 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 516/1810', 'Training loss: 1.9842', '7.6880 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 517/1810', 'Training loss: 1.9836', '7.6741 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 518/1810', 'Training loss: 1.9832', '8.0219 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 519/1810', 'Training loss: 1.9826', '8.1998 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 520/1810', 'Training loss: 1.9823', '7.8953 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 521/1810', 'Training loss: 1.9816', '7.9285 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 522/1810', 'Training loss: 1.9811', '7.9742 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 523/1810', 'Training loss: 1.9807', '8.2067 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 524/1810', 'Training loss: 1.9802', '7.8971 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 525/1810', 'Training loss: 1.9796', '7.8383 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 526/1810', 'Training loss: 1.9790', '7.8933 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 527/1810', 'Training loss: 1.9785', '7.6573 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 528/1810', 'Training loss: 1.9781', '7.6008 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 529/1810', 'Training loss: 1.9778', '7.7665 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 530/1810', 'Training loss: 1.9773', '8.1367 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 531/1810', 'Training loss: 1.9768', '7.8981 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 532/1810', 'Training loss: 1.9765', '7.8049 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 533/1810', 'Training loss: 1.9760', '7.6732 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 534/1810', 'Training loss: 1.9757', '8.6180 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 535/1810', 'Training loss: 1.9753', '7.9189 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 536/1810', 'Training loss: 1.9746', '7.8551 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 537/1810', 'Training loss: 1.9740', '7.7327 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 538/1810', 'Training loss: 1.9733', '7.8298 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 539/1810', 'Training loss: 1.9727', '7.7022 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 540/1810', 'Training loss: 1.9724', '7.7208 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 541/1810', 'Training loss: 1.9719', '7.6634 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 542/1810', 'Training loss: 1.9714', '7.9127 sec/batch')\n",
      "('Epoch 3/10 ', 'Iteration 543/1810', 'Training loss: 1.9709', '7.6223 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 544/1810', 'Training loss: 1.9422', '7.6497 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 545/1810', 'Training loss: 1.9076', '8.6768 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 546/1810', 'Training loss: 1.8903', '7.9255 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 547/1810', 'Training loss: 1.8855', '7.6764 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 548/1810', 'Training loss: 1.8825', '7.7263 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 549/1810', 'Training loss: 1.8843', '7.7594 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 550/1810', 'Training loss: 1.8786', '7.8106 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 551/1810', 'Training loss: 1.8759', '7.6811 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 552/1810', 'Training loss: 1.8741', '7.7263 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 553/1810', 'Training loss: 1.8722', '7.7960 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 554/1810', 'Training loss: 1.8747', '7.7212 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 555/1810', 'Training loss: 1.8739', '7.8286 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 556/1810', 'Training loss: 1.8723', '7.7258 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 557/1810', 'Training loss: 1.8704', '7.8532 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 558/1810', 'Training loss: 1.8728', '7.8608 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 559/1810', 'Training loss: 1.8691', '7.7014 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 560/1810', 'Training loss: 1.8673', '7.7203 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 561/1810', 'Training loss: 1.8671', '7.8392 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 562/1810', 'Training loss: 1.8666', '7.7864 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 563/1810', 'Training loss: 1.8679', '7.6440 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 564/1810', 'Training loss: 1.8674', '7.6823 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 565/1810', 'Training loss: 1.8662', '7.8569 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 566/1810', 'Training loss: 1.8655', '7.8913 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 567/1810', 'Training loss: 1.8662', '7.8627 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 568/1810', 'Training loss: 1.8654', '7.7279 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 569/1810', 'Training loss: 1.8653', '7.7708 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 570/1810', 'Training loss: 1.8652', '7.6551 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 571/1810', 'Training loss: 1.8630', '7.8306 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 572/1810', 'Training loss: 1.8629', '7.8317 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 573/1810', 'Training loss: 1.8630', '7.7107 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 574/1810', 'Training loss: 1.8629', '7.7563 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 575/1810', 'Training loss: 1.8627', '7.8103 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 576/1810', 'Training loss: 1.8622', '8.6628 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 577/1810', 'Training loss: 1.8616', '7.9345 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 578/1810', 'Training loss: 1.8614', '7.6700 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 579/1810', 'Training loss: 1.8621', '7.6038 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 580/1810', 'Training loss: 1.8617', '7.5545 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 581/1810', 'Training loss: 1.8599', '7.9593 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 582/1810', 'Training loss: 1.8592', '7.9097 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 583/1810', 'Training loss: 1.8578', '7.7402 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 584/1810', 'Training loss: 1.8561', '7.6988 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 585/1810', 'Training loss: 1.8551', '7.6568 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 586/1810', 'Training loss: 1.8542', '7.9101 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 587/1810', 'Training loss: 1.8532', '7.9533 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 588/1810', 'Training loss: 1.8526', '7.6626 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 589/1810', 'Training loss: 1.8518', '7.6551 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 590/1810', 'Training loss: 1.8506', '7.9376 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 591/1810', 'Training loss: 1.8493', '7.9312 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 592/1810', 'Training loss: 1.8490', '7.8813 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 593/1810', 'Training loss: 1.8486', '7.7699 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 594/1810', 'Training loss: 1.8481', '7.6626 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 595/1810', 'Training loss: 1.8473', '7.6514 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 596/1810', 'Training loss: 1.8473', '7.9097 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 597/1810', 'Training loss: 1.8465', '7.8682 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 598/1810', 'Training loss: 1.8461', '7.6901 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 599/1810', 'Training loss: 1.8460', '7.7946 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 600/1810', 'Training loss: 1.8456', '7.9104 sec/batch')\n",
      "('Validation loss:', 1.7137989, 'Saving checkpoint!')\n",
      "('Epoch 4/10 ', 'Iteration 601/1810', 'Training loss: 1.8457', '7.7342 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 602/1810', 'Training loss: 1.8453', '7.5705 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 603/1810', 'Training loss: 1.8449', '7.6522 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 604/1810', 'Training loss: 1.8449', '7.7619 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 605/1810', 'Training loss: 1.8443', '7.9196 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 606/1810', 'Training loss: 1.8438', '7.6140 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 607/1810', 'Training loss: 1.8433', '7.6392 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 608/1810', 'Training loss: 1.8436', '7.7906 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 609/1810', 'Training loss: 1.8442', '7.9553 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 610/1810', 'Training loss: 1.8435', '7.8023 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 611/1810', 'Training loss: 1.8433', '7.7846 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 612/1810', 'Training loss: 1.8424', '7.8447 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 613/1810', 'Training loss: 1.8423', '7.6672 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 614/1810', 'Training loss: 1.8419', '7.7145 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 615/1810', 'Training loss: 1.8413', '7.7241 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 616/1810', 'Training loss: 1.8410', '8.0442 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 617/1810', 'Training loss: 1.8409', '7.7922 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 618/1810', 'Training loss: 1.8407', '7.7778 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 619/1810', 'Training loss: 1.8403', '7.6133 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 620/1810', 'Training loss: 1.8400', '7.7364 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 621/1810', 'Training loss: 1.8394', '7.7934 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 622/1810', 'Training loss: 1.8387', '8.1185 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 623/1810', 'Training loss: 1.8385', '7.7382 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 624/1810', 'Training loss: 1.8380', '7.8025 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 625/1810', 'Training loss: 1.8373', '7.7791 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 626/1810', 'Training loss: 1.8368', '7.8431 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 627/1810', 'Training loss: 1.8363', '7.8388 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 628/1810', 'Training loss: 1.8355', '7.7335 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 629/1810', 'Training loss: 1.8346', '7.6142 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 630/1810', 'Training loss: 1.8338', '7.8067 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 631/1810', 'Training loss: 1.8332', '7.9629 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 632/1810', 'Training loss: 1.8328', '7.7735 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 633/1810', 'Training loss: 1.8322', '7.7745 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 634/1810', 'Training loss: 1.8315', '7.8324 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 635/1810', 'Training loss: 1.8306', '7.8535 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 636/1810', 'Training loss: 1.8302', '7.7737 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 637/1810', 'Training loss: 1.8296', '7.9057 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 638/1810', 'Training loss: 1.8292', '7.7307 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 639/1810', 'Training loss: 1.8286', '7.7576 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 640/1810', 'Training loss: 1.8282', '7.7839 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 641/1810', 'Training loss: 1.8278', '7.8753 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 642/1810', 'Training loss: 1.8271', '7.8894 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 643/1810', 'Training loss: 1.8266', '7.8070 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 644/1810', 'Training loss: 1.8257', '7.7535 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 645/1810', 'Training loss: 1.8249', '7.6448 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 646/1810', 'Training loss: 1.8241', '7.9512 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 647/1810', 'Training loss: 1.8232', '7.7080 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 648/1810', 'Training loss: 1.8230', '7.6805 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 649/1810', 'Training loss: 1.8224', '7.7973 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 650/1810', 'Training loss: 1.8218', '7.7536 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 651/1810', 'Training loss: 1.8215', '7.8097 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 652/1810', 'Training loss: 1.8208', '7.8492 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 653/1810', 'Training loss: 1.8203', '7.7873 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 654/1810', 'Training loss: 1.8198', '7.7301 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 655/1810', 'Training loss: 1.8197', '7.5446 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 656/1810', 'Training loss: 1.8195', '7.6659 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 657/1810', 'Training loss: 1.8191', '7.5833 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 658/1810', 'Training loss: 1.8186', '7.9203 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 659/1810', 'Training loss: 1.8180', '7.8235 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 660/1810', 'Training loss: 1.8174', '7.6067 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 661/1810', 'Training loss: 1.8168', '7.7302 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 662/1810', 'Training loss: 1.8163', '7.9971 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 663/1810', 'Training loss: 1.8157', '7.8275 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 664/1810', 'Training loss: 1.8152', '8.4006 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 665/1810', 'Training loss: 1.8148', '7.8449 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 666/1810', 'Training loss: 1.8142', '7.7733 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 667/1810', 'Training loss: 1.8141', '7.6455 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 668/1810', 'Training loss: 1.8134', '7.8263 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 669/1810', 'Training loss: 1.8128', '7.8350 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 670/1810', 'Training loss: 1.8122', '7.7938 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 671/1810', 'Training loss: 1.8117', '7.7894 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 672/1810', 'Training loss: 1.8112', '7.6195 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 673/1810', 'Training loss: 1.8108', '7.7278 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 674/1810', 'Training loss: 1.8103', '7.8075 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 675/1810', 'Training loss: 1.8099', '7.8590 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 676/1810', 'Training loss: 1.8095', '7.6363 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 677/1810', 'Training loss: 1.8089', '7.8837 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 678/1810', 'Training loss: 1.8083', '8.0565 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 679/1810', 'Training loss: 1.8080', '7.6657 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 680/1810', 'Training loss: 1.8074', '7.7408 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 681/1810', 'Training loss: 1.8071', '7.7337 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 682/1810', 'Training loss: 1.8069', '7.8281 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 683/1810', 'Training loss: 1.8064', '7.9262 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 684/1810', 'Training loss: 1.8060', '7.7546 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 685/1810', 'Training loss: 1.8057', '7.6813 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 686/1810', 'Training loss: 1.8054', '8.4854 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 687/1810', 'Training loss: 1.8051', '6.8145 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 688/1810', 'Training loss: 1.8047', '7.3895 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 689/1810', 'Training loss: 1.8043', '9.1065 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 690/1810', 'Training loss: 1.8039', '6.9818 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 691/1810', 'Training loss: 1.8035', '7.2146 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 692/1810', 'Training loss: 1.8033', '6.4580 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 693/1810', 'Training loss: 1.8027', '6.8835 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 694/1810', 'Training loss: 1.8025', '6.5379 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 695/1810', 'Training loss: 1.8022', '7.0313 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 696/1810', 'Training loss: 1.8022', '6.1872 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 697/1810', 'Training loss: 1.8019', '6.2901 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 698/1810', 'Training loss: 1.8013', '6.4141 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 699/1810', 'Training loss: 1.8011', '6.2522 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 700/1810', 'Training loss: 1.8006', '6.2038 sec/batch')\n",
      "('Validation loss:', 1.6295595, 'Saving checkpoint!')\n",
      "('Epoch 4/10 ', 'Iteration 701/1810', 'Training loss: 1.8007', '6.3031 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 702/1810', 'Training loss: 1.8003', '6.6113 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 703/1810', 'Training loss: 1.8000', '6.2984 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 704/1810', 'Training loss: 1.7997', '6.2058 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 705/1810', 'Training loss: 1.7993', '6.3272 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 706/1810', 'Training loss: 1.7987', '6.4012 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 707/1810', 'Training loss: 1.7982', '6.2698 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 708/1810', 'Training loss: 1.7978', '6.1566 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 709/1810', 'Training loss: 1.7976', '6.3440 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 710/1810', 'Training loss: 1.7975', '6.3165 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 711/1810', 'Training loss: 1.7971', '6.4160 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 712/1810', 'Training loss: 1.7967', '6.2428 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 713/1810', 'Training loss: 1.7965', '6.2692 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 714/1810', 'Training loss: 1.7963', '6.2499 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 715/1810', 'Training loss: 1.7962', '6.2038 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 716/1810', 'Training loss: 1.7959', '6.3230 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 717/1810', 'Training loss: 1.7955', '6.2162 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 718/1810', 'Training loss: 1.7950', '6.3212 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 719/1810', 'Training loss: 1.7945', '6.4041 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 720/1810', 'Training loss: 1.7941', '6.1832 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 721/1810', 'Training loss: 1.7938', '6.2911 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 722/1810', 'Training loss: 1.7935', '6.3377 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 723/1810', 'Training loss: 1.7934', '6.2710 sec/batch')\n",
      "('Epoch 4/10 ', 'Iteration 724/1810', 'Training loss: 1.7930', '6.3534 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 725/1810', 'Training loss: 1.8055', '6.3653 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 726/1810', 'Training loss: 1.7703', '6.3078 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 727/1810', 'Training loss: 1.7521', '6.1755 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 728/1810', 'Training loss: 1.7456', '6.2889 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 729/1810', 'Training loss: 1.7429', '6.3196 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 730/1810', 'Training loss: 1.7427', '6.2076 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 731/1810', 'Training loss: 1.7361', '6.3160 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 732/1810', 'Training loss: 1.7346', '6.3736 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 733/1810', 'Training loss: 1.7306', '6.3229 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 734/1810', 'Training loss: 1.7275', '6.2437 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 735/1810', 'Training loss: 1.7295', '6.1376 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 736/1810', 'Training loss: 1.7283', '6.3679 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 737/1810', 'Training loss: 1.7263', '6.3006 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 738/1810', 'Training loss: 1.7249', '6.2561 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 739/1810', 'Training loss: 1.7270', '6.3507 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 740/1810', 'Training loss: 1.7232', '6.2885 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 741/1810', 'Training loss: 1.7219', '6.3515 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 742/1810', 'Training loss: 1.7216', '6.2612 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 743/1810', 'Training loss: 1.7214', '6.3980 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 744/1810', 'Training loss: 1.7231', '6.3562 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 745/1810', 'Training loss: 1.7226', '6.3107 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 746/1810', 'Training loss: 1.7213', '6.1963 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 747/1810', 'Training loss: 1.7204', '6.3455 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 748/1810', 'Training loss: 1.7208', '6.2399 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 749/1810', 'Training loss: 1.7197', '6.2113 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 750/1810', 'Training loss: 1.7189', '6.3198 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 751/1810', 'Training loss: 1.7186', '6.4400 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 752/1810', 'Training loss: 1.7164', '6.3287 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 753/1810', 'Training loss: 1.7158', '6.1697 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 754/1810', 'Training loss: 1.7155', '6.3378 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 755/1810', 'Training loss: 1.7150', '6.4270 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 756/1810', 'Training loss: 1.7148', '6.2366 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 757/1810', 'Training loss: 1.7142', '6.3138 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 758/1810', 'Training loss: 1.7137', '6.1479 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 759/1810', 'Training loss: 1.7131', '6.3411 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 760/1810', 'Training loss: 1.7139', '6.3968 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 761/1810', 'Training loss: 1.7133', '6.3166 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 762/1810', 'Training loss: 1.7115', '6.4076 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 763/1810', 'Training loss: 1.7108', '6.1994 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 764/1810', 'Training loss: 1.7097', '6.2646 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 765/1810', 'Training loss: 1.7081', '6.3905 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 766/1810', 'Training loss: 1.7071', '6.2601 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 767/1810', 'Training loss: 1.7061', '6.2133 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 768/1810', 'Training loss: 1.7052', '6.2178 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 769/1810', 'Training loss: 1.7049', '6.2901 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 770/1810', 'Training loss: 1.7044', '6.2920 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 771/1810', 'Training loss: 1.7033', '6.1803 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 772/1810', 'Training loss: 1.7022', '6.2651 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 773/1810', 'Training loss: 1.7019', '6.3211 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 774/1810', 'Training loss: 1.7015', '6.2022 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 775/1810', 'Training loss: 1.7010', '6.2680 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 776/1810', 'Training loss: 1.7002', '6.2189 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 777/1810', 'Training loss: 1.7002', '6.3472 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 778/1810', 'Training loss: 1.6996', '6.3430 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 779/1810', 'Training loss: 1.6991', '6.1814 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 780/1810', 'Training loss: 1.6989', '6.2105 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 781/1810', 'Training loss: 1.6984', '6.4129 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 782/1810', 'Training loss: 1.6981', '6.2615 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 783/1810', 'Training loss: 1.6978', '6.3833 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 784/1810', 'Training loss: 1.6975', '6.2908 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 785/1810', 'Training loss: 1.6974', '6.2511 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 786/1810', 'Training loss: 1.6969', '6.2252 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 787/1810', 'Training loss: 1.6967', '6.2330 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 788/1810', 'Training loss: 1.6963', '6.3144 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 789/1810', 'Training loss: 1.6966', '6.2352 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 790/1810', 'Training loss: 1.6970', '6.2547 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 791/1810', 'Training loss: 1.6964', '6.2607 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 792/1810', 'Training loss: 1.6964', '6.1636 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 793/1810', 'Training loss: 1.6955', '6.2524 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 794/1810', 'Training loss: 1.6953', '6.5701 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 795/1810', 'Training loss: 1.6950', '6.2802 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 796/1810', 'Training loss: 1.6945', '6.2966 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 797/1810', 'Training loss: 1.6944', '6.2172 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 798/1810', 'Training loss: 1.6946', '6.2394 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 799/1810', 'Training loss: 1.6945', '6.2434 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 800/1810', 'Training loss: 1.6941', '6.4821 sec/batch')\n",
      "('Validation loss:', 1.5565574, 'Saving checkpoint!')\n",
      "('Epoch 5/10 ', 'Iteration 801/1810', 'Training loss: 1.6951', '6.3191 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 802/1810', 'Training loss: 1.6946', '6.2931 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 803/1810', 'Training loss: 1.6940', '6.2194 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 804/1810', 'Training loss: 1.6939', '6.1574 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 805/1810', 'Training loss: 1.6936', '6.2326 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 806/1810', 'Training loss: 1.6931', '6.2833 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 807/1810', 'Training loss: 1.6927', '6.1525 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 808/1810', 'Training loss: 1.6923', '6.2341 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 809/1810', 'Training loss: 1.6917', '6.3152 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 810/1810', 'Training loss: 1.6909', '6.2535 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 811/1810', 'Training loss: 1.6902', '6.1920 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 812/1810', 'Training loss: 1.6898', '6.3586 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 813/1810', 'Training loss: 1.6894', '6.2729 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 814/1810', 'Training loss: 1.6889', '6.2503 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 815/1810', 'Training loss: 1.6883', '6.1489 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 816/1810', 'Training loss: 1.6873', '6.2464 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 817/1810', 'Training loss: 1.6871', '6.2831 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 818/1810', 'Training loss: 1.6866', '6.2477 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 819/1810', 'Training loss: 1.6861', '6.1441 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 820/1810', 'Training loss: 1.6857', '6.2360 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 821/1810', 'Training loss: 1.6852', '6.2708 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 822/1810', 'Training loss: 1.6848', '6.3145 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 823/1810', 'Training loss: 1.6841', '6.1453 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 824/1810', 'Training loss: 1.6838', '6.2226 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 825/1810', 'Training loss: 1.6829', '6.3146 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 826/1810', 'Training loss: 1.6822', '6.2550 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 827/1810', 'Training loss: 1.6814', '6.2567 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 828/1810', 'Training loss: 1.6806', '6.3220 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 829/1810', 'Training loss: 1.6804', '6.2682 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 830/1810', 'Training loss: 1.6799', '6.2763 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 831/1810', 'Training loss: 1.6793', '6.2038 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 832/1810', 'Training loss: 1.6791', '6.4795 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 833/1810', 'Training loss: 1.6785', '6.4227 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 834/1810', 'Training loss: 1.6780', '6.2220 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 835/1810', 'Training loss: 1.6775', '6.1923 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 836/1810', 'Training loss: 1.6774', '6.1960 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 837/1810', 'Training loss: 1.6773', '6.2962 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 838/1810', 'Training loss: 1.6770', '6.3872 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 839/1810', 'Training loss: 1.6766', '6.2034 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 840/1810', 'Training loss: 1.6760', '6.1824 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 841/1810', 'Training loss: 1.6755', '6.2675 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 842/1810', 'Training loss: 1.6750', '6.2822 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 843/1810', 'Training loss: 1.6746', '6.2365 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 844/1810', 'Training loss: 1.6740', '6.2620 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 845/1810', 'Training loss: 1.6735', '6.2301 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 846/1810', 'Training loss: 1.6732', '7.1961 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 847/1810', 'Training loss: 1.6727', '7.6585 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 848/1810', 'Training loss: 1.6726', '7.7071 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 849/1810', 'Training loss: 1.6720', '8.4411 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 850/1810', 'Training loss: 1.6715', '8.7800 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 851/1810', 'Training loss: 1.6708', '8.1781 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 852/1810', 'Training loss: 1.6705', '7.6370 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 853/1810', 'Training loss: 1.6701', '8.2507 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 854/1810', 'Training loss: 1.6698', '7.4196 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 855/1810', 'Training loss: 1.6693', '8.6379 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 856/1810', 'Training loss: 1.6690', '7.5606 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 857/1810', 'Training loss: 1.6687', '7.8828 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 858/1810', 'Training loss: 1.6682', '8.7014 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 859/1810', 'Training loss: 1.6678', '8.6469 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 860/1810', 'Training loss: 1.6676', '9.2325 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 861/1810', 'Training loss: 1.6671', '9.2029 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 862/1810', 'Training loss: 1.6669', '9.4152 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 863/1810', 'Training loss: 1.6667', '9.5430 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 864/1810', 'Training loss: 1.6663', '10.1358 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 865/1810', 'Training loss: 1.6660', '10.3505 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 866/1810', 'Training loss: 1.6658', '9.7255 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 867/1810', 'Training loss: 1.6655', '9.4805 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 868/1810', 'Training loss: 1.6653', '9.7281 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 869/1810', 'Training loss: 1.6651', '11.5701 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 870/1810', 'Training loss: 1.6647', '11.1168 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 871/1810', 'Training loss: 1.6645', '10.6153 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 872/1810', 'Training loss: 1.6642', '10.9642 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 873/1810', 'Training loss: 1.6641', '11.7825 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 874/1810', 'Training loss: 1.6636', '9.0383 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 875/1810', 'Training loss: 1.6634', '10.9664 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 876/1810', 'Training loss: 1.6632', '11.5703 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 877/1810', 'Training loss: 1.6632', '13.3730 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 878/1810', 'Training loss: 1.6629', '10.4356 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 879/1810', 'Training loss: 1.6624', '10.9172 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 880/1810', 'Training loss: 1.6622', '10.7601 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 881/1810', 'Training loss: 1.6617', '8.9502 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 882/1810', 'Training loss: 1.6615', '9.7415 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 883/1810', 'Training loss: 1.6612', '9.8657 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 884/1810', 'Training loss: 1.6610', '10.1635 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 885/1810', 'Training loss: 1.6608', '11.0449 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 886/1810', 'Training loss: 1.6604', '9.5447 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 887/1810', 'Training loss: 1.6599', '10.3701 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 888/1810', 'Training loss: 1.6595', '14.6099 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 889/1810', 'Training loss: 1.6592', '12.1483 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 890/1810', 'Training loss: 1.6590', '9.6738 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 891/1810', 'Training loss: 1.6589', '10.5582 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 892/1810', 'Training loss: 1.6585', '10.8851 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 893/1810', 'Training loss: 1.6582', '10.4935 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 894/1810', 'Training loss: 1.6581', '10.4787 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 895/1810', 'Training loss: 1.6580', '10.5907 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 896/1810', 'Training loss: 1.6580', '10.9204 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 897/1810', 'Training loss: 1.6578', '11.5519 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 898/1810', 'Training loss: 1.6574', '9.5656 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 899/1810', 'Training loss: 1.6571', '9.6527 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 900/1810', 'Training loss: 1.6566', '9.5872 sec/batch')\n",
      "('Validation loss:', 1.4920191, 'Saving checkpoint!')\n",
      "('Epoch 5/10 ', 'Iteration 901/1810', 'Training loss: 1.6568', '11.0616 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 902/1810', 'Training loss: 1.6568', '11.3614 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 903/1810', 'Training loss: 1.6566', '10.2585 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 904/1810', 'Training loss: 1.6564', '9.8845 sec/batch')\n",
      "('Epoch 5/10 ', 'Iteration 905/1810', 'Training loss: 1.6560', '10.0281 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 906/1810', 'Training loss: 1.7008', '10.7441 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 907/1810', 'Training loss: 1.6572', '11.0860 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 908/1810', 'Training loss: 1.6382', '10.1524 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 909/1810', 'Training loss: 1.6327', '11.6098 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 910/1810', 'Training loss: 1.6302', '12.2024 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 911/1810', 'Training loss: 1.6274', '9.7662 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 912/1810', 'Training loss: 1.6204', '10.6727 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 913/1810', 'Training loss: 1.6176', '10.4795 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 914/1810', 'Training loss: 1.6150', '10.5763 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 915/1810', 'Training loss: 1.6104', '9.9038 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 916/1810', 'Training loss: 1.6132', '9.9620 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 917/1810', 'Training loss: 1.6105', '10.6615 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 918/1810', 'Training loss: 1.6093', '9.8583 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 919/1810', 'Training loss: 1.6078', '10.2822 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 920/1810', 'Training loss: 1.6101', '10.3909 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 921/1810', 'Training loss: 1.6059', '11.2404 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 922/1810', 'Training loss: 1.6043', '9.5493 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 923/1810', 'Training loss: 1.6039', '10.5523 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 924/1810', 'Training loss: 1.6038', '10.4729 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 925/1810', 'Training loss: 1.6049', '10.3691 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 926/1810', 'Training loss: 1.6043', '10.9337 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 927/1810', 'Training loss: 1.6028', '10.6003 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 928/1810', 'Training loss: 1.6018', '11.2359 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 929/1810', 'Training loss: 1.6019', '11.5064 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 930/1810', 'Training loss: 1.6008', '10.3017 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 931/1810', 'Training loss: 1.6005', '9.6885 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 932/1810', 'Training loss: 1.6002', '11.1815 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 933/1810', 'Training loss: 1.5979', '12.6836 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 934/1810', 'Training loss: 1.5973', '12.7471 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 935/1810', 'Training loss: 1.5967', '10.4515 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 936/1810', 'Training loss: 1.5963', '10.6865 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 937/1810', 'Training loss: 1.5962', '10.3800 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 938/1810', 'Training loss: 1.5961', '10.4539 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 939/1810', 'Training loss: 1.5955', '11.0775 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 940/1810', 'Training loss: 1.5953', '16.2990 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 941/1810', 'Training loss: 1.5960', '13.5161 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 942/1810', 'Training loss: 1.5957', '10.4500 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 943/1810', 'Training loss: 1.5941', '11.5831 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 944/1810', 'Training loss: 1.5933', '10.9349 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 945/1810', 'Training loss: 1.5919', '10.7335 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 946/1810', 'Training loss: 1.5906', '10.1944 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 947/1810', 'Training loss: 1.5899', '10.7399 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 948/1810', 'Training loss: 1.5891', '10.4175 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 949/1810', 'Training loss: 1.5884', '10.5707 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 950/1810', 'Training loss: 1.5882', '11.3012 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 951/1810', 'Training loss: 1.5877', '11.3086 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 952/1810', 'Training loss: 1.5867', '10.3231 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 953/1810', 'Training loss: 1.5859', '9.7988 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 954/1810', 'Training loss: 1.5857', '10.4285 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 955/1810', 'Training loss: 1.5857', '10.7287 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 956/1810', 'Training loss: 1.5854', '10.4126 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 957/1810', 'Training loss: 1.5847', '9.1278 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 958/1810', 'Training loss: 1.5849', '10.6083 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 959/1810', 'Training loss: 1.5847', '10.8028 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 960/1810', 'Training loss: 1.5844', '9.9168 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 961/1810', 'Training loss: 1.5847', '10.2069 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 962/1810', 'Training loss: 1.5844', '10.1022 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 963/1810', 'Training loss: 1.5842', '9.9037 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 964/1810', 'Training loss: 1.5840', '9.9429 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 965/1810', 'Training loss: 1.5838', '10.0242 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 966/1810', 'Training loss: 1.5839', '10.3718 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 967/1810', 'Training loss: 1.5834', '9.2526 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 968/1810', 'Training loss: 1.5835', '10.2465 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 969/1810', 'Training loss: 1.5832', '10.6589 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 970/1810', 'Training loss: 1.5837', '9.5998 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 971/1810', 'Training loss: 1.5843', '10.6848 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 972/1810', 'Training loss: 1.5837', '10.2680 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 973/1810', 'Training loss: 1.5838', '10.5290 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 974/1810', 'Training loss: 1.5832', '10.9838 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 975/1810', 'Training loss: 1.5832', '9.1749 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 976/1810', 'Training loss: 1.5831', '10.2604 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 977/1810', 'Training loss: 1.5828', '10.2521 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 978/1810', 'Training loss: 1.5828', '10.5793 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 979/1810', 'Training loss: 1.5830', '10.0094 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 980/1810', 'Training loss: 1.5831', '9.5988 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 981/1810', 'Training loss: 1.5829', '10.4045 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 982/1810', 'Training loss: 1.5828', '10.5728 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 983/1810', 'Training loss: 1.5825', '9.9451 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 984/1810', 'Training loss: 1.5819', '9.7008 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 985/1810', 'Training loss: 1.5819', '10.0765 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 986/1810', 'Training loss: 1.5817', '9.8599 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 987/1810', 'Training loss: 1.5812', '9.4166 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 988/1810', 'Training loss: 1.5809', '10.9789 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 989/1810', 'Training loss: 1.5804', '10.8070 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 990/1810', 'Training loss: 1.5801', '10.3866 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 991/1810', 'Training loss: 1.5794', '9.9189 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 992/1810', 'Training loss: 1.5786', '9.3161 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 993/1810', 'Training loss: 1.5783', '10.5090 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 994/1810', 'Training loss: 1.5780', '11.0789 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 995/1810', 'Training loss: 1.5776', '8.9681 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 996/1810', 'Training loss: 1.5771', '10.4643 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 997/1810', 'Training loss: 1.5763', '10.9704 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 998/1810', 'Training loss: 1.5761', '10.7510 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 999/1810', 'Training loss: 1.5756', '9.4159 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1000/1810', 'Training loss: 1.5754', '9.7745 sec/batch')\n",
      "('Validation loss:', 1.4360032, 'Saving checkpoint!')\n",
      "('Epoch 6/10 ', 'Iteration 1001/1810', 'Training loss: 1.5763', '9.3541 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1002/1810', 'Training loss: 1.5760', '10.4302 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1003/1810', 'Training loss: 1.5758', '9.9917 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1004/1810', 'Training loss: 1.5754', '10.4690 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1005/1810', 'Training loss: 1.5752', '9.2793 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1006/1810', 'Training loss: 1.5744', '10.2525 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1007/1810', 'Training loss: 1.5738', '10.6180 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1008/1810', 'Training loss: 1.5731', '10.2246 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1009/1810', 'Training loss: 1.5724', '10.1028 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1010/1810', 'Training loss: 1.5723', '10.0554 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1011/1810', 'Training loss: 1.5719', '10.4968 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1012/1810', 'Training loss: 1.5714', '10.5738 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1013/1810', 'Training loss: 1.5713', '10.7065 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1014/1810', 'Training loss: 1.5708', '10.1478 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1015/1810', 'Training loss: 1.5704', '11.0599 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1016/1810', 'Training loss: 1.5702', '10.1463 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1017/1810', 'Training loss: 1.5702', '10.1289 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1018/1810', 'Training loss: 1.5702', '10.1455 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1019/1810', 'Training loss: 1.5701', '9.6535 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1020/1810', 'Training loss: 1.5698', '9.4020 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1021/1810', 'Training loss: 1.5693', '9.9559 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1022/1810', 'Training loss: 1.5690', '11.0200 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1023/1810', 'Training loss: 1.5686', '10.1341 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1024/1810', 'Training loss: 1.5683', '10.2639 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1025/1810', 'Training loss: 1.5678', '9.7871 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1026/1810', 'Training loss: 1.5675', '11.3328 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1027/1810', 'Training loss: 1.5673', '10.0834 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1028/1810', 'Training loss: 1.5670', '11.1339 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1029/1810', 'Training loss: 1.5670', '9.3517 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1030/1810', 'Training loss: 1.5665', '7.7736 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1031/1810', 'Training loss: 1.5659', '7.4994 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1032/1810', 'Training loss: 1.5654', '7.8352 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1033/1810', 'Training loss: 1.5652', '6.7476 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1034/1810', 'Training loss: 1.5649', '8.3866 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1035/1810', 'Training loss: 1.5648', '8.0463 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1036/1810', 'Training loss: 1.5645', '8.3942 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1037/1810', 'Training loss: 1.5643', '7.2264 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1038/1810', 'Training loss: 1.5641', '7.1695 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1039/1810', 'Training loss: 1.5636', '6.8029 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1040/1810', 'Training loss: 1.5632', '6.8183 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1041/1810', 'Training loss: 1.5632', '6.8083 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1042/1810', 'Training loss: 1.5628', '6.6554 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1043/1810', 'Training loss: 1.5627', '6.6878 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1044/1810', 'Training loss: 1.5627', '6.6854 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1045/1810', 'Training loss: 1.5624', '6.7129 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1046/1810', 'Training loss: 1.5622', '6.7177 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1047/1810', 'Training loss: 1.5622', '6.7659 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1048/1810', 'Training loss: 1.5620', '6.6147 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1049/1810', 'Training loss: 1.5620', '6.7179 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1050/1810', 'Training loss: 1.5619', '6.8099 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1051/1810', 'Training loss: 1.5617', '6.8104 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1052/1810', 'Training loss: 1.5615', '6.8434 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1053/1810', 'Training loss: 1.5614', '6.8414 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1054/1810', 'Training loss: 1.5614', '6.7811 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1055/1810', 'Training loss: 1.5610', '6.6327 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1056/1810', 'Training loss: 1.5610', '6.8122 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1057/1810', 'Training loss: 1.5610', '6.7185 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1058/1810', 'Training loss: 1.5610', '6.6266 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1059/1810', 'Training loss: 1.5609', '6.7497 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1060/1810', 'Training loss: 1.5605', '6.7404 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1061/1810', 'Training loss: 1.5604', '6.7559 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1062/1810', 'Training loss: 1.5600', '6.6269 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1063/1810', 'Training loss: 1.5599', '6.8131 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1064/1810', 'Training loss: 1.5597', '6.7595 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1065/1810', 'Training loss: 1.5596', '7.1069 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1066/1810', 'Training loss: 1.5595', '10.2968 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1067/1810', 'Training loss: 1.5592', '8.0910 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1068/1810', 'Training loss: 1.5588', '10.0749 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1069/1810', 'Training loss: 1.5585', '7.6047 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1070/1810', 'Training loss: 1.5582', '6.8395 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1071/1810', 'Training loss: 1.5581', '6.8283 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1072/1810', 'Training loss: 1.5580', '6.6270 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1073/1810', 'Training loss: 1.5578', '6.4791 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1074/1810', 'Training loss: 1.5576', '7.0315 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1075/1810', 'Training loss: 1.5576', '6.5371 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1076/1810', 'Training loss: 1.5576', '6.4932 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1077/1810', 'Training loss: 1.5577', '6.3287 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1078/1810', 'Training loss: 1.5576', '6.4037 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1079/1810', 'Training loss: 1.5573', '6.5490 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1080/1810', 'Training loss: 1.5571', '6.4272 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1081/1810', 'Training loss: 1.5567', '6.3681 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1082/1810', 'Training loss: 1.5564', '6.4566 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1083/1810', 'Training loss: 1.5564', '6.5138 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1084/1810', 'Training loss: 1.5562', '6.3988 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1085/1810', 'Training loss: 1.5562', '6.3784 sec/batch')\n",
      "('Epoch 6/10 ', 'Iteration 1086/1810', 'Training loss: 1.5559', '6.3283 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1087/1810', 'Training loss: 1.6544', '6.5819 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1088/1810', 'Training loss: 1.5916', '6.3267 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1089/1810', 'Training loss: 1.5667', '6.5013 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1090/1810', 'Training loss: 1.5575', '6.3526 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1091/1810', 'Training loss: 1.5528', '6.9596 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1092/1810', 'Training loss: 1.5499', '8.0244 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1093/1810', 'Training loss: 1.5417', '8.3071 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1094/1810', 'Training loss: 1.5374', '7.8901 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1095/1810', 'Training loss: 1.5345', '7.2778 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1096/1810', 'Training loss: 1.5295', '6.3981 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1097/1810', 'Training loss: 1.5319', '6.4042 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1098/1810', 'Training loss: 1.5284', '6.3740 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1099/1810', 'Training loss: 1.5268', '6.5033 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1100/1810', 'Training loss: 1.5254', '6.3708 sec/batch')\n",
      "('Validation loss:', 1.396171, 'Saving checkpoint!')\n",
      "('Epoch 7/10 ', 'Iteration 1101/1810', 'Training loss: 1.5340', '7.9944 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1102/1810', 'Training loss: 1.5301', '6.8890 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1103/1810', 'Training loss: 1.5282', '6.7396 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1104/1810', 'Training loss: 1.5273', '6.7149 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1105/1810', 'Training loss: 1.5269', '8.6284 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1106/1810', 'Training loss: 1.5278', '9.4496 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1107/1810', 'Training loss: 1.5271', '9.5131 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1108/1810', 'Training loss: 1.5262', '9.7066 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1109/1810', 'Training loss: 1.5252', '11.5082 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1110/1810', 'Training loss: 1.5251', '14.2246 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1111/1810', 'Training loss: 1.5242', '10.5809 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1112/1810', 'Training loss: 1.5242', '10.1418 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1113/1810', 'Training loss: 1.5239', '9.0994 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1114/1810', 'Training loss: 1.5217', '9.5249 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1115/1810', 'Training loss: 1.5211', '10.9620 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1116/1810', 'Training loss: 1.5206', '10.7740 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1117/1810', 'Training loss: 1.5197', '10.1844 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1118/1810', 'Training loss: 1.5195', '11.1797 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1119/1810', 'Training loss: 1.5191', '12.2794 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1120/1810', 'Training loss: 1.5184', '12.4963 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1121/1810', 'Training loss: 1.5178', '10.3192 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1122/1810', 'Training loss: 1.5183', '11.4295 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1123/1810', 'Training loss: 1.5180', '10.2144 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1124/1810', 'Training loss: 1.5166', '10.2315 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1125/1810', 'Training loss: 1.5157', '11.1385 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1126/1810', 'Training loss: 1.5142', '10.7756 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1127/1810', 'Training loss: 1.5131', '10.7725 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1128/1810', 'Training loss: 1.5123', '13.4113 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1129/1810', 'Training loss: 1.5114', '11.8945 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1130/1810', 'Training loss: 1.5107', '10.7702 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1131/1810', 'Training loss: 1.5105', '11.6239 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1132/1810', 'Training loss: 1.5099', '10.9517 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1133/1810', 'Training loss: 1.5089', '10.1660 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1134/1810', 'Training loss: 1.5082', '10.6290 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1135/1810', 'Training loss: 1.5079', '10.2144 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1136/1810', 'Training loss: 1.5077', '10.7444 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1137/1810', 'Training loss: 1.5073', '10.3626 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1138/1810', 'Training loss: 1.5066', '10.0483 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1139/1810', 'Training loss: 1.5068', '10.1546 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1140/1810', 'Training loss: 1.5067', '10.1723 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1141/1810', 'Training loss: 1.5064', '11.7066 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1142/1810', 'Training loss: 1.5067', '9.9601 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1143/1810', 'Training loss: 1.5066', '10.5275 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1144/1810', 'Training loss: 1.5064', '10.8241 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1145/1810', 'Training loss: 1.5063', '10.6908 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1146/1810', 'Training loss: 1.5060', '10.7511 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1147/1810', 'Training loss: 1.5062', '10.3340 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1148/1810', 'Training loss: 1.5057', '10.4503 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1149/1810', 'Training loss: 1.5057', '10.7031 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1150/1810', 'Training loss: 1.5056', '10.2062 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1151/1810', 'Training loss: 1.5062', '9.9328 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1152/1810', 'Training loss: 1.5068', '9.8455 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1153/1810', 'Training loss: 1.5060', '10.7433 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1154/1810', 'Training loss: 1.5061', '10.5887 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1155/1810', 'Training loss: 1.5054', '11.0265 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1156/1810', 'Training loss: 1.5053', '10.7996 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1157/1810', 'Training loss: 1.5051', '10.8748 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1158/1810', 'Training loss: 1.5049', '10.8107 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1159/1810', 'Training loss: 1.5049', '11.2509 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1160/1810', 'Training loss: 1.5052', '10.7398 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1161/1810', 'Training loss: 1.5054', '10.4747 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1162/1810', 'Training loss: 1.5051', '10.7082 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1163/1810', 'Training loss: 1.5051', '10.5322 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1164/1810', 'Training loss: 1.5047', '10.6326 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1165/1810', 'Training loss: 1.5041', '10.4114 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1166/1810', 'Training loss: 1.5041', '11.0346 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1167/1810', 'Training loss: 1.5040', '9.4112 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1168/1810', 'Training loss: 1.5036', '9.7671 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1169/1810', 'Training loss: 1.5034', '10.1615 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1170/1810', 'Training loss: 1.5030', '10.1824 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1171/1810', 'Training loss: 1.5026', '10.1917 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1172/1810', 'Training loss: 1.5020', '10.2244 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1173/1810', 'Training loss: 1.5015', '10.2837 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1174/1810', 'Training loss: 1.5013', '10.0071 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1175/1810', 'Training loss: 1.5012', '11.3176 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1176/1810', 'Training loss: 1.5009', '9.4546 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1177/1810', 'Training loss: 1.5005', '10.6626 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1178/1810', 'Training loss: 1.4997', '11.1385 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1179/1810', 'Training loss: 1.4997', '10.8237 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1180/1810', 'Training loss: 1.4992', '10.1876 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1181/1810', 'Training loss: 1.4990', '10.1402 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1182/1810', 'Training loss: 1.4988', '10.1558 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1183/1810', 'Training loss: 1.4985', '10.2054 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1184/1810', 'Training loss: 1.4983', '11.0445 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1185/1810', 'Training loss: 1.4979', '8.7109 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1186/1810', 'Training loss: 1.4978', '9.9106 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1187/1810', 'Training loss: 1.4971', '9.9923 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1188/1810', 'Training loss: 1.4965', '9.9815 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1189/1810', 'Training loss: 1.4959', '9.9315 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1190/1810', 'Training loss: 1.4953', '10.9196 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1191/1810', 'Training loss: 1.4952', '10.5616 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1192/1810', 'Training loss: 1.4947', '10.3436 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1193/1810', 'Training loss: 1.4944', '10.9387 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1194/1810', 'Training loss: 1.4944', '9.5370 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1195/1810', 'Training loss: 1.4940', '10.4366 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1196/1810', 'Training loss: 1.4938', '10.4377 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1197/1810', 'Training loss: 1.4934', '10.1177 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1198/1810', 'Training loss: 1.4934', '10.1713 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1199/1810', 'Training loss: 1.4934', '9.9606 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1200/1810', 'Training loss: 1.4932', '9.9950 sec/batch')\n",
      "('Validation loss:', 1.3664099, 'Saving checkpoint!')\n",
      "('Epoch 7/10 ', 'Iteration 1201/1810', 'Training loss: 1.4940', '10.2969 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1202/1810', 'Training loss: 1.4936', '10.6894 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1203/1810', 'Training loss: 1.4934', '9.4799 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1204/1810', 'Training loss: 1.4930', '9.5645 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1205/1810', 'Training loss: 1.4928', '9.9275 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1206/1810', 'Training loss: 1.4926', '10.2818 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1207/1810', 'Training loss: 1.4922', '10.3187 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1208/1810', 'Training loss: 1.4920', '10.0364 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1209/1810', 'Training loss: 1.4917', '9.7523 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1210/1810', 'Training loss: 1.4917', '10.7003 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1211/1810', 'Training loss: 1.4912', '10.0482 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1212/1810', 'Training loss: 1.4907', '9.4551 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1213/1810', 'Training loss: 1.4902', '10.2692 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1214/1810', 'Training loss: 1.4901', '10.8060 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1215/1810', 'Training loss: 1.4899', '10.7076 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1216/1810', 'Training loss: 1.4897', '10.6310 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1217/1810', 'Training loss: 1.4894', '10.1852 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1218/1810', 'Training loss: 1.4892', '11.3757 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1219/1810', 'Training loss: 1.4890', '13.0924 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1220/1810', 'Training loss: 1.4886', '10.1928 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1221/1810', 'Training loss: 1.4883', '9.6405 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1222/1810', 'Training loss: 1.4883', '10.2529 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1223/1810', 'Training loss: 1.4880', '10.4276 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1224/1810', 'Training loss: 1.4879', '10.3121 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1225/1810', 'Training loss: 1.4879', '9.9424 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1226/1810', 'Training loss: 1.4877', '10.2113 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1227/1810', 'Training loss: 1.4876', '10.2874 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1228/1810', 'Training loss: 1.4874', '10.3133 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1229/1810', 'Training loss: 1.4873', '11.7614 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1230/1810', 'Training loss: 1.4873', '9.4518 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1231/1810', 'Training loss: 1.4873', '10.4384 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1232/1810', 'Training loss: 1.4871', '10.0287 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1233/1810', 'Training loss: 1.4870', '10.3635 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1234/1810', 'Training loss: 1.4868', '9.8889 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1235/1810', 'Training loss: 1.4869', '10.1222 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1236/1810', 'Training loss: 1.4866', '10.0343 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1237/1810', 'Training loss: 1.4866', '10.8245 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1238/1810', 'Training loss: 1.4866', '10.4960 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1239/1810', 'Training loss: 1.4867', '10.4022 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1240/1810', 'Training loss: 1.4866', '10.5781 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1241/1810', 'Training loss: 1.4862', '10.6488 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1242/1810', 'Training loss: 1.4862', '10.8957 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1243/1810', 'Training loss: 1.4858', '10.4133 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1244/1810', 'Training loss: 1.4858', '12.0647 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1245/1810', 'Training loss: 1.4856', '12.2951 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1246/1810', 'Training loss: 1.4856', '11.4402 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1247/1810', 'Training loss: 1.4855', '11.3695 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1248/1810', 'Training loss: 1.4853', '11.3549 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1249/1810', 'Training loss: 1.4850', '11.1398 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1250/1810', 'Training loss: 1.4847', '10.8190 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1251/1810', 'Training loss: 1.4846', '10.3073 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1252/1810', 'Training loss: 1.4846', '10.6447 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1253/1810', 'Training loss: 1.4847', '9.8548 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1254/1810', 'Training loss: 1.4845', '10.6654 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1255/1810', 'Training loss: 1.4842', '10.6772 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1256/1810', 'Training loss: 1.4844', '10.3729 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1257/1810', 'Training loss: 1.4844', '10.3034 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1258/1810', 'Training loss: 1.4846', '10.4579 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1259/1810', 'Training loss: 1.4845', '10.0780 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1260/1810', 'Training loss: 1.4842', '11.0518 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1261/1810', 'Training loss: 1.4841', '10.1281 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1262/1810', 'Training loss: 1.4838', '9.4922 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1263/1810', 'Training loss: 1.4836', '13.5071 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1264/1810', 'Training loss: 1.4836', '11.0740 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1265/1810', 'Training loss: 1.4834', '10.3492 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1266/1810', 'Training loss: 1.4835', '10.4061 sec/batch')\n",
      "('Epoch 7/10 ', 'Iteration 1267/1810', 'Training loss: 1.4832', '10.6173 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1268/1810', 'Training loss: 1.5773', '10.1433 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1269/1810', 'Training loss: 1.5207', '10.4464 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1270/1810', 'Training loss: 1.4980', '10.8228 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1271/1810', 'Training loss: 1.4905', '10.3665 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1272/1810', 'Training loss: 1.4879', '10.7231 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1273/1810', 'Training loss: 1.4839', '10.3778 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1274/1810', 'Training loss: 1.4777', '10.6025 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1275/1810', 'Training loss: 1.4747', '10.1768 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1276/1810', 'Training loss: 1.4711', '10.1490 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1277/1810', 'Training loss: 1.4661', '9.5281 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1278/1810', 'Training loss: 1.4679', '10.2578 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1279/1810', 'Training loss: 1.4645', '10.4670 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1280/1810', 'Training loss: 1.4630', '9.7866 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1281/1810', 'Training loss: 1.4615', '10.5298 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1282/1810', 'Training loss: 1.4632', '11.0153 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1283/1810', 'Training loss: 1.4600', '9.5581 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1284/1810', 'Training loss: 1.4577', '9.7289 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1285/1810', 'Training loss: 1.4582', '9.7463 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1286/1810', 'Training loss: 1.4588', '11.3120 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1287/1810', 'Training loss: 1.4600', '11.0378 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1288/1810', 'Training loss: 1.4593', '10.6978 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1289/1810', 'Training loss: 1.4583', '10.2178 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1290/1810', 'Training loss: 1.4576', '10.9052 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1291/1810', 'Training loss: 1.4577', '11.4157 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1292/1810', 'Training loss: 1.4570', '11.5032 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1293/1810', 'Training loss: 1.4569', '10.6195 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1294/1810', 'Training loss: 1.4566', '10.0966 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1295/1810', 'Training loss: 1.4545', '10.1480 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1296/1810', 'Training loss: 1.4542', '10.0989 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1297/1810', 'Training loss: 1.4539', '10.4708 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1298/1810', 'Training loss: 1.4533', '10.7407 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1299/1810', 'Training loss: 1.4535', '10.3173 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1300/1810', 'Training loss: 1.4533', '10.0744 sec/batch')\n",
      "('Validation loss:', 1.3296735, 'Saving checkpoint!')\n",
      "('Epoch 8/10 ', 'Iteration 1301/1810', 'Training loss: 1.4567', '10.7528 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1302/1810', 'Training loss: 1.4566', '11.7568 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1303/1810', 'Training loss: 1.4574', '10.8450 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1304/1810', 'Training loss: 1.4569', '10.3830 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1305/1810', 'Training loss: 1.4556', '10.5862 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1306/1810', 'Training loss: 1.4546', '10.5287 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1307/1810', 'Training loss: 1.4532', '10.2595 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1308/1810', 'Training loss: 1.4523', '10.2410 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1309/1810', 'Training loss: 1.4517', '10.3278 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1310/1810', 'Training loss: 1.4508', '10.0918 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1311/1810', 'Training loss: 1.4502', '9.7840 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1312/1810', 'Training loss: 1.4503', '9.8876 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1313/1810', 'Training loss: 1.4498', '10.2213 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1314/1810', 'Training loss: 1.4488', '11.0762 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1315/1810', 'Training loss: 1.4482', '10.5406 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1316/1810', 'Training loss: 1.4481', '11.9430 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1317/1810', 'Training loss: 1.4480', '11.0195 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1318/1810', 'Training loss: 1.4476', '10.6497 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1319/1810', 'Training loss: 1.4468', '10.7459 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1320/1810', 'Training loss: 1.4469', '10.5706 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1321/1810', 'Training loss: 1.4467', '10.2378 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1322/1810', 'Training loss: 1.4465', '10.5617 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1323/1810', 'Training loss: 1.4468', '11.2340 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1324/1810', 'Training loss: 1.4466', '10.7708 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1325/1810', 'Training loss: 1.4466', '10.9848 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1326/1810', 'Training loss: 1.4465', '10.7112 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1327/1810', 'Training loss: 1.4462', '11.9736 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1328/1810', 'Training loss: 1.4464', '10.8588 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1329/1810', 'Training loss: 1.4459', '10.1018 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1330/1810', 'Training loss: 1.4460', '10.5691 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1331/1810', 'Training loss: 1.4459', '11.7654 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1332/1810', 'Training loss: 1.4465', '11.0930 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1333/1810', 'Training loss: 1.4472', '12.5724 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1334/1810', 'Training loss: 1.4466', '11.0191 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1335/1810', 'Training loss: 1.4468', '11.0891 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1336/1810', 'Training loss: 1.4463', '7.9192 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1337/1810', 'Training loss: 1.4462', '7.4301 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1338/1810', 'Training loss: 1.4461', '7.3117 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1339/1810', 'Training loss: 1.4459', '7.3472 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1340/1810', 'Training loss: 1.4459', '6.6837 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1341/1810', 'Training loss: 1.4462', '6.4131 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1342/1810', 'Training loss: 1.4464', '6.4773 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1343/1810', 'Training loss: 1.4462', '6.4975 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1344/1810', 'Training loss: 1.4463', '6.2990 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1345/1810', 'Training loss: 1.4459', '6.4147 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1346/1810', 'Training loss: 1.4454', '7.5149 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1347/1810', 'Training loss: 1.4453', '7.9527 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1348/1810', 'Training loss: 1.4452', '8.9174 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1349/1810', 'Training loss: 1.4448', '6.9984 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1350/1810', 'Training loss: 1.4446', '6.6010 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1351/1810', 'Training loss: 1.4441', '7.0895 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1352/1810', 'Training loss: 1.4435', '6.5481 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1353/1810', 'Training loss: 1.4429', '6.6859 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1354/1810', 'Training loss: 1.4423', '6.4287 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1355/1810', 'Training loss: 1.4421', '6.3363 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1356/1810', 'Training loss: 1.4421', '6.3617 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1357/1810', 'Training loss: 1.4418', '6.4929 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1358/1810', 'Training loss: 1.4415', '6.4023 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1359/1810', 'Training loss: 1.4407', '6.4871 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1360/1810', 'Training loss: 1.4406', '6.3366 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1361/1810', 'Training loss: 1.4402', '6.5120 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1362/1810', 'Training loss: 1.4401', '6.4958 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1363/1810', 'Training loss: 1.4398', '6.4246 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1364/1810', 'Training loss: 1.4397', '6.5464 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1365/1810', 'Training loss: 1.4395', '7.5460 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1366/1810', 'Training loss: 1.4390', '6.3741 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1367/1810', 'Training loss: 1.4389', '6.4497 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1368/1810', 'Training loss: 1.4383', '6.4742 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1369/1810', 'Training loss: 1.4378', '6.4163 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1370/1810', 'Training loss: 1.4372', '6.3848 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1371/1810', 'Training loss: 1.4367', '6.4658 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1372/1810', 'Training loss: 1.4367', '6.6987 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1373/1810', 'Training loss: 1.4363', '6.4252 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1374/1810', 'Training loss: 1.4359', '6.3607 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1375/1810', 'Training loss: 1.4360', '6.5340 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1376/1810', 'Training loss: 1.4357', '6.5113 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1377/1810', 'Training loss: 1.4354', '6.3378 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1378/1810', 'Training loss: 1.4350', '6.4849 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1379/1810', 'Training loss: 1.4351', '6.4024 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1380/1810', 'Training loss: 1.4351', '6.4562 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1381/1810', 'Training loss: 1.4349', '6.4896 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1382/1810', 'Training loss: 1.4348', '6.3349 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1383/1810', 'Training loss: 1.4344', '6.4198 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1384/1810', 'Training loss: 1.4342', '6.3381 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1385/1810', 'Training loss: 1.4338', '6.3936 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1386/1810', 'Training loss: 1.4336', '6.4814 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1387/1810', 'Training loss: 1.4333', '6.4186 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1388/1810', 'Training loss: 1.4330', '6.4472 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1389/1810', 'Training loss: 1.4328', '6.5593 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1390/1810', 'Training loss: 1.4326', '6.7164 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1391/1810', 'Training loss: 1.4326', '6.5568 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1392/1810', 'Training loss: 1.4322', '6.4243 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1393/1810', 'Training loss: 1.4316', '6.3754 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1394/1810', 'Training loss: 1.4310', '6.4770 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1395/1810', 'Training loss: 1.4309', '6.3601 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1396/1810', 'Training loss: 1.4307', '6.3765 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1397/1810', 'Training loss: 1.4305', '6.5121 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1398/1810', 'Training loss: 1.4303', '6.4178 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1399/1810', 'Training loss: 1.4302', '6.4011 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1400/1810', 'Training loss: 1.4300', '6.5538 sec/batch')\n",
      "('Validation loss:', 1.3069074, 'Saving checkpoint!')\n",
      "('Epoch 8/10 ', 'Iteration 1401/1810', 'Training loss: 1.4305', '6.3752 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1402/1810', 'Training loss: 1.4302', '6.6884 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1403/1810', 'Training loss: 1.4303', '6.3821 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1404/1810', 'Training loss: 1.4301', '6.3570 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1405/1810', 'Training loss: 1.4300', '6.5796 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1406/1810', 'Training loss: 1.4301', '6.4907 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1407/1810', 'Training loss: 1.4299', '6.3374 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1408/1810', 'Training loss: 1.4298', '6.2976 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1409/1810', 'Training loss: 1.4297', '6.5243 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1410/1810', 'Training loss: 1.4296', '6.5048 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1411/1810', 'Training loss: 1.4296', '6.4401 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1412/1810', 'Training loss: 1.4296', '6.3126 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1413/1810', 'Training loss: 1.4295', '6.3502 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1414/1810', 'Training loss: 1.4295', '6.4707 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1415/1810', 'Training loss: 1.4294', '6.5149 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1416/1810', 'Training loss: 1.4295', '6.3936 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1417/1810', 'Training loss: 1.4292', '6.3330 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1418/1810', 'Training loss: 1.4292', '6.4871 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1419/1810', 'Training loss: 1.4292', '6.3022 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1420/1810', 'Training loss: 1.4293', '6.6008 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1421/1810', 'Training loss: 1.4293', '6.3902 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1422/1810', 'Training loss: 1.4289', '6.5205 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1423/1810', 'Training loss: 1.4288', '6.4464 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1424/1810', 'Training loss: 1.4285', '6.3705 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1425/1810', 'Training loss: 1.4286', '6.3889 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1426/1810', 'Training loss: 1.4284', '6.3996 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1427/1810', 'Training loss: 1.4284', '6.5193 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1428/1810', 'Training loss: 1.4284', '6.4728 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1429/1810', 'Training loss: 1.4283', '6.3753 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1430/1810', 'Training loss: 1.4280', '6.4314 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1431/1810', 'Training loss: 1.4277', '6.5361 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1432/1810', 'Training loss: 1.4275', '6.4537 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1433/1810', 'Training loss: 1.4276', '6.3796 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1434/1810', 'Training loss: 1.4275', '6.4393 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1435/1810', 'Training loss: 1.4274', '6.2862 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1436/1810', 'Training loss: 1.4272', '6.4481 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1437/1810', 'Training loss: 1.4274', '6.3862 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1438/1810', 'Training loss: 1.4274', '6.4122 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1439/1810', 'Training loss: 1.4277', '6.5314 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1440/1810', 'Training loss: 1.4277', '6.4757 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1441/1810', 'Training loss: 1.4275', '6.3374 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1442/1810', 'Training loss: 1.4274', '6.3490 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1443/1810', 'Training loss: 1.4272', '6.3914 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1444/1810', 'Training loss: 1.4269', '6.5087 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1445/1810', 'Training loss: 1.4269', '6.3995 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1446/1810', 'Training loss: 1.4269', '6.7532 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1447/1810', 'Training loss: 1.4269', '6.5683 sec/batch')\n",
      "('Epoch 8/10 ', 'Iteration 1448/1810', 'Training loss: 1.4267', '6.4856 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1449/1810', 'Training loss: 1.5451', '6.4370 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1450/1810', 'Training loss: 1.4768', '6.3969 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1451/1810', 'Training loss: 1.4530', '6.4863 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1452/1810', 'Training loss: 1.4460', '6.5458 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1453/1810', 'Training loss: 1.4415', '6.4293 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1454/1810', 'Training loss: 1.4374', '6.4278 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1455/1810', 'Training loss: 1.4304', '6.4449 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1456/1810', 'Training loss: 1.4276', '6.3949 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1457/1810', 'Training loss: 1.4252', '6.4067 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1458/1810', 'Training loss: 1.4196', '6.6050 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1459/1810', 'Training loss: 1.4212', '7.6781 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1460/1810', 'Training loss: 1.4175', '11.0023 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1461/1810', 'Training loss: 1.4159', '9.5345 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1462/1810', 'Training loss: 1.4150', '6.8370 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1463/1810', 'Training loss: 1.4168', '6.6181 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1464/1810', 'Training loss: 1.4132', '6.5452 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1465/1810', 'Training loss: 1.4109', '6.4964 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1466/1810', 'Training loss: 1.4107', '6.5403 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1467/1810', 'Training loss: 1.4110', '6.5489 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1468/1810', 'Training loss: 1.4125', '6.3868 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1469/1810', 'Training loss: 1.4114', '6.4760 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1470/1810', 'Training loss: 1.4107', '6.4036 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1471/1810', 'Training loss: 1.4097', '6.3785 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1472/1810', 'Training loss: 1.4098', '7.0080 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1473/1810', 'Training loss: 1.4091', '6.9984 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1474/1810', 'Training loss: 1.4091', '6.3476 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1475/1810', 'Training loss: 1.4091', '6.6275 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1476/1810', 'Training loss: 1.4068', '6.3941 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1477/1810', 'Training loss: 1.4065', '6.5514 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1478/1810', 'Training loss: 1.4057', '6.3406 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1479/1810', 'Training loss: 1.4052', '6.4084 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1480/1810', 'Training loss: 1.4055', '6.4676 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1481/1810', 'Training loss: 1.4053', '6.3042 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1482/1810', 'Training loss: 1.4050', '6.5331 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1483/1810', 'Training loss: 1.4046', '7.7987 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1484/1810', 'Training loss: 1.4052', '6.4541 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1485/1810', 'Training loss: 1.4045', '6.3877 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1486/1810', 'Training loss: 1.4034', '6.4622 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1487/1810', 'Training loss: 1.4023', '6.5368 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1488/1810', 'Training loss: 1.4008', '6.3500 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1489/1810', 'Training loss: 1.3998', '6.4406 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1490/1810', 'Training loss: 1.3994', '6.3469 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1491/1810', 'Training loss: 1.3987', '6.4554 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1492/1810', 'Training loss: 1.3979', '6.4778 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1493/1810', 'Training loss: 1.3981', '6.5914 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1494/1810', 'Training loss: 1.3975', '6.4153 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1495/1810', 'Training loss: 1.3965', '6.7647 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1496/1810', 'Training loss: 1.3961', '6.4936 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1497/1810', 'Training loss: 1.3959', '6.4143 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1498/1810', 'Training loss: 1.3957', '6.5117 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1499/1810', 'Training loss: 1.3955', '6.3862 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1500/1810', 'Training loss: 1.3950', '6.4288 sec/batch')\n",
      "('Validation loss:', 1.2814722, 'Saving checkpoint!')\n",
      "('Epoch 9/10 ', 'Iteration 1501/1810', 'Training loss: 1.3977', '6.4238 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1502/1810', 'Training loss: 1.3977', '6.3506 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1503/1810', 'Training loss: 1.3976', '6.5356 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1504/1810', 'Training loss: 1.3979', '6.5743 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1505/1810', 'Training loss: 1.3978', '6.3039 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1506/1810', 'Training loss: 1.3976', '6.5068 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1507/1810', 'Training loss: 1.3974', '6.5231 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1508/1810', 'Training loss: 1.3971', '6.2901 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1509/1810', 'Training loss: 1.3975', '6.5126 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1510/1810', 'Training loss: 1.3971', '6.4633 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1511/1810', 'Training loss: 1.3972', '6.4807 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1512/1810', 'Training loss: 1.3971', '6.4617 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1513/1810', 'Training loss: 1.3976', '6.4777 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1514/1810', 'Training loss: 1.3984', '6.4093 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1515/1810', 'Training loss: 1.3978', '6.4783 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1516/1810', 'Training loss: 1.3980', '6.4616 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1517/1810', 'Training loss: 1.3975', '6.6310 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1518/1810', 'Training loss: 1.3974', '6.5040 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1519/1810', 'Training loss: 1.3972', '6.3740 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1520/1810', 'Training loss: 1.3972', '6.4940 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1521/1810', 'Training loss: 1.3972', '6.4233 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1522/1810', 'Training loss: 1.3975', '6.3858 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1523/1810', 'Training loss: 1.3975', '6.6308 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1524/1810', 'Training loss: 1.3973', '6.4657 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1525/1810', 'Training loss: 1.3974', '6.4948 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1526/1810', 'Training loss: 1.3970', '6.3668 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1527/1810', 'Training loss: 1.3967', '6.4502 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1528/1810', 'Training loss: 1.3966', '6.4173 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1529/1810', 'Training loss: 1.3965', '6.4293 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1530/1810', 'Training loss: 1.3961', '6.4493 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1531/1810', 'Training loss: 1.3960', '6.3360 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1532/1810', 'Training loss: 1.3956', '6.4332 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1533/1810', 'Training loss: 1.3952', '6.4096 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1534/1810', 'Training loss: 1.3947', '6.3936 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1535/1810', 'Training loss: 1.3941', '6.4482 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1536/1810', 'Training loss: 1.3941', '6.4065 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1537/1810', 'Training loss: 1.3940', '6.4752 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1538/1810', 'Training loss: 1.3940', '6.3758 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1539/1810', 'Training loss: 1.3936', '6.6720 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1540/1810', 'Training loss: 1.3929', '6.5886 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1541/1810', 'Training loss: 1.3930', '6.5624 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1542/1810', 'Training loss: 1.3927', '6.4746 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1543/1810', 'Training loss: 1.3926', '6.3901 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1544/1810', 'Training loss: 1.3923', '6.4808 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1545/1810', 'Training loss: 1.3922', '6.4539 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1546/1810', 'Training loss: 1.3921', '6.4675 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1547/1810', 'Training loss: 1.3917', '6.3983 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1548/1810', 'Training loss: 1.3916', '6.4693 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1549/1810', 'Training loss: 1.3910', '6.4229 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1550/1810', 'Training loss: 1.3905', '6.4647 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1551/1810', 'Training loss: 1.3900', '6.3057 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1552/1810', 'Training loss: 1.3895', '6.4843 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1553/1810', 'Training loss: 1.3894', '6.5284 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1554/1810', 'Training loss: 1.3889', '6.3799 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1555/1810', 'Training loss: 1.3886', '6.3920 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1556/1810', 'Training loss: 1.3888', '6.3963 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1557/1810', 'Training loss: 1.3886', '6.5179 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1558/1810', 'Training loss: 1.3884', '6.5466 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1559/1810', 'Training loss: 1.3880', '6.3515 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1560/1810', 'Training loss: 1.3881', '6.5350 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1561/1810', 'Training loss: 1.3881', '6.3348 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1562/1810', 'Training loss: 1.3880', '6.4497 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1563/1810', 'Training loss: 1.3879', '6.4200 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1564/1810', 'Training loss: 1.3876', '6.4150 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1565/1810', 'Training loss: 1.3875', '6.3379 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1566/1810', 'Training loss: 1.3871', '6.4188 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1567/1810', 'Training loss: 1.3869', '6.6009 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1568/1810', 'Training loss: 1.3867', '6.3647 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1569/1810', 'Training loss: 1.3863', '6.3655 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1570/1810', 'Training loss: 1.3863', '6.2880 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1571/1810', 'Training loss: 1.3861', '6.4496 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1572/1810', 'Training loss: 1.3862', '6.5438 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1573/1810', 'Training loss: 1.3857', '6.3455 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1574/1810', 'Training loss: 1.3852', '6.3325 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1575/1810', 'Training loss: 1.3848', '6.4199 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1576/1810', 'Training loss: 1.3847', '6.5268 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1577/1810', 'Training loss: 1.3845', '6.4696 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1578/1810', 'Training loss: 1.3845', '6.3523 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1579/1810', 'Training loss: 1.3843', '6.5390 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1580/1810', 'Training loss: 1.3841', '6.5379 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1581/1810', 'Training loss: 1.3839', '6.5585 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1582/1810', 'Training loss: 1.3834', '6.3603 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1583/1810', 'Training loss: 1.3832', '6.3841 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1584/1810', 'Training loss: 1.3832', '6.4789 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1585/1810', 'Training loss: 1.3829', '6.3582 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1586/1810', 'Training loss: 1.3829', '6.4659 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1587/1810', 'Training loss: 1.3830', '6.3847 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1588/1810', 'Training loss: 1.3829', '6.5637 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1589/1810', 'Training loss: 1.3828', '6.3773 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1590/1810', 'Training loss: 1.3827', '6.3560 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1591/1810', 'Training loss: 1.3827', '6.8223 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1592/1810', 'Training loss: 1.3828', '6.5941 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1593/1810', 'Training loss: 1.3828', '6.3803 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1594/1810', 'Training loss: 1.3826', '6.3732 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1595/1810', 'Training loss: 1.3826', '6.4173 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1596/1810', 'Training loss: 1.3825', '6.4599 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1597/1810', 'Training loss: 1.3826', '6.5591 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1598/1810', 'Training loss: 1.3824', '6.3904 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1599/1810', 'Training loss: 1.3826', '6.3469 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1600/1810', 'Training loss: 1.3826', '6.4961 sec/batch')\n",
      "('Validation loss:', 1.264752, 'Saving checkpoint!')\n",
      "('Epoch 9/10 ', 'Iteration 1601/1810', 'Training loss: 1.3836', '6.3548 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1602/1810', 'Training loss: 1.3836', '6.4205 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1603/1810', 'Training loss: 1.3832', '6.4594 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1604/1810', 'Training loss: 1.3832', '6.3939 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1605/1810', 'Training loss: 1.3829', '6.3276 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1606/1810', 'Training loss: 1.3829', '6.3675 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1607/1810', 'Training loss: 1.3828', '6.4138 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1608/1810', 'Training loss: 1.3828', '6.4241 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1609/1810', 'Training loss: 1.3829', '6.5475 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1610/1810', 'Training loss: 1.3828', '6.2819 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1611/1810', 'Training loss: 1.3825', '6.5060 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1612/1810', 'Training loss: 1.3823', '6.7603 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1613/1810', 'Training loss: 1.3821', '6.3780 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1614/1810', 'Training loss: 1.3822', '6.4001 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1615/1810', 'Training loss: 1.3823', '6.3626 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1616/1810', 'Training loss: 1.3821', '6.4735 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1617/1810', 'Training loss: 1.3821', '6.3932 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1618/1810', 'Training loss: 1.3823', '6.3059 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1619/1810', 'Training loss: 1.3824', '6.3881 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1620/1810', 'Training loss: 1.3827', '6.5215 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1621/1810', 'Training loss: 1.3826', '6.4620 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1622/1810', 'Training loss: 1.3825', '6.3912 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1623/1810', 'Training loss: 1.3824', '6.3899 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1624/1810', 'Training loss: 1.3823', '6.4110 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1625/1810', 'Training loss: 1.3821', '6.5547 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1626/1810', 'Training loss: 1.3822', '6.4188 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1627/1810', 'Training loss: 1.3822', '6.3400 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1628/1810', 'Training loss: 1.3822', '6.4779 sec/batch')\n",
      "('Epoch 9/10 ', 'Iteration 1629/1810', 'Training loss: 1.3822', '6.5030 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1630/1810', 'Training loss: 1.4908', '6.4856 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1631/1810', 'Training loss: 1.4313', '6.3509 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1632/1810', 'Training loss: 1.4078', '6.3779 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1633/1810', 'Training loss: 1.4025', '6.4671 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1634/1810', 'Training loss: 1.3984', '7.3973 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1635/1810', 'Training loss: 1.3953', '6.8718 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1636/1810', 'Training loss: 1.3895', '6.4605 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1637/1810', 'Training loss: 1.3869', '6.3985 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1638/1810', 'Training loss: 1.3843', '6.4815 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1639/1810', 'Training loss: 1.3793', '6.5562 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1640/1810', 'Training loss: 1.3802', '8.1864 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1641/1810', 'Training loss: 1.3774', '6.3613 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1642/1810', 'Training loss: 1.3763', '6.3564 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1643/1810', 'Training loss: 1.3752', '6.5237 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1644/1810', 'Training loss: 1.3768', '6.5583 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1645/1810', 'Training loss: 1.3734', '6.4600 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1646/1810', 'Training loss: 1.3708', '6.6834 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1647/1810', 'Training loss: 1.3705', '6.9967 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1648/1810', 'Training loss: 1.3708', '6.3290 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1649/1810', 'Training loss: 1.3717', '6.5464 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1650/1810', 'Training loss: 1.3707', '6.3959 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1651/1810', 'Training loss: 1.3703', '6.5420 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1652/1810', 'Training loss: 1.3695', '6.3906 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1653/1810', 'Training loss: 1.3694', '6.3359 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1654/1810', 'Training loss: 1.3689', '6.5073 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1655/1810', 'Training loss: 1.3688', '6.3793 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1656/1810', 'Training loss: 1.3686', '7.1363 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1657/1810', 'Training loss: 1.3664', '7.6263 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1658/1810', 'Training loss: 1.3661', '7.2241 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1659/1810', 'Training loss: 1.3658', '7.2650 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1660/1810', 'Training loss: 1.3654', '7.2373 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1661/1810', 'Training loss: 1.3655', '7.0347 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1662/1810', 'Training loss: 1.3656', '7.2780 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1663/1810', 'Training loss: 1.3651', '7.7475 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1664/1810', 'Training loss: 1.3645', '7.6887 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1665/1810', 'Training loss: 1.3651', '7.6050 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1666/1810', 'Training loss: 1.3649', '7.9438 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1667/1810', 'Training loss: 1.3638', '7.1345 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1668/1810', 'Training loss: 1.3626', '6.8928 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1669/1810', 'Training loss: 1.3610', '7.3281 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1670/1810', 'Training loss: 1.3599', '7.0400 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1671/1810', 'Training loss: 1.3594', '7.1189 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1672/1810', 'Training loss: 1.3588', '7.1478 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1673/1810', 'Training loss: 1.3582', '7.0659 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1674/1810', 'Training loss: 1.3586', '7.0622 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1675/1810', 'Training loss: 1.3582', '6.9929 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1676/1810', 'Training loss: 1.3573', '7.1412 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1677/1810', 'Training loss: 1.3569', '7.0869 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1678/1810', 'Training loss: 1.3569', '7.0336 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1679/1810', 'Training loss: 1.3569', '7.0384 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1680/1810', 'Training loss: 1.3565', '7.1881 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1681/1810', 'Training loss: 1.3558', '7.0580 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1682/1810', 'Training loss: 1.3558', '7.5005 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1683/1810', 'Training loss: 1.3557', '7.4647 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1684/1810', 'Training loss: 1.3556', '7.2515 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1685/1810', 'Training loss: 1.3559', '7.0607 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1686/1810', 'Training loss: 1.3559', '7.0285 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1687/1810', 'Training loss: 1.3559', '7.0719 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1688/1810', 'Training loss: 1.3557', '7.2151 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1689/1810', 'Training loss: 1.3556', '7.1823 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1690/1810', 'Training loss: 1.3559', '6.9991 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1691/1810', 'Training loss: 1.3556', '7.0704 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1692/1810', 'Training loss: 1.3559', '7.0434 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1693/1810', 'Training loss: 1.3559', '7.0246 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1694/1810', 'Training loss: 1.3566', '7.1269 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1695/1810', 'Training loss: 1.3574', '7.0174 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1696/1810', 'Training loss: 1.3569', '7.1202 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1697/1810', 'Training loss: 1.3573', '7.3356 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1698/1810', 'Training loss: 1.3570', '7.0234 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1699/1810', 'Training loss: 1.3568', '7.0491 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1700/1810', 'Training loss: 1.3569', '7.0847 sec/batch')\n",
      "('Validation loss:', 1.2479845, 'Saving checkpoint!')\n",
      "('Epoch 10/10 ', 'Iteration 1701/1810', 'Training loss: 1.3589', '7.0543 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1702/1810', 'Training loss: 1.3589', '7.1675 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1703/1810', 'Training loss: 1.3593', '7.0524 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1704/1810', 'Training loss: 1.3595', '6.9819 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1705/1810', 'Training loss: 1.3594', '7.0333 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1706/1810', 'Training loss: 1.3595', '7.1769 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1707/1810', 'Training loss: 1.3593', '7.0170 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1708/1810', 'Training loss: 1.3588', '7.1038 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1709/1810', 'Training loss: 1.3587', '7.7281 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1710/1810', 'Training loss: 1.3586', '7.7506 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1711/1810', 'Training loss: 1.3584', '8.1082 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1712/1810', 'Training loss: 1.3583', '8.9378 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1713/1810', 'Training loss: 1.3579', '7.8020 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1714/1810', 'Training loss: 1.3575', '7.6481 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1715/1810', 'Training loss: 1.3569', '7.6988 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1716/1810', 'Training loss: 1.3562', '7.5754 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1717/1810', 'Training loss: 1.3562', '7.6755 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1718/1810', 'Training loss: 1.3562', '7.6722 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1719/1810', 'Training loss: 1.3559', '7.6045 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1720/1810', 'Training loss: 1.3555', '7.9181 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1721/1810', 'Training loss: 1.3547', '7.6037 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1722/1810', 'Training loss: 1.3547', '7.7157 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1723/1810', 'Training loss: 1.3544', '7.4624 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1724/1810', 'Training loss: 1.3542', '7.5090 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1725/1810', 'Training loss: 1.3540', '6.9632 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1726/1810', 'Training loss: 1.3538', '6.3720 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1727/1810', 'Training loss: 1.3538', '6.4259 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1728/1810', 'Training loss: 1.3535', '6.6572 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1729/1810', 'Training loss: 1.3534', '7.5772 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1730/1810', 'Training loss: 1.3529', '7.4470 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1731/1810', 'Training loss: 1.3526', '7.4982 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1732/1810', 'Training loss: 1.3521', '7.3988 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1733/1810', 'Training loss: 1.3516', '7.5588 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1734/1810', 'Training loss: 1.3515', '7.7331 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1735/1810', 'Training loss: 1.3511', '7.4561 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1736/1810', 'Training loss: 1.3509', '7.4651 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1737/1810', 'Training loss: 1.3508', '7.5623 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1738/1810', 'Training loss: 1.3505', '7.9925 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1739/1810', 'Training loss: 1.3503', '7.9893 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1740/1810', 'Training loss: 1.3500', '7.3556 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1741/1810', 'Training loss: 1.3501', '7.5964 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1742/1810', 'Training loss: 1.3502', '7.4346 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1743/1810', 'Training loss: 1.3500', '7.4701 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1744/1810', 'Training loss: 1.3499', '7.6599 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1745/1810', 'Training loss: 1.3496', '7.8738 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1746/1810', 'Training loss: 1.3495', '7.6654 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1747/1810', 'Training loss: 1.3491', '7.6292 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1748/1810', 'Training loss: 1.3489', '7.4982 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1749/1810', 'Training loss: 1.3488', '7.3944 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1750/1810', 'Training loss: 1.3485', '7.5019 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1751/1810', 'Training loss: 1.3484', '7.4847 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1752/1810', 'Training loss: 1.3483', '7.6511 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1753/1810', 'Training loss: 1.3484', '7.4256 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1754/1810', 'Training loss: 1.3481', '7.6446 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1755/1810', 'Training loss: 1.3476', '7.6070 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1756/1810', 'Training loss: 1.3472', '7.3759 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1757/1810', 'Training loss: 1.3471', '6.8827 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1758/1810', 'Training loss: 1.3470', '6.3960 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1759/1810', 'Training loss: 1.3469', '6.4330 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1760/1810', 'Training loss: 1.3467', '7.6871 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1761/1810', 'Training loss: 1.3467', '7.4378 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1762/1810', 'Training loss: 1.3465', '7.6236 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1763/1810', 'Training loss: 1.3461', '7.4966 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1764/1810', 'Training loss: 1.3459', '7.4252 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1765/1810', 'Training loss: 1.3458', '7.6788 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1766/1810', 'Training loss: 1.3456', '6.5421 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1767/1810', 'Training loss: 1.3456', '6.3967 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1768/1810', 'Training loss: 1.3457', '6.4173 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1769/1810', 'Training loss: 1.3457', '6.3567 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1770/1810', 'Training loss: 1.3456', '6.5468 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1771/1810', 'Training loss: 1.3455', '6.4060 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1772/1810', 'Training loss: 1.3455', '6.5885 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1773/1810', 'Training loss: 1.3456', '6.3870 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1774/1810', 'Training loss: 1.3456', '6.3868 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1775/1810', 'Training loss: 1.3456', '6.9861 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1776/1810', 'Training loss: 1.3455', '7.7783 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1777/1810', 'Training loss: 1.3454', '7.3313 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1778/1810', 'Training loss: 1.3456', '7.4559 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1779/1810', 'Training loss: 1.3454', '7.7376 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1780/1810', 'Training loss: 1.3455', '7.4227 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1781/1810', 'Training loss: 1.3456', '7.5632 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1782/1810', 'Training loss: 1.3457', '7.5026 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1783/1810', 'Training loss: 1.3458', '7.5491 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1784/1810', 'Training loss: 1.3454', '7.4982 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1785/1810', 'Training loss: 1.3454', '7.1216 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1786/1810', 'Training loss: 1.3451', '6.4625 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1787/1810', 'Training loss: 1.3451', '6.3925 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1788/1810', 'Training loss: 1.3450', '6.5274 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1789/1810', 'Training loss: 1.3450', '6.6587 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1790/1810', 'Training loss: 1.3449', '6.3516 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1791/1810', 'Training loss: 1.3448', '6.5375 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1792/1810', 'Training loss: 1.3445', '6.4989 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1793/1810', 'Training loss: 1.3443', '6.4110 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1794/1810', 'Training loss: 1.3442', '6.4746 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1795/1810', 'Training loss: 1.3442', '6.4306 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1796/1810', 'Training loss: 1.3444', '6.4142 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1797/1810', 'Training loss: 1.3442', '6.4280 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1798/1810', 'Training loss: 1.3441', '6.3847 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1799/1810', 'Training loss: 1.3443', '6.4188 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1800/1810', 'Training loss: 1.3444', '6.3762 sec/batch')\n",
      "('Validation loss:', 1.2296156, 'Saving checkpoint!')\n",
      "('Epoch 10/10 ', 'Iteration 1801/1810', 'Training loss: 1.3455', '6.4501 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1802/1810', 'Training loss: 1.3456', '6.3995 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1803/1810', 'Training loss: 1.3455', '6.4117 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1804/1810', 'Training loss: 1.3454', '6.3952 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1805/1810', 'Training loss: 1.3452', '6.4868 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1806/1810', 'Training loss: 1.3451', '6.3205 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1807/1810', 'Training loss: 1.3451', '6.9205 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1808/1810', 'Training loss: 1.3450', '6.4015 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1809/1810', 'Training loss: 1.3451', '6.4681 sec/batch')\n",
      "('Epoch 10/10 ', 'Iteration 1810/1810', 'Training loss: 1.3450', '6.3460 sec/batch')\n",
      "('Validation loss:', 1.22922, 'Saving checkpoint!')\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "save_every_n = 100\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter('./logs/2/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('./logs/2/test')\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.5,\n",
    "                    model.initial_state: new_state}\n",
    "            summary, batch_loss, new_state, _ = sess.run([model.merged, model.cost, \n",
    "                                                          model.final_state, model.optimizer], \n",
    "                                                          feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "            \n",
    "            train_writer.add_summary(summary, iteration)\n",
    "        \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    summary, batch_loss, new_state = sess.run([model.merged, model.cost, \n",
    "                                                               model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "                    \n",
    "                test_writer.add_summary(summary, iteration)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                #saver.save(sess, \"checkpoints/anna/i{}_l{}_{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/anna/i3560_l512_1.122.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i200_l512_2.432.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i400_l512_1.980.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i600_l512_1.750.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i800_l512_1.595.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1000_l512_1.484.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1200_l512_1.407.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1400_l512_1.349.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1600_l512_1.292.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1800_l512_1.255.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i2000_l512_1.224.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i2200_l512_1.204.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i2400_l512_1.187.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i2600_l512_1.172.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i2800_l512_1.160.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i3000_l512_1.148.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i3200_l512_1.137.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i3400_l512_1.129.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i3560_l512_1.122.ckpt\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/anna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    prime = \"Far\"\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farlathit that if had so\n",
      "like it that it were. He could not trouble to his wife, and there was\n",
      "anything in them of the side of his weaky in the creature at his forteren\n",
      "to him.\n",
      "\n",
      "\"What is it? I can't bread to those,\" said Stepan Arkadyevitch. \"It's not\n",
      "my children, and there is an almost this arm, true it mays already,\n",
      "and tell you what I have say to you, and was not looking at the peasant,\n",
      "why is, I don't know him out, and she doesn't speak to me immediately, as\n",
      "you would say the countess and the more frest an angelembre, and time and\n",
      "things's silent, but I was not in my stand that is in my head. But if he\n",
      "say, and was so feeling with his soul. A child--in his soul of his\n",
      "soul of his soul. He should not see that any of that sense of. Here he\n",
      "had not been so composed and to speak for as in a whole picture, but\n",
      "all the setting and her excellent and society, who had been delighted\n",
      "and see to anywing had been being troed to thousand words on them,\n",
      "we liked him.\n",
      "\n",
      "That set in her money at the table, he came into the party. The capable\n",
      "of his she could not be as an old composure.\n",
      "\n",
      "\"That's all something there will be down becime by throe is\n",
      "such a silent, as in a countess, I should state it out and divorct.\n",
      "The discussion is not for me. I was that something was simply they are\n",
      "all three manshess of a sensitions of mind it all.\"\n",
      "\n",
      "\"No,\" he thought, shouted and lifting his soul. \"While it might see your\n",
      "honser and she, I could burst. And I had been a midelity. And I had a\n",
      "marnief are through the countess,\" he said, looking at him, a chosing\n",
      "which they had been carried out and still solied, and there was a sen that\n",
      "was to be completely, and that this matter of all the seconds of it, and\n",
      "a concipation were to her husband, who came up and conscaously, that he\n",
      "was not the station. All his fourse she was always at the country,,\n",
      "to speak oft, and though they were to hear the delightful throom and\n",
      "whether they came towards the morning, and his living and a coller and\n",
      "hold--the children. \n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i3560_l512_1.122.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farnt him oste wha sorind thans tout thint asd an sesand an hires on thime sind thit aled, ban thand and out hore as the ter hos ton ho te that, was tis tart al the hand sostint him sore an tit an son thes, win he se ther san ther hher tas tarereng,.\n",
      "\n",
      "Anl at an ades in ond hesiln, ad hhe torers teans, wast tar arering tho this sos alten sorer has hhas an siton ther him he had sin he ard ate te anling the sosin her ans and\n",
      "arins asd and ther ale te tot an tand tanginge wath and ho ald, so sot th asend sat hare sother horesinnd, he hesense wing ante her so tith tir sherinn, anded and to the toul anderin he sorit he torsith she se atere an ting ot hand and thit hhe so the te wile har\n",
      "ens ont in the sersise, and we he seres tar aterer, to ato tat or has he he wan ton here won and sen heren he sosering, to to theer oo adent har herere the wosh oute, was serild ward tous hed astend..\n",
      "\n",
      "I's sint on alt in har tor tit her asd hade shithans ored he talereng an soredendere tim tot hees. Tise sor and \n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i200_l512_2.432.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fard as astice her said he celatice of to seress in the raice, and to be the some and sere allats to that said to that the sark and a cast a the wither ald the pacinesse of her had astition, he said to the sount as she west at hissele. Af the cond it he was a fact onthis astisarianing.\n",
      "\n",
      "\n",
      "\"Or a ton to to be that's a more at aspestale as the sont of anstiring as\n",
      "thours and trey.\n",
      "\n",
      "The same wo dangring the\n",
      "raterst, who sore and somethy had ast out an of his book. \"We had's beane were that, and a morted a thay he had to tere. Then to\n",
      "her homent andertersed his his ancouted to the pirsted, the soution for of the pirsice inthirgest and stenciol, with the hard and and\n",
      "a colrice of to be oneres,\n",
      "the song to this anderssad.\n",
      "The could ounterss the said to serom of\n",
      "soment a carsed of sheres of she\n",
      "torded\n",
      "har and want in their of hould, but\n",
      "her told in that in he tad a the same to her. Serghing an her has and with the seed, and the camt ont his about of the\n",
      "sail, the her then all houg ant or to hus to \n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i600_l512_1.750.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farrat, his felt has at it.\n",
      "\n",
      "\"When the pose ther hor exceed\n",
      "to his sheant was,\" weat a sime of his sounsed. The coment and the facily that which had began terede a marilicaly whice whether the pose of his hand, at she was alligated herself the same on she had to\n",
      "taiking to his forthing and streath how to hand\n",
      "began in a lang at some at it, this he cholded not set all her. \"Wo love that is setthing. Him anstering as seen that.\"\n",
      "\n",
      "\"Yes in the man that say the mare a crances is it?\" said Sergazy Ivancatching. \"You doon think were somether is ifficult of a mone of\n",
      "though the most at the countes that the\n",
      "mean on the come to say the most, to\n",
      "his feesing of\n",
      "a man she, whilo he\n",
      "sained and well, that he would still at to said. He wind at his for the sore in the most\n",
      "of hoss and almoved to see him. They have betine the sumper into at he his stire, and what he was that at the so steate of the\n",
      "sound, and shin should have a geest of shall feet on the conderation to she had been at that imporsing the dre\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i1000_l512_1.484.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
